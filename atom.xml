<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Aier02</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://aier02.com/"/>
  <updated>2019-01-15T09:03:57.369Z</updated>
  <id>http://aier02.com/</id>
  
  <author>
    <name>易安明</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>inception</title>
    <link href="http://aier02.com/2019/01/15/inception/"/>
    <id>http://aier02.com/2019/01/15/inception/</id>
    <published>2019-01-15T08:23:32.427Z</published>
    <updated>2019-01-15T09:03:57.369Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p>Inception 网络是 CNN 分类器发展史上一个重要的里程碑。在 Inception 出现之前，大部分流行 CNN 仅仅是把卷积层堆叠得越来越多，使网络越来越深，以此希望能够得到更好的性能。</p><p>GoogLeNet 最大的特点就是使用了 Inception 模块，它的目的是设计一种具有优良局部拓扑结构的网络，即对输入图像并行地执行多个卷积运算或池化操作，并将所有输出结果拼接为一个非常深的特征图。因为 1x1,3x3 或 5x5 等不同的卷积运算与池化操作可以获得输入图像的不同信息，并行处理这些运算并结合所有结果将获得更好的图像表征</p><h3 id="inception-v1"><a class="markdownIt-Anchor" href="#inception-v1"></a> Inception v1</h3><p>问题：</p><ul><li>图像中突出部分的大小差别很大。例如，狗的图像可以是以下任意情况。每张图像中狗所占区域都是不同的，占据全图，图的中间部分，图的上方小部分。</li><li>同一数据集中由于信息位置的差异巨大，为卷积操作选择的合适大小的卷积核也更加困难；信息分布更全局性的图像偏好大的卷积核，信息分布比较局部的图像偏好于小的卷积核。</li><li>非常深的网络更容易过拟合，将梯度更新传输到整个网络十分困难。</li><li>简单的堆叠较大的卷积层非常消耗计算资源</li></ul><p>解决方法：</p><p>拓宽网络层的宽度而不是深度</p><p>下图是「原始」Inception 模块。它使用 3 个不同大小的滤波器（1x1、3x3、5x5）对输入执行卷积操作，此外它还会执行最大池化。所有子层的输出最后会被级联起来，并传送至下一个 Inception 模块。</p><p><img src="/images/190115/inceptionv1.jpeg" alt="inceptionv1"></p><p>为了降低算力成本，作者在 3x3 和 5x5 卷积层之前添加额外的 1x1 卷积层，来限制输入信道的数量。尽管添加额外的卷积操作似乎是反直觉的，但是 1x1 卷积比 5x5 卷积要廉价很多，而且输入信道数量减少也有利于降低算力成本。不过一定要注意，1x1 卷积是在最大池化层之后，而不是之前。</p><p><img src="/images/190115/inceptionv1b.jpeg" alt="inceptionv1b"></p><h3 id="inception-v2"><a class="markdownIt-Anchor" href="#inception-v2"></a> Inception v2</h3><p>问题：</p><ul><li>减少特征的表征性瓶颈。直观上来说，当卷积不会大幅度改变输入维度时，神经网络可能会执行地更好。过多地减少维度可能会造成信息的损失，这也称为「表征性瓶颈」。</li><li>使用更优秀的因子分解方法，卷积才能在计算复杂度上更加高效。</li></ul><p>解决方案：</p><ul><li>将 5×5 的卷积分解为两个 3×3 的卷积运算以提升计算速度。尽管这有点违反直觉，但一个 5×5 的卷积在计算成本上是一个 3×3 卷积的 2.78 倍。所以叠加两个 3×3 卷积实际上在性能上会有所提升，如下图所示</li></ul><p><img src="/images/190115/v2.jpeg" alt="v2"></p><ul><li>将 nxn 的卷积核尺寸分解为 1×n 和 n×1 两个卷积。例如，一个 3×3 的卷积等价于首先执行一个 1×3 的卷积再执行一个 3×1 的卷积。他们还发现这种方法在成本上要比单个 3×3 的卷积降低 33%.</li></ul><p><img src="/images/190115/v2b.jpeg" alt="v2b"></p><ul><li>模块中的滤波器组被扩展（即变得更宽而不是更深），以解决表征性瓶颈。如果该模块没有被拓展宽度，而是变得更深，那么维度会过多减少，造成信息损失.</li></ul><p><img src="/images/190115/v2c.jpeg" alt="v2c"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Inception 网络是 CNN 分类器发展史上一个重要的里程碑。在 Inception 出现之前，大部分流行 CNN 仅仅是把卷积层堆叠得越
      
    
    </summary>
    
      <category term="cv" scheme="http://aier02.com/categories/cv/"/>
    
    
      <category term="blog" scheme="http://aier02.com/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>BoW in cv</title>
    <link href="http://aier02.com/2019/01/12/BoW_in_cv/"/>
    <id>http://aier02.com/2019/01/12/BoW_in_cv/</id>
    <published>2019-01-12T02:59:03.404Z</published>
    <updated>2019-01-12T02:59:23.356Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><h3 id="bag-of-words"><a class="markdownIt-Anchor" href="#bag-of-words"></a> Bag of words</h3><p>词袋模型可以理解为直方图统计，都是对目标的频率统计，而没有序列信息；但和histogram不同的是，histogram统计的的是某个区间的频率，词袋模型统计的是words字典中每个单词出现的次数。</p><p>比如现有两个文档</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">John likes to watch movies. Mary likes too.</span><br><span class="line">John also likes to watch football games.</span><br></pre></td></tr></table></figure><p>找出两个文档的并集，构建dictionary</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">"John"</span>:<span class="number">1</span>, <span class="string">'likes'</span>:<span class="number">2</span>, <span class="string">"to"</span>:<span class="number">3</span>, <span class="string">'watch'</span>:<span class="number">4</span>, <span class="string">'movies'</span>:<span class="number">5</span>, <span class="string">'also'</span>:<span class="number">6</span>, <span class="string">'football'</span>:<span class="number">7</span>, <span class="string">'games'</span>:<span class="number">8</span>, <span class="string">'Mary'</span>:<span class="number">9</span>, <span class="string">'too'</span>:<span class="number">10</span>&#125;</span><br></pre></td></tr></table></figure><p>则这两篇文档得到的BoW(刚开始用于自然语言处理和信息检索中的一种文档表示方法)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line">[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="bow-model-in-cv"><a class="markdownIt-Anchor" href="#bow-model-in-cv"></a> BoW model in CV</h3><p>先提取图像集中的特征集合，然后使用聚类的方法得到若干类，利用这些类构建dictionary，相当于words，最后每个图像统计字典中word出现的频数作为输出向量，用于后续的检索、分类等操作。</p><p>下图以提取图像的sift特征为例；SIFT的全称是Scale Invariant Feature Transform，尺度不变特征变换，由加拿大教授David G.Lowe提出的。SIFT特征对旋转、尺度缩放、亮度变化等保持不变性，是一种非常稳定的局部特征。</p><p><img src="/images/190112/bow.png" alt="bow"></p><p>之后在每一幅图像中统计sift特征点在dictionary上的频数分布，得到的向量就是该图像的BoW向量</p><p>下图是图像集中包含人脸、自行车、吉他等。</p><p><img src="/images/190112/exam.png" alt="exam"></p><p>可以理解为每张图都是一个‘词袋’，包含了不同种类的数量不同的特征，然后统计该图中不同‘单词’的频数。</p><p><img src="/images/190112/process.png" alt="process"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;bag-of-words&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#bag-of-words&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="cv" scheme="http://aier02.com/categories/cv/"/>
    
    
      <category term="blog" scheme="http://aier02.com/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>gradient vanishing &amp; exploding problem</title>
    <link href="http://aier02.com/2019/01/11/gradient-vanishing-&amp;-exploding-problem/"/>
    <id>http://aier02.com/2019/01/11/gradient-vanishing-&amp;-exploding-problem/</id>
    <published>2019-01-11T07:33:27.121Z</published>
    <updated>2019-01-11T07:33:27.121Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p>在面senetime的时候由resnet为何能解决随着网络加深而不再导致准确率下降的问题引出了梯度的问题，发现自己的对梯度消失和梯度爆炸问题认识非常有限，仅局限于激活函数的影响，也就是cs231n介绍激活函数的时候所介绍的饱和问题，但是显然若使用Relu函数不存在这种问题，并且resnet的巧妙之处也不是激活函数的创新；这里在查找了一些blog后进行个人的总结。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;在面senetime的时候由resnet为何能解决随着网络加深而不再导致准确率下降的问题引出了梯度的问题，发现自己的对梯度消失和梯度爆炸问题认识
      
    
    </summary>
    
      <category term="dl" scheme="http://aier02.com/categories/dl/"/>
    
    
      <category term="gradient" scheme="http://aier02.com/tags/gradient/"/>
    
  </entry>
  
  <entry>
    <title>LSTMs</title>
    <link href="http://aier02.com/2019/01/11/LSTMs/"/>
    <id>http://aier02.com/2019/01/11/LSTMs/</id>
    <published>2019-01-11T07:33:13.044Z</published>
    <updated>2019-01-11T08:59:06.676Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p>毕设任务暂定为video classification，一种实现方式使用到了LSTM神经网络，先做简单的入门准备。</p><h3 id="recurrent-neural-networks"><a class="markdownIt-Anchor" href="#recurrent-neural-networks"></a> Recurrent Neural Networks</h3><p>RNN(循环神经网络)的提出是传统的CNN难以应对tmporal context的情况下提出的，他的目的就是针对时许关系，把上一个时间的网络输出联合当前时间的输入作为新的当前时间的输入，以实现利用上个时间的信息。</p><p>未展开的RNN结构</p><p><img src="/images/190111/RNN-rolled.png" alt="RNN-rolled"></p><p>展开的RNN结构</p><p><img src="/images/190111/RNN-unrolled.png" alt="RNN-unrolled"></p><p>一般RNN的问题在于伴随着gap的变大，网络的输出与之前的较久的输出的关系不断减弱，使得后面的网络无法学习到较早的知识，即存在长时间的依赖问题。</p><h3 id="lstm-networks"><a class="markdownIt-Anchor" href="#lstm-networks"></a> LSTM Networks</h3><p>Long Short Term Memory networks的提出就是针对普通RNN固有的记忆时间短的问题，是一种特殊的RNN，善于解决长时间依赖问题，LSTM默认的行为时记住长时间的信息，所有的RNN都具有重复nn的链式结构</p><ul><li>标准RNN的链式结构</li></ul><p><img src="/images/190111/LSTM3-SimpleRNN.png" alt="LSTM3-SimpleRNN"></p><ul><li>LSTM的链式结构</li></ul><p><img src="/images/190111/LSTM3-chain.png" alt="LSTM3-chain"></p><p>上图各种标示的意义</p><p><img src="/images/190111/LSTM2-notation.png" alt="LSTM2-notation"></p><p>黄色的矩形表示用于学习的神经网络，粉色的原点表示逐点的运算，横箭头表示向量的操作，两条线的合箭头表示串联，分箭头表示复制了该向量，并分别用于不同的部分。</p><h3 id="the-core-idea-behind-lstms"><a class="markdownIt-Anchor" href="#the-core-idea-behind-lstms"></a> The Core Idea Behind LSTMs</h3><p>LSTM中的关键时cell state，下图中顶部的水平线即为cell state的传送路径，用于信息传送,他能直接贯通整个LSTM；</p><p><img src="/images/190111/LSTM3-C-line.png" alt="LSTM3-C-line"></p><p>LSTM中用gate实现对cell state的信息移除或者添加，gate是一条有选择性的信息通路，它由sigmoid神经网络层和一个逐点乘法运算组成。通过sigmoid决定要通过的程度，与cell state相乘得到想要控制通过的cell state部分</p><p><img src="/images/190111/LSTM3-gate.png" alt="LSTM3-gate"></p><p>s型曲线的输出为[0,1]，0表示全部信息都不通过，相反1为全部信息通过。一个LSTM中有3个这样的gate，用于控制和保护cell state.</p><h3 id="step-by-step-lstm-walk-through"><a class="markdownIt-Anchor" href="#step-by-step-lstm-walk-through"></a> Step-by-Step LSTM Walk Through</h3><ol><li>LSTM中的第一步是决定哪些信息需要从cell state中移除，利用sigmoid layer实现这一操作，即通过‘forget gate layer’，控制信息是否通过，利用<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">h_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.902771em;vertical-align:-.208331em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">h</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">−</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.58056em;vertical-align:-.15em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>的信息输出0到1，0表示全部‘忘记’，1表示‘记住’上一次全部的cell state.</li></ol><p><img src="/images/190111/LSTM3-focus-f.png" alt="LSTM3-focus-f"></p><p>2.第二步是要决定哪些新的信息需要保存在cell state中，包括两个部分：</p><ul><li>input gate layer：一个sigmoid layer，决定了需要更新哪些值。</li><li>Tanh layer:创建新的候选cell state向量</li></ul><p><img src="/images/190111/LSTM3-focus-i.png" alt="LSTM3-focus-i"></p><p>3.第三步综合之前两步决定要遗忘的信息和要更新的信息，更新本次的cell state，新的cell state由两部分组成：</p><ul><li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>f</mi><mi>t</mi></msub><mo>∗</mo><msub><mi>c</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">f_t*c_{t-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.902771em;vertical-align:-.208331em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">∗</span><span class="mord"><span class="mord mathit">c</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">t</span><span class="mbin">−</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>上一次的cell state 乘以遗忘系数得到目前需要的旧cell state 的信息程度。</li><li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>i</mi><mi>t</mi></msub><mo>∗</mo><mover accent="true"><mrow><msub><mi>c</mi><mi>t</mi></msub></mrow><mo stretchy="true">‾</mo></mover></mrow><annotation encoding="application/x-tex">i_t*\overline{c_t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.65952em"></span><span class="strut bottom" style="height:.80952em;vertical-align:-.15em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">i</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">∗</span><span class="overline mord"><span class="vlist"><span style="top:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em">​</span></span><span class="mord textstyle cramped"><span class="mord"><span class="mord mathit">c</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span><span style="top:-.5505599999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em">​</span></span><span class="reset-textstyle textstyle uncramped overline-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em">​</span></span>​</span></span></span></span></span></span>要更新的信息值对应的index乘以候选的cell state得到本次要更新后的候选cell state</li></ul><p><img src="/images/190111/LSTM3-focus-C.png" alt="LSTM3-focus-C"></p><p>4.最后决定输出，先通过一个sigmoid layer决定cell state哪些部分需要输出(该操作和第三步的决定cell state哪些部分需要更新的操作一致)，同时将之前3步算出来的cell state通过tanh layer(压缩输出到[-1,1])，然后tanh layer 得到的值和sigmoid layer 得到的值进行相乘，得到最后要输出的部分。</p><p><img src="/images/190111/LSTM3-focus-o.png" alt="LSTM3-focus-o"></p><h3 id="variants-on-long-short-term-memory"><a class="markdownIt-Anchor" href="#variants-on-long-short-term-memory"></a> Variants on Long Short Term Memory</h3><p>上述为正常的LSTM结构，但是在不同情况下可能存在不同LSTM，下面分别描述不同的变体。</p><p>1.gate layer也关注cell state,即cell state也作为所有gate的输入，术语上说是给gate layer添加一个peephole</p><p><img src="/images/190111/LSTM3-var-peepholes.png" alt="LSTM3-var-peepholes"></p><p>2.直接使用成对的forget gate 和 input gate，同时决定遗忘的部分和更新的部分；而不是分别去决定哪些需要忘记和哪些需要添加。</p><p><img src="/images/190111/LSTM3-var-tied.png" alt="LSTM3-var-tied"></p><ol start="3"><li>比较特别的变体是Gated Recurrent Unit，将forget gate和input gate合并为了一个update gate,同时合并了cell state 和hidden state</li></ol><p><img src="/images/190111/LSTM3-var-GRU.png" alt="LSTM3-var-GRU"></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;毕设任务暂定为video classification，一种实现方式使用到了LSTM神经网络，先做简单的入门准备。&lt;/p&gt;&lt;h3 id=&quot;rec
      
    
    </summary>
    
      <category term="cv" scheme="http://aier02.com/categories/cv/"/>
    
    
      <category term="blog" scheme="http://aier02.com/tags/blog/"/>
    
  </entry>
  
  <entry>
    <title>epoch &amp; iteration</title>
    <link href="http://aier02.com/2019/01/10/epoch%20&amp;%20iteration/"/>
    <id>http://aier02.com/2019/01/10/epoch &amp; iteration/</id>
    <published>2019-01-10T11:15:52.935Z</published>
    <updated>2019-01-11T05:16:34.023Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p>再使用pytorch进行训练时，经常会看到batch_size,iteration,step,epoch这几个词，他们的意思和关系分别如下：</p><ul><li>epoch:一个epoch表示所有训练的样本运算学习一遍</li><li>iteration/step：表示每运行一个iteration/step，更新一次参数权重，即进行一次学习，每进行一次更新都需要batch_size个样本进行运算学习，根据运算的结果更新一次参数；对应训练时为一张卡一个进程读取了batch_size张图片，froward/backward一次后，学习了一次参数，计作一次step</li><li>batch_size：一次参数更新运算所需的样本数量，即一张卡中读取的sample数量</li></ul><p>在多卡多机训练时，每进行一次step后，都要重新加载数据，即使用train_loader重新读取一个batch到GPU上。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;再使用pytorch进行训练时，经常会看到batch_size,iteration,step,epoch这几个词，他们的意思和关系分别如下：&lt;/
      
    
    </summary>
    
      <category term="cv" scheme="http://aier02.com/categories/cv/"/>
    
    
      <category term="pytorch" scheme="http://aier02.com/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>BN</title>
    <link href="http://aier02.com/2019/01/09/BN/"/>
    <id>http://aier02.com/2019/01/09/BN/</id>
    <published>2019-01-09T11:45:40.264Z</published>
    <updated>2019-01-09T11:45:41.115Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p>L2正则化通常对稀疏的有尖峰的权重响亮施加比较大的惩罚，而偏向于均匀的参数，这样做的效果时鼓励神经但愿利用上层的所有输入，而不是部分输入，所以L2正则项加入之后，权重的绝对值大小都会整体倾向于减少，尤其不会出现特别大的值，即网络偏向于学习比较小的权重，所以L2正则化在深度学习中又叫做‘权重衰减’(weight decay),在机器学习中又叫做‘岭回归’(rifge regression)</p><p><img src="/images/190109/bn.png" alt="bn"></p><p>BN的第四步做的尺度变换和偏移，由于归一化后<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.58056em;vertical-align:-.15em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>基本被限制在了正态分布下，使得网络的表达能力下降，引入两个参数，使得网络在训练时自己进行学习。数据被压缩到中心区域，对于大多数激活函数而言，中间区域的梯度都是最大的或者说是有梯度的，是变化敏感区域，这样可以加快收敛速度。</p><p>在训练时，对同一批次的数据均值和方差进行求解，进而进行归一化操作；而在预测的时候，所使用的均值和方差来源于训练集，比如在训练时，我们记录下每个batch的均值和方差，待训练完毕后，求整个训练样本的均值和方差的期望，作为预测时进行BN的均值和方差。</p><p><img src="/images/190109/expectation.png" alt="expectation"></p><p>测试阶段使用的BN公式为：</p><p><img src="/images/190109/precdiction.png" alt="precdiction"></p><p>关于BN层使用的位置，在CNN中一般是作用于激活函数之前。</p><p><img src="/images/190109/position.png" alt="position"></p><p>若遇到梯度爆炸或者收敛速度很慢时可以考虑使用BN。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;L2正则化通常对稀疏的有尖峰的权重响亮施加比较大的惩罚，而偏向于均匀的参数，这样做的效果时鼓励神经但愿利用上层的所有输入，而不是部分输入，所以L
      
    
    </summary>
    
      <category term="cv" scheme="http://aier02.com/categories/cv/"/>
    
    
      <category term="regularization" scheme="http://aier02.com/tags/regularization/"/>
    
  </entry>
  
  <entry>
    <title>pytorch argparser</title>
    <link href="http://aier02.com/2019/01/07/pytorch_argparser/"/>
    <id>http://aier02.com/2019/01/07/pytorch_argparser/</id>
    <published>2019-01-07T07:27:16.595Z</published>
    <updated>2019-01-10T11:12:26.691Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p>argparse 是在pytorch中用于解析命令行参数和选项的标准模块，在编写main脚本时，常常有很多路径、选项等参数需要确定，为了提高代码的健壮性，需要提高命令选项用于修改main中的参数，而不至于为了修改参数而要打开源代码进行。</p><h3 id="一般步骤"><a class="markdownIt-Anchor" href="#一般步骤"></a> 一般步骤</h3><ul><li>import argparse 引用模块</li><li>parser=argparse.ArgumentParser(description=‘介绍该解释器’) 创建一个parser对象</li><li>parser.add_argument() 向解析器对象添加要关注的命令行参数和选项，一个add对应一个参数</li><li>parser.parse_args() 调用parse_args方法进行解析，即可使用</li></ul><p>###add_argument(name or flags…[, action][nargs],[ const],[ default],[ type], choices],[ required],[help],[ metavar],[dest])</p><p>其中：</p><p>name or flags：命令行参数名或者选项，如上面的address或者-p,–port.其中命令行参数如果没给定，且没有设置defualt，则出错。但是如果是选项的话，则设置为None</p><p>action:action=‘store_true’,表示若出现name命令参数，则设置该参数为True，若无出现，则默认为False，反之action=“store_false”，表示若出现则设置为False,否则默认为True.</p><p>nargs：命令行参数的个数，一般使用通配符表示，其中，’?‘表示只用一个，’*‘表示0到多个，’+'表示至少一个</p><p>default：默认值</p><p>type：参数的类型，默认是字符串string类型，还有float、int等类型</p><p>help：和ArgumentParser方法中的参数作用相似，出现的场合也一致</p><h3 id="注意"><a class="markdownIt-Anchor" href="#注意"></a> 注意</h3><p>args=parser.parse_args()时，比如parser.add_argument(’–load-path’,default=’’,type=str),则运行了python命令–load-path='example_file’后，在py文件中读取该参数时是通过args.load_path，注意不是load-path，模块已经把中间的-换成了下滑线。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;argparse 是在pytorch中用于解析命令行参数和选项的标准模块，在编写main脚本时，常常有很多路径、选项等参数需要确定，为了提高代码
      
    
    </summary>
    
      <category term="pytorch" scheme="http://aier02.com/categories/pytorch/"/>
    
    
      <category term="sensetime" scheme="http://aier02.com/tags/sensetime/"/>
    
      <category term="basic knowledge" scheme="http://aier02.com/tags/basic-knowledge/"/>
    
  </entry>
  
  <entry>
    <title>label smoothing</title>
    <link href="http://aier02.com/2019/01/07/label-smoothing/"/>
    <id>http://aier02.com/2019/01/07/label-smoothing/</id>
    <published>2019-01-07T03:44:48.760Z</published>
    <updated>2019-01-07T03:44:48.760Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p>由于训练的样本通常会存在小量的错误标签，这些错误的标签会影响预测的结果，为了减少模型对训练集的依赖，增强模型的泛化能力，采用标签平滑的思路进行解决：在训练时假设标签可能存在错误，避免过分地相信模型的标签，“告诉”模型样本的标签不一定正确，则训练出来的模型对于少量的样本错误就有“免疫力”。</p><p>在二分类任务中，训练样本为(<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i,y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.625em;vertical-align:-.19444em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>),<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.625em;vertical-align:-.19444em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>为样本标签，即0或者1，而在训练中每次迭代时，并不直接将<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">(x_i,y_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>放入训练中集中，而是设置一个错误率<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.43056em;vertical-align:0"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span>,以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">1-\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.64444em"></span><span class="strut bottom" style="height:.72777em;vertical-align:-.08333em"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mbin">−</span><span class="mord mathit">ϵ</span></span></span></span>的概率将样本直接代入，以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.43056em;vertical-align:0"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span>的概率代入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">,</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">(x_i,1-y_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mbin">−</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span></span></span></span>，则模型在训练时，既有正确的标签输入，又有错误的标签输入，这样训练出来的模型不会“全力匹配”每一个标签，而只是在一定程度上匹配，因此出现错误标签时，模型受到的影响就会降低。</p><p>当采用交叉熵来描述损失函数时，对于每一个样本而言，标签变换成了</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mrow><mo fence="true">{</mo><mtable><mtr><mtd><mrow><mi>ϵ</mi><mo separator="true">,</mo></mrow></mtd><mtd><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd><mrow><mn>1</mn><mo>−</mo><mi>ϵ</mi><mo separator="true">,</mo></mrow></mtd><mtd><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">y_i = \begin{cases} \epsilon, &amp; y_i=0\\ 1-\epsilon, &amp; y_i = 1 \end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.75em"></span><span class="strut bottom" style="height:3.0000299999999998em;vertical-align:-1.25003em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist"><span style="top:-.6819999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit">ϵ</span><span class="mpunct">,</span></span></span><span style="top:.7579999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">1</span><span class="mbin">−</span><span class="mord mathit">ϵ</span><span class="mpunct">,</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="arraycolsep" style="width:1em"></span><span class="col-align-l"><span class="vlist"><span style="top:-.6819999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord mathrm">0</span></span></span><span style="top:.7579999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><p>即当标签为0时，不把0直接代入训练，而是替换成一个较小的数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.43056em;vertical-align:0"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span>,同理，当标签为1时，替换成<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">1-\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.64444em"></span><span class="strut bottom" style="height:.72777em;vertical-align:-.08333em"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mbin">−</span><span class="mord mathit">ϵ</span></span></span></span>。所谓平滑即两个极端的值0和1替换成不那么极端的值；在多分类任务中，仍然假设标签值在一定概率下不变，以一定概率变为其他值，若假设均匀分布，则把所有的标签1变为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mo>−</mo><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">1-\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.64444em"></span><span class="strut bottom" style="height:.72777em;vertical-align:-.08333em"></span><span class="base textstyle uncramped"><span class="mord mathrm">1</span><span class="mbin">−</span><span class="mord mathit">ϵ</span></span></span></span>，所有的标签0变为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mi>ϵ</mi><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac \epsilon {k-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.695392em"></span><span class="strut bottom" style="height:1.0987230000000001em;vertical-align:-.403331em"></span><span class="base textstyle uncramped"><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.345em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:.03148em">k</span><span class="mbin">−</span><span class="mord mathrm">1</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.394em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">ϵ</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>.</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;由于训练的样本通常会存在小量的错误标签，这些错误的标签会影响预测的结果，为了减少模型对训练集的依赖，增强模型的泛化能力，采用标签平滑的思路进行解
      
    
    </summary>
    
      <category term="cv" scheme="http://aier02.com/categories/cv/"/>
    
    
      <category term="regularization" scheme="http://aier02.com/tags/regularization/"/>
    
      <category term="sensetime" scheme="http://aier02.com/tags/sensetime/"/>
    
  </entry>
  
  <entry>
    <title>independent component analysis</title>
    <link href="http://aier02.com/2018/12/16/independent_component/"/>
    <id>http://aier02.com/2018/12/16/independent_component/</id>
    <published>2018-12-16T12:42:23.350Z</published>
    <updated>2018-12-16T14:59:53.745Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p><img src="/images/181216/ICA/IMG_3728.jpg" alt=""></p><p><img src="/images/181216/ICA/IMG_3730.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;img src=&quot;/images/181216/ICA/IMG_3728.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/imag
      
    
    </summary>
    
      <category term="cs229n" scheme="http://aier02.com/categories/cs229n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>singular value decomposition</title>
    <link href="http://aier02.com/2018/12/16/singular_value_decomposit/"/>
    <id>http://aier02.com/2018/12/16/singular_value_decomposit/</id>
    <published>2018-12-16T12:41:34.425Z</published>
    <updated>2018-12-16T15:00:01.717Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p><img src="/images/181216/SVD/IMG_3725.jpg" alt=""></p><p><img src="/images/181216/SVD/IMG_3726.jpg" alt=""></p><p><img src="/images/181216/SVD/IMG_3727.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;img src=&quot;/images/181216/SVD/IMG_3725.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/imag
      
    
    </summary>
    
      <category term="cs229n" scheme="http://aier02.com/categories/cs229n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>principal component analysis</title>
    <link href="http://aier02.com/2018/12/16/principal_component/"/>
    <id>http://aier02.com/2018/12/16/principal_component/</id>
    <published>2018-12-16T12:40:51.673Z</published>
    <updated>2018-12-16T15:00:11.897Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p><img src="/images/181216/PCA/IMG_3721.jpg" alt=""></p><p><img src="/images/181216/PCA/IMG_3722.jpg" alt=""></p><p><img src="/images/181216/PCA/IMG_3723.jpg" alt=""></p><p><img src="/images/181216/PCA/IMG_3724.jpg" alt=""></p><p><img src="/images/181216/PCA/IMG_3725.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;img src=&quot;/images/181216/PCA/IMG_3721.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/imag
      
    
    </summary>
    
      <category term="cs229n" scheme="http://aier02.com/categories/cs229n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>mixture of Gaussian</title>
    <link href="http://aier02.com/2018/12/16/mixture_of_Gau/"/>
    <id>http://aier02.com/2018/12/16/mixture_of_Gau/</id>
    <published>2018-12-16T12:39:57.674Z</published>
    <updated>2018-12-16T15:00:20.365Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p><img src="/images/181216/GAUSSIAN_MIXTURE/IMG_3717.jpg" alt=""></p><p><img src="/images/181216/GAUSSIAN_MIXTURE/IMG_3718.jpg" alt=""></p><p><img src="/images/181216/GAUSSIAN_MIXTURE/IMG_3719.jpg" alt=""></p><p><img src="/images/181216/GAUSSIAN_MIXTURE/IMG_3720.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;img src=&quot;/images/181216/GAUSSIAN_MIXTURE/IMG_3717.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;i
      
    
    </summary>
    
      <category term="cs229n" scheme="http://aier02.com/categories/cs229n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>exception maximization</title>
    <link href="http://aier02.com/2018/12/16/exception_maximization/"/>
    <id>http://aier02.com/2018/12/16/exception_maximization/</id>
    <published>2018-12-16T12:39:07.498Z</published>
    <updated>2018-12-16T15:03:43.711Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p><img src="/images/181216/EM/IMG_3714.jpg" alt=""></p><p><img src="/images/181216/EM/IMG_3715.jpg" alt=""></p><p><img src="/images/181216/EM/IMG_3716.jpg" alt=""></p><p><img src="/images/181216/EM/IMG_3717.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;img src=&quot;/images/181216/EM/IMG_3714.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/image
      
    
    </summary>
    
      <category term="cs229n" scheme="http://aier02.com/categories/cs229n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>bayesian statistic</title>
    <link href="http://aier02.com/2018/12/16/bayesian_statist/"/>
    <id>http://aier02.com/2018/12/16/bayesian_statist/</id>
    <published>2018-12-16T12:37:36.576Z</published>
    <updated>2018-12-16T15:00:54.868Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p><img src="/images/181216/BAYESIAN_STATISTIC/IMG_3711.jpg" alt=""></p><p><img src="/images/181216/BAYESIAN_STATISTIC/IMG_3712.jpg" alt=""></p><p><img src="/images/181216/BAYESIAN_STATISTIC/IMG_3713.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;img src=&quot;/images/181216/BAYESIAN_STATISTIC/IMG_3711.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;
      
    
    </summary>
    
      <category term="cs229n" scheme="http://aier02.com/categories/cs229n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>feature selection</title>
    <link href="http://aier02.com/2018/12/16/feature_selection/"/>
    <id>http://aier02.com/2018/12/16/feature_selection/</id>
    <published>2018-12-16T12:36:22.956Z</published>
    <updated>2018-12-16T15:01:29.782Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p><img src="/images/181216/FEATURE_SELECTION/IMG_3709.jpg" alt=""></p><p><img src="/images/181216/FEATURE_SELECTION/IMG_3710.jpg" alt=""></p><p><img src="/images/181216/FEATURE_SELECTION/IMG_3711.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;img src=&quot;/images/181216/FEATURE_SELECTION/IMG_3709.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;
      
    
    </summary>
    
      <category term="cs229n" scheme="http://aier02.com/categories/cs229n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>empirial risk minimization</title>
    <link href="http://aier02.com/2018/12/16/empirial_risk_minimize/"/>
    <id>http://aier02.com/2018/12/16/empirial_risk_minimize/</id>
    <published>2018-12-16T12:35:49.024Z</published>
    <updated>2018-12-16T15:02:02.606Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p><img src="/images/181216/ERM/IMG_3706.jpg" alt=""></p><p><img src="/images/181216/ERM/IMG_3707.jpg" alt=""></p><p><img src="/images/181216/ERM/IMG_3708.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;img src=&quot;/images/181216/ERM/IMG_3706.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/imag
      
    
    </summary>
    
      <category term="cs229n" scheme="http://aier02.com/categories/cs229n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>kernel method</title>
    <link href="http://aier02.com/2018/12/16/kernel_method/"/>
    <id>http://aier02.com/2018/12/16/kernel_method/</id>
    <published>2018-12-16T12:34:21.064Z</published>
    <updated>2018-12-16T15:05:13.356Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p><img src="/images/181216/KERNEL_METHOD/IMG_3703.jpg" alt=""></p><p><img src="/images/181216/KERNEL_METHOD/IMG_3704.jpg" alt=""></p><p><img src="/images/181216/KERNEL_METHOD/IMG_3705.jpg" alt=""></p><p><img src="/images/181216/KERNEL_METHOD/IMG_3706.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;img src=&quot;/images/181216/KERNEL_METHOD/IMG_3703.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;img 
      
    
    </summary>
    
      <category term="cs229n" scheme="http://aier02.com/categories/cs229n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>Lagrange multiplian</title>
    <link href="http://aier02.com/2018/12/16/Lagrange_multiplian/"/>
    <id>http://aier02.com/2018/12/16/Lagrange_multiplian/</id>
    <published>2018-12-16T12:32:02.432Z</published>
    <updated>2018-12-16T15:05:31.241Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p><img src="/images/181216/LAGRANGE_MULTIPLIAN/IMG_3700.jpg" alt=""></p><p><img src="/images/181216/LAGRANGE_MULTIPLIAN/IMG_3701.jpg" alt=""></p><p><img src="/images/181216/LAGRANGE_MULTIPLIAN/IMG_3702.jpg" alt=""></p><p><img src="/images/181216/LAGRANGE_MULTIPLIAN/IMG_3703.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;img src=&quot;/images/181216/LAGRANGE_MULTIPLIAN/IMG_3700.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p
      
    
    </summary>
    
      <category term="cs229n" scheme="http://aier02.com/categories/cs229n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>naive Bayes</title>
    <link href="http://aier02.com/2018/12/16/naive_Bayse/"/>
    <id>http://aier02.com/2018/12/16/naive_Bayse/</id>
    <published>2018-12-16T12:29:24.745Z</published>
    <updated>2018-12-16T15:06:47.058Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p><img src="/images/181216/NAIVE_BAYSE/IMG_3698.jpg" alt=""></p><p><img src="/images/181216/NAIVE_BAYSE/IMG_3699.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;img src=&quot;/images/181216/NAIVE_BAYSE/IMG_3698.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;img sr
      
    
    </summary>
    
      <category term="cs229n" scheme="http://aier02.com/categories/cs229n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>generative learning algorithms</title>
    <link href="http://aier02.com/2018/12/16/generative_learning_algorithms/"/>
    <id>http://aier02.com/2018/12/16/generative_learning_algorithms/</id>
    <published>2018-12-16T12:27:05.959Z</published>
    <updated>2018-12-16T12:27:49.137Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --><p><img src="/images/181216/GLA/IMG_3696.jpeg" alt=""></p><p><img src="/images/181216/GLA/IMG_3697.jpeg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Nov 07 2021 19:08:53 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;img src=&quot;/images/181216/GLA/IMG_3696.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;&lt;p&gt;&lt;img src=&quot;/ima
      
    
    </summary>
    
      <category term="cs229n" scheme="http://aier02.com/categories/cs229n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
</feed>
