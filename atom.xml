<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Aier02</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://aier02.com/"/>
  <updated>2018-12-09T07:05:10.410Z</updated>
  <id>http://aier02.com/</id>
  
  <author>
    <name>易安明</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>leetcode-linked-list</title>
    <link href="http://aier02.com/2018/11/23/leetcode_linked_list/"/>
    <id>http://aier02.com/2018/11/23/leetcode_linked_list/</id>
    <published>2018-11-23T02:00:17.093Z</published>
    <updated>2018-12-09T07:05:10.410Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h3 id="2-add-two-numbers两个链表的对应节点值相加"><a class="markdownIt-Anchor" href="#2-add-two-numbers两个链表的对应节点值相加"></a> 2. Add Two Numbers（两个链表的对应节点值相加）</h3><p>You are given two <strong>non-empty</strong> linked lists representing two non-negative integers. The digits are stored in <strong>reverse order</strong> and each of their nodes contain a single digit. Add the two numbers and return it as a linked list.</p><p>You may assume the two numbers do not contain any leading zero, except the number 0 itself.</p><p><strong>Example:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)</span><br><span class="line">Output: 7 -&gt; 0 -&gt; 8</span><br><span class="line">Explanation: <span class="number">342</span> + <span class="number">465</span> = <span class="number">807.</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line"><span class="comment"># @return a ListNode</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">addTwoNumbers</span><span class="params">(self, l1, l2)</span>:</span></span><br><span class="line">        <span class="comment">#串行进位加法器</span></span><br><span class="line">        carry=<span class="number">0</span><span class="comment">#记录进位</span></span><br><span class="line">        res=n=ListNode(<span class="number">0</span>)<span class="comment">#创建结果节点</span></span><br><span class="line">        <span class="comment">#依次将l1和l2对应节点的val进行相加，每次加完后将指针往前移一位</span></span><br><span class="line">        <span class="keyword">while</span> l1 <span class="keyword">or</span> l2 <span class="keyword">or</span> carry:</span><br><span class="line">            <span class="comment">#每次循环必须得先初始化两个节点的值为0，再根据节点的情况进行赋值</span></span><br><span class="line">            v1=v2=<span class="number">0</span></span><br><span class="line">            <span class="keyword">if</span> l1:</span><br><span class="line">                v1=l1.val</span><br><span class="line">                l1=l1.next</span><br><span class="line">            <span class="keyword">if</span> l2:</span><br><span class="line">                v2=l2.val</span><br><span class="line">                l2=l2.next</span><br><span class="line">            <span class="comment">#使用divmod()实现求商和余数，从而得到进位和本位和</span></span><br><span class="line">            carry,val=divmod(v1+v2+carry,<span class="number">10</span>)</span><br><span class="line">            <span class="comment">#创建对应的节点</span></span><br><span class="line">            n.next=ListNode(val)</span><br><span class="line">            <span class="comment">#指定下一个节点</span></span><br><span class="line">            n=n.next</span><br><span class="line">        <span class="keyword">return</span> res.next</span><br></pre></td></tr></table></figure><h3 id="19-remove-nth-node-from-end-of-list移除从后往前数的第n个节点"><a class="markdownIt-Anchor" href="#19-remove-nth-node-from-end-of-list移除从后往前数的第n个节点"></a> 19. Remove Nth Node From End of List（移除从后往前数的第n个节点）</h3><p>Given a linked list, remove the <em>n</em>-th node from the end of list and return its head.</p><p><strong>Example:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Given linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5, and n = 2.</span><br><span class="line"></span><br><span class="line">After removing the second node from the end, the linked list becomes 1-&gt;2-&gt;3-&gt;5.</span><br></pre></td></tr></table></figure><p><strong>Note:</strong></p><p>Given <em>n</em> will always be valid.</p><p><strong>Follow up:</strong></p><p>Could you do this in one pass?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeNthFromEnd</span><span class="params">(self, head, n)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type head: ListNode</span></span><br><span class="line"><span class="string">        :type n: int</span></span><br><span class="line"><span class="string">        :rtype: ListNode</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#找到从头开始搜索的要移除的节点的位置，记录为t</span></span><br><span class="line">        l=<span class="number">1</span></span><br><span class="line">        temp=head.next</span><br><span class="line">        <span class="keyword">while</span> temp!=<span class="keyword">None</span>:</span><br><span class="line">            l+=<span class="number">1</span></span><br><span class="line">            temp=temp.next</span><br><span class="line">        t=l-n</span><br><span class="line">        <span class="keyword">if</span> t==<span class="number">0</span>:</span><br><span class="line">            head=head.next</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            temp=head</span><br><span class="line">            j=<span class="number">1</span></span><br><span class="line">            <span class="keyword">while</span> j&lt;t:</span><br><span class="line">                temp=temp.next</span><br><span class="line">                j+=<span class="number">1</span></span><br><span class="line">            temp.next=temp.next.next</span><br><span class="line">                </span><br><span class="line">        <span class="keyword">return</span> head</span><br></pre></td></tr></table></figure><h3 id="23-merge-k-sorted-lists合并k个已经排好序的链表"><a class="markdownIt-Anchor" href="#23-merge-k-sorted-lists合并k个已经排好序的链表"></a> 23. Merge k Sorted Lists（合并k个已经排好序的链表）</h3><p>Merge <em>k</em> sorted linked lists and return it as one sorted list. Analyze and describe its complexity.</p><p><strong>Example:</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Input:</span><br><span class="line">[</span><br><span class="line">  1-&gt;4-&gt;5,</span><br><span class="line">  1-&gt;3-&gt;4,</span><br><span class="line">  2-&gt;6</span><br><span class="line">]</span><br><span class="line">Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6</span><br></pre></td></tr></table></figure><p>比较直接的做法是使用经典的合并算法，这里的做法是直接读取所有节点值，然后统一进行排序后以链表的形式进行表示。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mergeKLists</span><span class="params">(self, lists)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type lists: List[ListNode]</span></span><br><span class="line"><span class="string">        :rtype: ListNode</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ls=[]</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> lists:</span><br><span class="line">            <span class="keyword">while</span> t!=<span class="keyword">None</span>:</span><br><span class="line">                ls.append(t.val)</span><br><span class="line">                t=t.next</span><br><span class="line">        ls.sort()</span><br><span class="line">        head=fhead=ListNode(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(ls)):</span><br><span class="line">            head.next=ListNode(ls[i])</span><br><span class="line">            head=head.next</span><br><span class="line">        <span class="keyword">return</span> fhead.next</span><br></pre></td></tr></table></figure><h3 id="25-reverse-nodes-in-k-group"><a class="markdownIt-Anchor" href="#25-reverse-nodes-in-k-group"></a> 25. Reverse Nodes in k-Group</h3><p>Given a linked list, reverse the nodes of a linked list <em>k</em> at a time and return its modified list.</p><p><em>k</em> is a positive integer and is less than or equal to the length of the linked list. If the number of nodes is not a multiple of <em>k</em> then left-out nodes in the end should remain as it is.</p><p><strong>Example:</strong></p><p>Given this linked list: <code>1-&gt;2-&gt;3-&gt;4-&gt;5</code></p><p>For <em>k</em> = 2, you should return: <code>2-&gt;1-&gt;4-&gt;3-&gt;5</code></p><p>For <em>k</em> = 3, you should return: <code>3-&gt;2-&gt;1-&gt;4-&gt;5</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseKGroup</span><span class="params">(self, head, k)</span>:</span></span><br><span class="line">        count,node=<span class="number">0</span>,head</span><br><span class="line">        <span class="keyword">while</span> node <span class="keyword">and</span> count&lt;k:</span><br><span class="line">            node=node.next</span><br><span class="line">            count+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> count&lt;k:<span class="keyword">return</span> head</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment">#翻转后返回的是该段翻转之后的头节点prev，和下一个需要翻转的部分的头节点temp</span></span><br><span class="line">            temp,prev=self.reverse(head,k)</span><br><span class="line">            <span class="comment">#本段翻转后，原来的head成了尾部，需要和下一段的头进行对接，故每次都要返回翻转之后的头prev</span></span><br><span class="line">            head.next=self.reverseKGroup(temp,k)</span><br><span class="line">            <span class="keyword">return</span> prev</span><br><span class="line">    <span class="comment">#每次翻转后要保留断开位置的前一个节点和后一个节点</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverse</span><span class="params">(self,head,k)</span>:</span></span><br><span class="line">        prev,cur,nxt=<span class="keyword">None</span>,head,head</span><br><span class="line">        <span class="keyword">while</span> k&gt;<span class="number">0</span>:</span><br><span class="line">            nxt=cur.next</span><br><span class="line">            cur.next=prev</span><br><span class="line">            prev=cur</span><br><span class="line">            cur=nxt</span><br><span class="line">            k-=<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> (cur,prev)</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;2-add-two-numbers两个链表的对应节点值相加&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#
      
    
    </summary>
    
      <category term="algorithms" scheme="http://aier02.com/categories/algorithms/"/>
    
    
      <category term="linked list" scheme="http://aier02.com/tags/linked-list/"/>
    
      <category term="leetcode" scheme="http://aier02.com/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>leetcode_hash_table</title>
    <link href="http://aier02.com/2018/11/23/leetcode_hash_table/"/>
    <id>http://aier02.com/2018/11/23/leetcode_hash_table/</id>
    <published>2018-11-23T01:36:36.136Z</published>
    <updated>2018-12-09T07:04:00.228Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><p>hash table是指对目标值进行一定变换后映射到表中的某个位置（或者某个值），可以用list或者dict实现这个哈希表</p><h3 id="1-two-sum"><a class="markdownIt-Anchor" href="#1-two-sum"></a> 1. Two Sum</h3><p>Given an array of integers, return <strong>indices</strong> of the two numbers such that they add up to a specific target.</p><p>You may assume that each input would have <strong>exactly</strong> one solution, and you may not use the <em>same</em> element twice.</p><p><strong>Example:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Given nums = [2, 7, 11, 15], target = 9,</span><br><span class="line"></span><br><span class="line">Because nums[0] + nums[1] = 2 + 7 = 9,</span><br><span class="line">return [0, 1].</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">twoSum</span><span class="params">(self, nums, target)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(nums)&lt;=<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sum_dict=&#123;&#125;</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">                <span class="keyword">if</span> nums[i] <span class="keyword">in</span> sum_dict:</span><br><span class="line">                    <span class="keyword">return</span> [sum_dict[nums[i]],i]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="comment">#用key值保存索引值，key对应的value为差值</span></span><br><span class="line">                    sum_dict[target-nums[i]]=i</span><br></pre></td></tr></table></figure><p>这个sum_dict的关键在于他的转换是将数组中每个值与target的差值保存为key，查表时只要查找是否数组中存在这样的差值即可找到相应的两个和为target的元素。</p><h3 id="3-longest-substring-without-repeating-characters"><a class="markdownIt-Anchor" href="#3-longest-substring-without-repeating-characters"></a> 3. Longest Substring Without Repeating Characters</h3><p>Given a string, find the length of the <strong>longest substring</strong> without repeating characters.</p><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input: <span class="string">"abcabcbb"</span></span><br><span class="line">Output: <span class="number">3</span> </span><br><span class="line">Explanation: The answer <span class="keyword">is</span> <span class="string">"abc"</span>, <span class="keyword">with</span> the length of <span class="number">3.</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        dic, res, start, = &#123;&#125;, <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, ch <span class="keyword">in</span> enumerate(s):</span><br><span class="line">            <span class="comment"># when char already in dictionary</span></span><br><span class="line">            <span class="keyword">if</span> ch <span class="keyword">in</span> dic:</span><br><span class="line">                <span class="comment"># check length from start of string to index</span></span><br><span class="line">                res = max(res, i-start)</span><br><span class="line">                <span class="comment"># update start of string index to the next index</span></span><br><span class="line">                start = max(start, dic[ch]+<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># add/update char to/of dictionary </span></span><br><span class="line">            dic[ch] = i</span><br><span class="line">        <span class="comment"># answer is either in the begining/middle OR some mid to the end of string</span></span><br><span class="line">        <span class="keyword">return</span> max(res, len(s)-start)</span><br></pre></td></tr></table></figure><p>用dict记录每个出现过的字母和他的索引，若遇到重复的字母则更新res和重新记录的开始位置。</p><h3 id="18-4sum"><a class="markdownIt-Anchor" href="#18-4sum"></a> 18. 4Sum</h3><p>Given an array <code>nums</code> of <em>n</em> integers and an integer <code>target</code>, are there elements <em>a</em>, <em>b</em>, <em>c</em>, and <em>d</em> in <code>nums</code> such that <em>a</em> + <em>b</em> + <em>c</em> + <em>d</em> = <code>target</code>? Find all unique quadruplets in the array which gives the sum of <code>target</code>.</p><p><strong>Note:</strong></p><p>The solution set must not contain duplicate quadruplets.</p><p><strong>Example:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Given array nums = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>, <span class="number">0</span>, <span class="number">-2</span>, <span class="number">2</span>], <span class="keyword">and</span> target = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">A solution set <span class="keyword">is</span>:</span><br><span class="line">[</span><br><span class="line">  [<span class="number">-1</span>,  <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">  [<span class="number">-2</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">  [<span class="number">-2</span>,  <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>]</span><br><span class="line">]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fourSum</span><span class="params">(self, nums, target)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type target: int</span></span><br><span class="line"><span class="string">        :rtype: List[List[int]]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#先进行排序，这样做的好处是后期对两个数字进行搜索时可以更加方便的移动l和r指针</span></span><br><span class="line">        nums.sort()</span><br><span class="line">        results=[]</span><br><span class="line">        self.findNsum(nums,target,<span class="number">4</span>,[],results)</span><br><span class="line">        <span class="keyword">return</span> results</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findNsum</span><span class="params">(self,nums,target,N,result,results)</span>:</span></span><br><span class="line">        <span class="comment">#检查在剩余的nums中查找target的可能性</span></span><br><span class="line">        <span class="keyword">if</span> len(nums)&lt;N <span class="keyword">or</span> N&lt;<span class="number">2</span> <span class="keyword">or</span> target&lt;nums[<span class="number">0</span>]*N <span class="keyword">or</span> target&gt;nums[<span class="number">-1</span>]*N:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">elif</span> N==<span class="number">2</span>:</span><br><span class="line">            <span class="comment">#在数组中首尾进行搜索和为target的两个位置</span></span><br><span class="line">            l,r=<span class="number">0</span>,len(nums)<span class="number">-1</span></span><br><span class="line">            <span class="keyword">while</span> l&lt;r:</span><br><span class="line">                <span class="comment">#找到目标值</span></span><br><span class="line">                <span class="keyword">if</span> target==nums[l]+nums[r]:</span><br><span class="line">                    results.append(result+[nums[l],nums[r]])</span><br><span class="line">                    l+=<span class="number">1</span></span><br><span class="line">                    <span class="comment">#当前后l指定的数字相同时，l+1，跳过这个重复的值</span></span><br><span class="line">                    <span class="keyword">while</span> l&lt;r <span class="keyword">and</span> nums[l<span class="number">-1</span>]==nums[l]:</span><br><span class="line">                        l+=<span class="number">1</span></span><br><span class="line">                <span class="comment">#两个指针对应的值和小于target，因为nums[r]已经最大，所以往前移动l</span></span><br><span class="line">                <span class="keyword">elif</span> nums[l]+nums[r]&lt;target:</span><br><span class="line">                    l+=<span class="number">1</span></span><br><span class="line">               <span class="comment">#往后移动r</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    r-=<span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">           <span class="comment">#深度优先搜索，每次先确定一个值，在剩余的nums中寻找修改后的target</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)-N+<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">if</span> i==<span class="number">0</span> <span class="keyword">or</span> (i&gt;<span class="number">0</span> <span class="keyword">and</span> nums[i<span class="number">-1</span>]!=nums[i]):</span><br><span class="line">                    self.findNsum(nums[i+<span class="number">1</span>:],target-nums[i],N<span class="number">-1</span>,result+[nums[i]],results)</span><br></pre></td></tr></table></figure><p>先对nums进行排序，然后迭代的每次保留一个数字然后往后进行搜索，直到只剩下两个数字时使用两个指针从首尾进行搜索，特别注意相同数字的处理，当两个数字相同时，l要继续往前+1。</p><h3 id="30-substring-with-concatenation-of-all-words"><a class="markdownIt-Anchor" href="#30-substring-with-concatenation-of-all-words"></a> 30. Substring with Concatenation of All Words</h3><p>You are given a string, <strong>s</strong>, and a list of words, <strong>words</strong>, that are all of the same length. Find all starting indices of substring(s) in <strong>s</strong> that is a concatenation of each word in <strong>words</strong> exactly once and without any intervening characters.</p><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Input:</span><br><span class="line">  s = <span class="string">"barfoothefoobarman"</span>,</span><br><span class="line">  words = [<span class="string">"foo"</span>,<span class="string">"bar"</span>]</span><br><span class="line">Output: [<span class="number">0</span>,<span class="number">9</span>]</span><br><span class="line">Explanation: Substrings starting at index <span class="number">0</span> <span class="keyword">and</span> <span class="number">9</span> are <span class="string">"barfoor"</span> <span class="keyword">and</span> <span class="string">"foobar"</span> respectively.</span><br><span class="line">The output order does <span class="keyword">not</span> matter, returning [<span class="number">9</span>,<span class="number">0</span>] <span class="keyword">is</span> fine too.</span><br></pre></td></tr></table></figure><p><strong>Example 2:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Input:</span><br><span class="line">  s = <span class="string">"wordgoodstudentgoodword"</span>,</span><br><span class="line">  words = [<span class="string">"word"</span>,<span class="string">"student"</span>]</span><br><span class="line">Output: []</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">findSubstring</span><span class="params">(self, s, words)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> len(words) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        <span class="comment"># initialize d, l, ans</span></span><br><span class="line">        l = len(words[<span class="number">0</span>])</span><br><span class="line">        d = &#123;&#125;</span><br><span class="line">        <span class="comment">#用字典d记录words中出现的各个word的次数</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">            <span class="keyword">if</span> w <span class="keyword">in</span> d:</span><br><span class="line">                d[w] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                d[w] = <span class="number">1</span></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        ans = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># sliding window(s)</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(l):</span><br><span class="line">        <span class="comment">#left指针用于指示左端第一个有效字符的起始位置</span></span><br><span class="line">        <span class="comment">#count用于记录当前有效的word数目</span></span><br><span class="line">            left = k</span><br><span class="line">            subd = &#123;&#125;</span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line">            <span class="comment">#处理开始索引k，k必定在一个l的范围内</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k, len(s)-l+<span class="number">1</span>, l):</span><br><span class="line">                tword = s[j:j+l]</span><br><span class="line">                <span class="comment"># valid word，即该字符串出现在words中</span></span><br><span class="line">                <span class="keyword">if</span> tword <span class="keyword">in</span> d:</span><br><span class="line">                    <span class="keyword">if</span> tword <span class="keyword">in</span> subd:</span><br><span class="line">                        subd[tword] += <span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        subd[tword] = <span class="number">1</span></span><br><span class="line">                    count += <span class="number">1</span></span><br><span class="line">                    <span class="comment">#纠正某个单词的数目，若超过了words中的数目则需要进行前向调整</span></span><br><span class="line">                    <span class="keyword">while</span> subd[tword] &gt; d[tword]:</span><br><span class="line">                        subd[s[left:left+l]] -= <span class="number">1</span></span><br><span class="line">                        left += l</span><br><span class="line">                        count -= <span class="number">1</span></span><br><span class="line">                    <span class="keyword">if</span> count == len(words):</span><br><span class="line">                        ans.append(left)</span><br><span class="line">                <span class="comment"># not valid</span></span><br><span class="line">                <span class="comment">#遇到无效的单词时，需要重新计算left的位置，默认下一个单词有效，同时需要重新统计字典</span></span><br><span class="line">                <span class="comment">#这种解法关键在于使用两个字典分别统计words中各个单词出现的次数，当统计到的有效单词数目一致时则认为是有效的连续字串</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    left = j + l</span><br><span class="line">                    subd = &#123;&#125;</span><br><span class="line">                    count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><p>这里的hash tabel为一个字典，记录了words中各个单词的数目。</p><h3 id="36-valid-sudoku"><a class="markdownIt-Anchor" href="#36-valid-sudoku"></a> 36. Valid Sudoku</h3><p>Determine if a 9x9 Sudoku board is valid. Only the filled cells need to be validated <strong>according to the following rules</strong>:</p><ol><li>Each row must contain the digits <code>1-9</code> without repetition.</li><li>Each column must contain the digits <code>1-9</code> without repetition.</li><li>Each of the 9 <code>3x3</code> sub-boxes of the grid must contain the digits <code>1-9</code> without repetition.</li></ol><p><strong>Example 1:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Input:</span><br><span class="line">[</span><br><span class="line">  [<span class="string">"5"</span>,<span class="string">"3"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"7"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>],</span><br><span class="line">  [<span class="string">"6"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"1"</span>,<span class="string">"9"</span>,<span class="string">"5"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>],</span><br><span class="line">  [<span class="string">"."</span>,<span class="string">"9"</span>,<span class="string">"8"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"6"</span>,<span class="string">"."</span>],</span><br><span class="line">  [<span class="string">"8"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"6"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"3"</span>],</span><br><span class="line">  [<span class="string">"4"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"8"</span>,<span class="string">"."</span>,<span class="string">"3"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"1"</span>],</span><br><span class="line">  [<span class="string">"7"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"2"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"6"</span>],</span><br><span class="line">  [<span class="string">"."</span>,<span class="string">"6"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"2"</span>,<span class="string">"8"</span>,<span class="string">"."</span>],</span><br><span class="line">  [<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"4"</span>,<span class="string">"1"</span>,<span class="string">"9"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"5"</span>],</span><br><span class="line">  [<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"8"</span>,<span class="string">"."</span>,<span class="string">"."</span>,<span class="string">"7"</span>,<span class="string">"9"</span>]</span><br><span class="line">]</span><br><span class="line">Output: true</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isValidSudoku</span><span class="params">(self, board)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type board: List[List[str]]</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#set中存储的是不重复的元素</span></span><br><span class="line">        big = set()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">9</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,<span class="number">9</span>):</span><br><span class="line">                <span class="keyword">if</span> board[i][j]!=<span class="string">'.'</span>:</span><br><span class="line">                    cur = board[i][j]</span><br><span class="line">                    <span class="comment">#记录board中数字出现的列，行，还有所在单独的3x3的区域的位置，9x9分为了9个区域，用两个索引可以表示3x3在9x9的位置</span></span><br><span class="line">                    <span class="keyword">if</span> (i,cur) <span class="keyword">in</span> big <span class="keyword">or</span> (cur,j) <span class="keyword">in</span> big <span class="keyword">or</span> (int(i/<span class="number">3</span>),int(j/<span class="number">3</span>),cur) <span class="keyword">in</span> big:</span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">                    <span class="comment">#一行中是否有重复数字</span></span><br><span class="line">                    big.add((i,cur))</span><br><span class="line">                    <span class="comment">#一列中是否有重复数字</span></span><br><span class="line">                    big.add((cur,j))</span><br><span class="line">                    big.add((int(i/<span class="number">3</span>),int(j/<span class="number">3</span>),cur))</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>关键在于如何存储表示第i行出现了某个数字，第j列出现了某个数字，第n个区域出现了某个数字。通过行和列确定区域所在的位置。</p><h3 id="37-sudoku-solver"><a class="markdownIt-Anchor" href="#37-sudoku-solver"></a> 37. Sudoku Solver</h3><p>Write a program to solve a Sudoku puzzle by filling the empty cells.</p><p>A sudoku solution must satisfy <strong>all of the following rules</strong>:</p><ol><li>Each of the digits <code>1-9</code> must occur exactly once in each row.</li><li>Each of the digits <code>1-9</code> must occur exactly once in each column.</li><li>Each of the the digits <code>1-9</code> must occur exactly once in each of the 9 <code>3x3</code> sub-boxes of the grid.</li></ol><p>Empty cells are indicated by the character <code>'.'</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment">#检查在(row,col)位置插入k值后是否还是有效的数独板</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isValidSudoku</span><span class="params">(self, board, row, col, k)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">            <span class="keyword">if</span> board[row][i] != <span class="string">'.'</span> <span class="keyword">and</span> board[row][i] == k:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">            <span class="keyword">if</span> board[j][col] != <span class="string">'.'</span> <span class="keyword">and</span> board[j][col] == k:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        r1 = (row // <span class="number">3</span>) * <span class="number">3</span></span><br><span class="line">        c1 = (col // <span class="number">3</span>) * <span class="number">3</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(r1, r1+<span class="number">3</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(c1, c1+<span class="number">3</span>):</span><br><span class="line">                <span class="keyword">if</span> board[i][j] != <span class="string">'.'</span> <span class="keyword">and</span> board[i][j] == k:</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">solveSudoku</span><span class="params">(self, board)</span>:</span></span><br><span class="line">        self.board = board</span><br><span class="line">        self.solve(board)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">solve</span><span class="params">(self, board)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type board: List[List[str]]</span></span><br><span class="line"><span class="string">        :rtype: void Do not return anything, modify board in-place instead.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">9</span>):</span><br><span class="line">                <span class="comment">#找到空缺的位置</span></span><br><span class="line">                <span class="keyword">if</span> board[i][j] == <span class="string">'.'</span>:</span><br><span class="line">                    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">10</span>):</span><br><span class="line">                        <span class="comment">#检查在（i，j）插入k之后是否有效</span></span><br><span class="line">                        <span class="keyword">if</span> self.isValidSudoku(board, i, j, str(k)):</span><br><span class="line">                            <span class="comment">#令（i，j）为k</span></span><br><span class="line">                            board[i][j] = str(k)</span><br><span class="line">                            <span class="comment">#递归检查是否能solve</span></span><br><span class="line">                            <span class="keyword">if</span> self.solve(board):</span><br><span class="line">                                <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line">                        <span class="comment">#无效则回溯</span></span><br><span class="line">                        board[i][j] = <span class="string">'.'</span></span><br><span class="line">                    <span class="comment">#进行了所有尝试都无效则返回false</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="comment">#前面没有返回flase则成功</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br></pre></td></tr></table></figure><p>每次尝试在空缺位置插入一个值，检查插入后board是否还是有效的，无效则进行回溯，尝试下一个值。</p><h3 id="49-group-anagrams"><a class="markdownIt-Anchor" href="#49-group-anagrams"></a> 49. Group Anagrams</h3><p>Given an array of strings, group anagrams together.</p><p><strong>Example:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Input: [<span class="string">"eat"</span>, <span class="string">"tea"</span>, <span class="string">"tan"</span>, <span class="string">"ate"</span>, <span class="string">"nat"</span>, <span class="string">"bat"</span>],</span><br><span class="line">Output:</span><br><span class="line">[</span><br><span class="line">  [<span class="string">"ate"</span>,<span class="string">"eat"</span>,<span class="string">"tea"</span>],</span><br><span class="line">  [<span class="string">"nat"</span>,<span class="string">"tan"</span>],</span><br><span class="line">  [<span class="string">"bat"</span>]</span><br><span class="line">]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">groupAnagrams</span><span class="params">(self, strs)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type strs: List[str]</span></span><br><span class="line"><span class="string">        :rtype: List[List[str]]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#字符串匹配问题，如何实现快速匹配而不是一个一个字母匹配</span></span><br><span class="line">        <span class="comment">#构造一个hash函数，使得含有相同字母的str都映射到统一个位置</span></span><br><span class="line">        <span class="comment">#方法一是进行排序，没有构造hash function</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        dic=&#123;&#125;</span></span><br><span class="line"><span class="string">        for s in strs:</span></span><br><span class="line"><span class="string">            sp=''.join(sorted(s))</span></span><br><span class="line"><span class="string">            if sp in dic:</span></span><br><span class="line"><span class="string">                dic[sp].append(s)</span></span><br><span class="line"><span class="string">            else:</span></span><br><span class="line"><span class="string">                dic[sp]=[s]</span></span><br><span class="line"><span class="string">        return list(dic.values())</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#方法二是构造一个tuple记录字母出现的位置</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">convert</span><span class="params">(s)</span>:</span></span><br><span class="line">            res=[<span class="number">0</span>]*<span class="number">26</span></span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> s:</span><br><span class="line">                res[ord(c)-ord(<span class="string">'a'</span>)]+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> tuple(res)</span><br><span class="line">        <span class="comment">#tuple对应的索引</span></span><br><span class="line">        dic=&#123;&#125;</span><br><span class="line">        <span class="comment">#记录结果</span></span><br><span class="line">        res=[]</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> strs:</span><br><span class="line">            t=convert(s)</span><br><span class="line">            <span class="keyword">if</span> t <span class="keyword">in</span> dic:</span><br><span class="line">                res[dic[t]].append(s)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                res.append([s])</span><br><span class="line">                dic[t]=len(res)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p>构造一个tuple表，使得字符串映射到字符位置固定的tuple中，这样做能使得相同字母构造的不同字符串能到映射到相同的位置。</p><p><strong>hash table的核心思想是使得具有某种规律的输入映射到相同的输出，在python 中用字典的形式存储这种key-value的对应关系，具体的关系要根据实际问题进行设计</strong></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;hash table是指对目标值进行一定变换后映射到表中的某个位置（或者某个值），可以用list或者dict实现这个哈希表&lt;/p&gt;&lt;h3 id=
      
    
    </summary>
    
      <category term="algorithms" scheme="http://aier02.com/categories/algorithms/"/>
    
    
      <category term="leetcode" scheme="http://aier02.com/tags/leetcode/"/>
    
      <category term="hash table" scheme="http://aier02.com/tags/hash-table/"/>
    
  </entry>
  
  <entry>
    <title>VGG</title>
    <link href="http://aier02.com/2018/11/12/VGG/"/>
    <id>http://aier02.com/2018/11/12/VGG/</id>
    <published>2018-11-12T03:28:24.262Z</published>
    <updated>2018-11-19T11:38:09.607Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><p><a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">read paper online</a></p><h3 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h3><p>主要工作是观察加深网络深度在大型图像分类问题上的效果，使用3x3的卷积核来加深CNNs到16-19层，这种做法使得该模型在ILSVCR2014取得了第一和第二的成绩。</p><h3 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1 INTRODUCTION</h3><p>介绍了今年convnet取得的成就，越来越多人尝试从不同方面改进convnet取实现更高的分类准确率；有的修改感受野的大小，使用小的步长，或者从训练和测试的时候使用图片的大小和数量入手。而本文关注的是convnet的深度，所有卷积层使用的filter均为3x3，增加了整体的深度。结果取得了当时在ILSVR中最好的结果，而且在其他数据集中都是最好的。最后说明了本文各个section和appendix中的主要内容。</p><h3 id="2-convnet-configurations"><a class="markdownIt-Anchor" href="#2-convnet-configurations"></a> 2 CONVNET CONFIGURATIONS</h3><p>所有卷积层的配置使用同一个规则。</p><h4 id="21-architecture"><a class="markdownIt-Anchor" href="#21-architecture"></a> 2.1 ARCHITECTURE</h4><p>固定大小为224x224的RGB图片，预处理是每张图片减去整个训练集图像的RGB均值，卷积核为3x3，另外1x1的卷积核可以看作是输入channel的线性变换，卷积步长为1，padding为1，空间池化为在某些卷积层后使用最大池化层2x2,步长为2，但不是所有卷积层都使用；所有卷积层后接3层fc，前两层为4096个channel的fc，第三层为1000个channel；整个网络的最后一层为sofmax。</p><p>所有隐藏层都使用Relus作为激活函数，只有一层使用了LRN。</p><h4 id="22-configurations"><a class="markdownIt-Anchor" href="#22-configurations"></a> 2.2 CONFIGURATIONS</h4><p>卷积层中的channel从64每次经过一个pooling层则翻倍最后到512channel。</p><h4 id="23-discussion"><a class="markdownIt-Anchor" href="#23-discussion"></a> 2.3 DISCUSSION</h4><p>和目前为止的模型不同之处在于在开始的conv layer不使用大的感受野，而是在整个网络中都只使用3x3，显然两个这样的卷积层堆叠起来的感受野为5x5，中间不经过pooling层，同理3层堆叠为7x7，这样做的作用是一方面把使用3个RLUs替代一个，使得决策函数更加有影响力；另一方面3个3x3的参数相较于一个7x7的参数更少，前者为3(3<sup>2C</sup>2),即27C<sup>2,后者为7</sup>2C<sup>2=49C</sup>2;</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
      <category term="paper" scheme="http://aier02.com/categories/paper/"/>
    
    
      <category term="cv" scheme="http://aier02.com/tags/cv/"/>
    
      <category term="paper reading" scheme="http://aier02.com/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>AlexNet</title>
    <link href="http://aier02.com/2018/10/30/AlexNet/"/>
    <id>http://aier02.com/2018/10/30/AlexNet/</id>
    <published>2018-10-30T02:56:58.917Z</published>
    <updated>2018-11-03T09:23:15.726Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><p><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">read paper online</a></p><h3 id="abstract"><a class="markdownIt-Anchor" href="#abstract"></a> Abstract</h3><ul><li>训练集和测试集均来自ImageNet</li><li>网络框架的结构为5层conv layer，接一个max pooling和3层fully connected layer</li><li>训练过程中使用的方法，高效的GPU代码和减少overfitting的dropout</li><li>项目达到的成就，当时最好的测试误差率</li></ul><h3 id="1-introduction"><a class="markdownIt-Anchor" href="#1-introduction"></a> 1 Introduction</h3><p>当时的分类方法主要是高效地运用机器学习方法，要提高分类的表现可以从增大数据集、学习更强的模型和减少overfitting入手。</p><ol><li>更大的数据集:ImageNet数据集拥有1500万张高清图片并且含有22000个种类</li><li>更好的模型:CNN模型能有效地处理大量的图片，他的参数相较于传统的前向神经网络要少，更加容易去训练。</li><li>更好的训练方法:在GPU上高效地实现二维卷积能有效地加快训练的过程。</li></ol><p>该paper特别的贡献，以及当时硬件的限制。</p><h3 id="2-the-dataset"><a class="markdownIt-Anchor" href="#2-the-dataset"></a> 2 The Dataset</h3><p>介绍具体使用的数据集的来源、图片大小、图片种类、图片数量,以及训练过程中加入的图像预处理。</p><p>paper中使用的pre-processing为图像下采样为256x256，pixel值减去整个训练集的中值，最后使用的是RGB格式的纯pixel值。</p><p>top1就是你预测的label取最后概率向量里面最大的那一个作为预测结果，你的预测结果中概率最大的那个类必须是正确类别才算预测正确。而top5就是最后概率向量最大的前五名中出现了正确概率即为预测正确。故top-1 error rate 和 top-5 error rate都是在该错误率定义下在test data set中的概率。</p><h3 id="3-the-architecture"><a class="markdownIt-Anchor" href="#3-the-architecture"></a> 3 The Architecture</h3><p>介绍使用的CNN框架中新颖的设计</p><h4 id="31-relu-nonlinearity"><a class="markdownIt-Anchor" href="#31-relu-nonlinearity"></a> 3.1 ReLU Nonlinearity</h4><p>训练时使用GD 算法,饱和的非线性函数如tanh(x) 和 sigmoid function (1 + e−x)−1 使得训练的时间要比不饱和的非线性函数f(x) = max(0,x),称为Rectified Linear Units要长。通过图例说明饱和的activation function比不饱和的收敛得更慢。</p><h4 id="32-training-on-multiple-gpus"><a class="markdownIt-Anchor" href="#32-training-on-multiple-gpus"></a> 3.2 Training on Multiple GPUs</h4><p>受限于硬件设备，将整个网络(神经元)平分到2个GPU中进行并行计算，GPU能够直接读写别的GPU的内存而不占用主机的内存;两个GPU的沟通只在特定的layer进行,沟通是指两者产生feature maps作为对方的输入,不沟通就是只用该GPU产生的feature maps作为下一层的输入。</p><h4 id="33-local-response-normalizationno-idea"><a class="markdownIt-Anchor" href="#33-local-response-normalizationno-idea"></a> 3.3 Local Response Normalization（no idea==）</h4><p>Relus不需要为了避免饱和而进行输入的规范化，</p><h4 id="34-overlapping-pooling"><a class="markdownIt-Anchor" href="#34-overlapping-pooling"></a> 3.4 Overlapping Pooling</h4><p>pooling unit的strke和size一致，为tradiitonal local pooling，strike小于size则会发生overlapping。</p><h4 id="35-overall-architecture"><a class="markdownIt-Anchor" href="#35-overall-architecture"></a> 3.5 Overall Architecture</h4><p>fully connected layers中的神经元是一个和feature map大小一致的filter，全连接层的目的是高度提取特征，将feature map浓缩为一个数字，用于后面的分类或者回归。在最后一个卷积层输出到全连接层中，将输入看作一个neuron表示一张feature map，所有输入都得连接到每一个全连接层的neuron中，该过程和普通的卷积层类似，不同点在于全连接层的每一个w与单个feature map大小一致而且每一个输出都为一个数字，而卷积层的大小自定而且输出为一张feature map。</p><h3 id="4-reducing-overfitting"><a class="markdownIt-Anchor" href="#4-reducing-overfitting"></a> 4 Reducing Overfitting</h3><p>整个网络中有六千万个参数，尽管从image到label的映射中存在10位二进制的约束，但是如此多的参数无可避免会存在过拟合。</p><h4 id="41-data-augmentation"><a class="markdownIt-Anchor" href="#41-data-augmentation"></a> 4.1 Data Augmentation</h4><p>一般先提及以往的方法，指出不足之处后提出自己的做法，突出自己的优势。</p><p>在GPU进行训练的过程中，利用CPU对还未训练的图像进行小量的改变，故可以说数据增强在计算代价上面是“免费的”。（CPU的计算代价远小于GPU）</p><ul><li>图像平移和水平翻转:从原图和翻转后的图像中(256x256)分别随机选取224x224的区域作为增强后的图像进行训练，这种做法虽然增加了训练集的大小，但是显然部分数据存在内部的依赖(那为啥还这么做呢？给出理由),实际情况是不使用这种增强的模式，训练会出现严重的overfitting，迫使使用小的网络，权衡之后这种方式还是有效的。在测试的时候，分别在原图和翻转后的图像定位选取5个子区域，四个角的224x224，还有中心位置的224x224，即一张原图产生了10张用于测试的图像,进行预测时即对这十张图片的softmax得到的值进行平均得到某个class的最大值为预测值。</li><li>改变训练图像的RGB通道强度，对训练集中的RGB像素进行PCA，然后对三个通道的值进行成比例的修改。(这里没看懂)</li></ul><h4 id="42-dropout"><a class="markdownIt-Anchor" href="#42-dropout"></a> 4.2 Dropout</h4><p>集成学习对于减少错误率非常有效，但是对于大型的网络而言该做法计算代价太大，dropout能同时兼顾两者，既能实现不同的网络，同时能降低训练成本；对于每一个隐藏层的neuron，以0.5的概率对他的输出进行归零，即该神经元在本次训练(包括fp和bp)均失效；每一个epoch均可能是不同的结构，这种做法会导致训练次数的增多(大致一倍)才使得模型收敛，在test的时候，根据概率论的知识，每一个神经元的输出要乘以0.5，才和训练时的设置保持一致；本文中的dropout在后两层的全连接层实现。</p><h3 id="5-details-of-learning"><a class="markdownIt-Anchor" href="#5-details-of-learning"></a> 5 Details of learning</h3><ul><li>训练过程中使用了SGD算法，一个batch为128张图片，momentum为0.9，weight decay为0.0005，实验发现这个0.0005对于减少错误率很重要(玄学？)</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>v</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>:</mo><mo>=</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>9</mn><mo>∗</mo><msub><mi>v</mi><mrow><mi>i</mi></mrow></msub><mo>−</mo><mn>0</mn><mi mathvariant="normal">.</mi><mn>0</mn><mn>0</mn><mn>0</mn><mn>5</mn><mo>∗</mo><mi>ϵ</mi><mo>∗</mo><msub><mi>w</mi><mi>i</mi></msub><mo>−</mo><mi>ϵ</mi><mo>∗</mo><mo>&lt;</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>W</mi></mrow></mfrac><msub><mi mathvariant="normal">∣</mi><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow></msub><msub><mo>&gt;</mo><mrow><msub><mi>D</mi><mi>i</mi></msub></mrow></msub><msub><mi>w</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>:</mo><mo>=</mo><msub><mi>w</mi><mrow><mi>i</mi></mrow></msub><mo>+</mo><msub><mi>v</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">v_{i+1}:=0.9*v_{i}-0.0005*\epsilon*w_i-\epsilon*&lt;\frac{\partial L}{\partial W}|_{w_i}&gt;_{D_i} w_{i+1}:=w_{i}+v_{i+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.37144em"></span><span class="strut bottom" style="height:2.05744em;vertical-align:-.686em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">v</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">:</span><span class="mrel">=</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">9</span><span class="mbin">∗</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">v</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord mathrm">0</span><span class="mord mathrm">.</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">0</span><span class="mord mathrm">5</span><span class="mbin">∗</span><span class="mord mathit">ϵ</span><span class="mbin">∗</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord mathit">ϵ</span><span class="mbin">∗</span><span class="mrel">&lt;</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm" style="margin-right:.05556em">∂</span><span class="mord mathit" style="margin-right:.13889em">W</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathrm" style="margin-right:.05556em">∂</span><span class="mord mathit">L</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mord"><span class="mord mathrm">∣</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel"><span class="mrel">&gt;</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.02778em">D</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.02778em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">:</span><span class="mrel">=</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">v</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mbin">+</span><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></span></p><p>i是epoch的轮数，v是动量变量，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.43056em;vertical-align:0"></span><span class="base textstyle uncramped"><span class="mord mathit">ϵ</span></span></span></span>是learning rate，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>&lt;</mo><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>W</mi></mrow></mfrac><msub><mi mathvariant="normal">∣</mi><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow></msub><msub><mo>&gt;</mo><mrow><msub><mi>D</mi><mi>i</mi></msub></mrow></msub></mrow><annotation encoding="application/x-tex">&lt;\frac{\partial L}{\partial W}|_{w_i}&gt;_{D_i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.8801079999999999em"></span><span class="strut bottom" style="height:1.2251079999999999em;vertical-align:-.345em"></span><span class="base textstyle uncramped"><span class="mrel">&lt;</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.345em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm" style="margin-right:.05556em">∂</span><span class="mord mathit" style="margin-right:.13889em">W</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.394em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm" style="margin-right:.05556em">∂</span><span class="mord mathit">L</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mord"><span class="mord mathrm">∣</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel"><span class="mrel">&gt;</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.02778em">D</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.02778em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>是在第i次的batch中损失函数关于w的导数的平均值。</p><ul><li><p>对于每一层中的w的初始化，使用的是均值为0，标准差为0.01的高斯分布，而对于2，4，5卷积层和全连接层的bias，初始化为1，其他bias初始化为0；使得训练初始阶段Relus有正数输入，训练加快。</p></li><li><p>所有层使用相同的初始化学习率0.01，然后在训练过程中手工调整，启发式做法是当验证的错误率不在减少时，对当前的学习率进行/10操作。</p></li></ul><h3 id="6-results"><a class="markdownIt-Anchor" href="#6-results"></a> 6 Results</h3><p>和其他model进行对比，突出自己的成绩，横向对比。</p><h3 id="7-discussion"><a class="markdownIt-Anchor" href="#7-discussion"></a> 7 Discussion</h3><p>强调卷积神经网络大且深的重要性，并对未来的工作进行展望，本文为希望用在视频序列帧中。</p><h3 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h3><p>所有引用的文章出处。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/4824-imagenet-classification-with-d
      
    
    </summary>
    
      <category term="paper" scheme="http://aier02.com/categories/paper/"/>
    
    
      <category term="cv" scheme="http://aier02.com/tags/cv/"/>
    
      <category term="paper reading" scheme="http://aier02.com/tags/paper-reading/"/>
    
  </entry>
  
  <entry>
    <title>backpropagation</title>
    <link href="http://aier02.com/2018/10/29/backpropagation/"/>
    <id>http://aier02.com/2018/10/29/backpropagation/</id>
    <published>2018-10-29T08:04:42.275Z</published>
    <updated>2018-11-21T08:28:53.411Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h2 id="introduction"><a class="markdownIt-Anchor" href="#introduction"></a> Introduction</h2><ul><li><p><strong>backpropagation</strong>, which is a way of computing gradients of expressions through recursive application of <strong>chain rule</strong>. 反向传播算法通过链式法则计算梯度</p></li><li><p>in practice we usually only compute the gradient for the parameters (e.g. W,b) so that we can use it to perform a parameter update.一般实际中计算的是权重的梯度，用于更新权重，也可能计算输入x的梯度，用于可视化和解释是神经网络的工作</p></li></ul><h2 id="simple-expressions-and-interpretation-of-the-gradient"><a class="markdownIt-Anchor" href="#simple-expressions-and-interpretation-of-the-gradient"></a> Simple expressions and interpretation of the gradient</h2><ul><li>the derivative on each variable tells you the sensitivity of the whole expression on its value，导函数表明了整个函数对于不同自变量的敏感程度</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo>)</mo><mo>=</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>+</mo><mi>h</mi><mfrac><mrow><mi>d</mi><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">f(x + h) = f(x) + h \frac{df(x)}{dx}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em"></span><span class="strut bottom" style="height:2.113em;vertical-align:-.686em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mbin">+</span><span class="mord mathit">h</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mbin">+</span><span class="mord mathit">h</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit">d</span><span class="mord mathit">x</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit">d</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><p>表明了f(x)随x变化的幅度，即x变化h后，f(x+h)如何变化,自变量的变化量和函数值的变化量的关系。</p><ul><li><p>Even though the gradient is technically a vector, we will often use terms such as <em>“the gradient on x”</em> instead of the technically correct phrase <em>“the partial derivative on x”</em> for simplicity.出于方便，notebook中将对于x的偏导数说成对于x的梯度</p></li><li><p>he derivatives tell us nothing about the effect of such large changes on the inputs of a function; They are only informative for tiny, infinitesimally small changes on the inputs, as indicated by the <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>lim</mi><mrow><mi>h</mi><mo>→</mo><mn>0</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\lim_{h \rightarrow 0}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.84444em;vertical-align:-.15em"></span><span class="base textstyle uncramped"><span class="mop"><span class="mop">lim</span><span class="vlist"><span style="top:.15em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">h</span><span class="mrel">→</span><span class="mord mathrm">0</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span> in its definition.</p></li></ul><h2 id="compound-expressions-with-chain-rule"><a class="markdownIt-Anchor" href="#compound-expressions-with-chain-rule"></a> Compound expressions with chain rule</h2><p>利用微积分的知识求某个函数的导数有时候是十分困难的，或者相当的繁琐，这种情况下如何快速求出函数对某个参数的偏导数呢？此时，chain rule就很关键了。譬如存在函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo separator="true">,</mo><mi>z</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>x</mi><mo>+</mo><mi>y</mi><mo>)</mo><mi>z</mi></mrow><annotation encoding="application/x-tex">f(x,y,z)=(x+y)z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.03588em">y</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.04398em">z</span><span class="mclose">)</span><span class="mrel">=</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:.03588em">y</span><span class="mclose">)</span><span class="mord mathit" style="margin-right:.04398em">z</span></span></span></span>,则使用chain rule有：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set some inputs</span></span><br><span class="line">x = <span class="number">-2</span>; y = <span class="number">5</span>; z = <span class="number">-4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># perform the forward pass</span></span><br><span class="line">q = x + y <span class="comment"># q becomes 3</span></span><br><span class="line">f = q * z <span class="comment"># f becomes -12</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># perform the backward pass (backpropagation) in reverse order:</span></span><br><span class="line"><span class="comment"># first backprop through f = q * z</span></span><br><span class="line">dfdz = q <span class="comment"># df/dz = q, so gradient on z becomes 3</span></span><br><span class="line">dfdq = z <span class="comment"># df/dq = z, so gradient on q becomes -4</span></span><br><span class="line"><span class="comment"># now backprop through q = x + y</span></span><br><span class="line">dfdx = <span class="number">1.0</span> * dfdq <span class="comment"># dq/dx = 1. And the multiplication here is the chain rule!</span></span><br><span class="line">dfdy = <span class="number">1.0</span> * dfdq <span class="comment"># dq/dy = 1</span></span><br></pre></td></tr></table></figure><h2 id="intuitive-understanding-of-backpropagation"><a class="markdownIt-Anchor" href="#intuitive-understanding-of-backpropagation"></a> Intuitive understanding of backpropagation</h2><ul><li>Every gate in a circuit diagram gets some inputs and can right away compute two things: 1. its output value and 2. the <em>local</em> gradient of its inputs with respect to its output value.将函数拆分为各个初等函数的混合计算，可视化为一个circuit diagram，图中的每个节点为一个基本计算。每个节点都可以独立计算他的output和local gradient</li><li>Backpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher</li></ul><h2 id="modularity-sigmoid-example"><a class="markdownIt-Anchor" href="#modularity-sigmoid-example"></a> Modularity: Sigmoid example</h2><p>sigmoid function(logistic function):</p>\sigma(x) = \frac{1}{1+e^{-x}} \\\\ \rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) = \left( 1 - \sigma(x) \right) \sigma(x)<!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;introduction&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#introduction&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="cs231n" scheme="http://aier02.com/categories/cs231n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
      <category term="chain rule" scheme="http://aier02.com/tags/chain-rule/"/>
    
      <category term="backpropagation" scheme="http://aier02.com/tags/backpropagation/"/>
    
  </entry>
  
  <entry>
    <title>leetcode summary 26-30</title>
    <link href="http://aier02.com/2018/10/24/leetcode-summary-26-30/"/>
    <id>http://aier02.com/2018/10/24/leetcode-summary-26-30/</id>
    <published>2018-10-24T03:20:21.962Z</published>
    <updated>2018-10-30T02:56:49.273Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h3 id="26-remove-duplicates-from-sorted-array"><a class="markdownIt-Anchor" href="#26-remove-duplicates-from-sorted-array"></a> 26. Remove Duplicates from Sorted Array</h3><p>Given a sorted array <em>nums</em>, remove the duplicates in-place, such that each element appear only <em>once</em> and return the new length.</p><p>Do not allocate extra space for another array, you must do this by <strong>modifying the input array in-place</strong> with O(1) extra memory.</p><p>题目有另外的提示就是要求返回一个int值的length，检测器就会检查modified的nums前length个值是否为排除了重复值后的正确答案。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeDuplicates</span><span class="params">(self, nums)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> len(nums)&lt;=<span class="number">1</span>:<span class="keyword">return</span> len(nums)</span><br><span class="line">        l,r,length=<span class="number">0</span>,<span class="number">1</span>,len(nums)</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">            <span class="keyword">if</span> r==length:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> nums[r]&gt;nums[l]:</span><br><span class="line">                    l+=<span class="number">1</span></span><br><span class="line">                    nums[l]=nums[r]</span><br><span class="line">                r+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> l+<span class="number">1</span></span><br></pre></td></tr></table></figure><p>使用两个pointer对nums数组进行遍历，每发现一个新的元素，则后一个pointer l+1，同时赋值nums[l]为nums[r]，r指针用于遍历数组,最终使得nums的前l+1个元素均不重复。</p><h3 id="27-remove-element"><a class="markdownIt-Anchor" href="#27-remove-element"></a> 27. Remove Element</h3><p>Given an array <em>nums</em> and a value <em>val</em>, remove all instances of that value in-place and return the new length.</p><p>Do not allocate extra space for another array, you must do this by <strong>modifying the input array in-place</strong> with O(1) extra memory.</p><p>The order of elements can be changed. It doesn’t matter what you leave beyond the new length.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeElement</span><span class="params">(self, nums, val)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type val: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> val <span class="keyword">not</span> <span class="keyword">in</span> nums:<span class="keyword">return</span> len(nums)</span><br><span class="line">        res=<span class="number">0</span></span><br><span class="line">        l,r,length=<span class="number">0</span>,<span class="number">1</span>,len(nums)</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">            <span class="keyword">if</span> r&gt;=length <span class="keyword">or</span> l&gt;=length:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> nums[l]==val:</span><br><span class="line">                    <span class="keyword">if</span> nums[r]!=val:</span><br><span class="line">                        nums[l]=nums[r]</span><br><span class="line">                        nums[r]=val</span><br><span class="line">                        l+=<span class="number">1</span></span><br><span class="line">                        r+=<span class="number">1</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        r+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    l+=<span class="number">1</span></span><br><span class="line">                    r=l+<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> num!=val:</span><br><span class="line">                res+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><p>使用两个pointer，l指针寻找等于val的索引，r指针寻找在l之后的位置中不等于val的索引，若找到满足条件的r，则交换l和r索引指示的位置的值，l+1往后继续寻找val的位置，注意r无论如何必定在l之后。</p><h3 id="28-implement-strstr"><a class="markdownIt-Anchor" href="#28-implement-strstr"></a> 28. Implement strStr()</h3><p>Implement strStr().</p><p>Return the index of the first occurrence of needle in haystack, or <strong>-1</strong> if needle is not part of haystack.</p><p>For the purpose of this problem, we will return 0 when <code>needle</code> is an empty string</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">strStr</span><span class="params">(self, haystack, needle)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type haystack: str</span></span><br><span class="line"><span class="string">        :type needle: str</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ph,pn,hlength,nlength=<span class="number">0</span>,<span class="number">0</span>,len(haystack),len(needle)</span><br><span class="line">        num=<span class="number">0</span></span><br><span class="line">        flag=<span class="number">0</span></span><br><span class="line">        res=<span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">            <span class="keyword">if</span> num==nlength <span class="keyword">or</span> ph==hlength:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> haystack[ph]==needle[pn]:</span><br><span class="line">                    <span class="keyword">if</span> flag==<span class="number">0</span>:</span><br><span class="line">                        flag=<span class="number">1</span></span><br><span class="line">                        res=ph</span><br><span class="line">                    pn+=<span class="number">1</span></span><br><span class="line">                    num+=<span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> flag==<span class="number">1</span>:</span><br><span class="line">                        pn=<span class="number">0</span></span><br><span class="line">                        num=<span class="number">0</span></span><br><span class="line">                        flag=<span class="number">0</span></span><br><span class="line">                        ph=res</span><br><span class="line">                ph+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> num==nlength:</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><p>使用两个pointer分别遍历haystack和needle，tips在于使用flag表示是否找到两者一样的初始char，同时res记录第一个相同在haystack中的位置，前向遍历时若发现两个字母不相同，则检查flag是否为1，若为1则表示部分匹配，此时需要重置needle的指针pn和haystack指针ph为刚才记录的位置(后面会+1，表示继续往前找，而不用管前面的字母).</p><p>方法二：利用python的切片特性，没有使用两个指针</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">strStr</span><span class="params">(self, haystack, needle)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    :type haystack: str</span></span><br><span class="line"><span class="string">    :type needle: str</span></span><br><span class="line"><span class="string">    :rtype: int</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    a = len(needle)</span><br><span class="line">    b = len(haystack)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> a &lt;= b:</span><br><span class="line">        <span class="keyword">if</span> needle == haystack[i:i+a]:</span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        b -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><h3 id="29-divide-two-integers"><a class="markdownIt-Anchor" href="#29-divide-two-integers"></a> 29. Divide Two Integers</h3><p>Given two integers <code>dividend</code> and <code>divisor</code>, divide two integers without using multiplication, division and mod operator.</p><p>Return the quotient after dividing <code>dividend</code> by <code>divisor</code>.</p><p>The integer division should truncate toward zero.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">divide</span><span class="params">(self, dividend, divisor)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type dividend: int</span></span><br><span class="line"><span class="string">        :type divisor: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">        <span class="keyword">if</span> dividend==<span class="number">0</span>:<span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        i,result,p,q=map(abs,(<span class="number">0</span>,<span class="number">0</span>,dividend,divisor))</span><br><span class="line">        <span class="keyword">while</span> q&lt;&lt;i &lt;=p:i+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> reversed(range(i)):</span><br><span class="line">            <span class="keyword">if</span> q&lt;&lt;j &lt;=p:</span><br><span class="line">                p,result=p-(q&lt;&lt;j),result+(<span class="number">1</span>&lt;&lt;j)</span><br><span class="line">        <span class="keyword">if</span> (dividend&lt;<span class="number">0</span>)!=(divisor&lt;<span class="number">0</span>) <span class="keyword">or</span> result&lt;(<span class="number">-1</span>&lt;&lt;<span class="number">31</span>):result=-result</span><br><span class="line">        <span class="keyword">return</span> min(result,(<span class="number">1</span>&lt;&lt;<span class="number">31</span>)<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p>使用移位操作实现乘除，基于二进制的思想，统计最多移位的位数，然后想平常手工计算两个数相除一样，上一位，除数变小，在继续同样的步骤。</p><h3 id="30-substring-with-concatenation-of-all-words"><a class="markdownIt-Anchor" href="#30-substring-with-concatenation-of-all-words"></a> 30. Substring with Concatenation of All Words</h3><p>You are given a string, <strong>s</strong>, and a list of words, <strong>words</strong>, that are all of the same length. Find all starting indices of substring(s) in <strong>s</strong> that is a concatenation of each word in <strong>words</strong> exactly once and without any intervening characters.</p><p>class Solution:<br>​ def findSubstring(self, s, words):<br>​ if len(words) == 0:<br>​ return []<br># initialize d, l, ans<br>​ l = len(words[0])<br>​ d = {}<br>​ for w in words:<br>​ if w in d:<br>​ d[w] += 1<br>​ else:<br>​ d[w] = 1<br>​ i = 0<br>​ ans = []</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sliding window(s)</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(l):</span><br><span class="line"><span class="comment">#left指针用于指示左端第一个有效字符的起始位置</span></span><br><span class="line"><span class="comment">#count用于记录当前有效的word数目</span></span><br><span class="line">    left = k</span><br><span class="line">    subd = &#123;&#125;</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(k, len(s)-l+<span class="number">1</span>, l):</span><br><span class="line">        tword = s[j:j+l]</span><br><span class="line">        <span class="comment"># valid word</span></span><br><span class="line">        <span class="keyword">if</span> tword <span class="keyword">in</span> d:</span><br><span class="line">            <span class="keyword">if</span> tword <span class="keyword">in</span> subd:</span><br><span class="line">                subd[tword] += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                subd[tword] = <span class="number">1</span></span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">            <span class="comment">#纠正某个单词的数目，若超过了words中的数目则需要进行前向调整</span></span><br><span class="line">            <span class="keyword">while</span> subd[tword] &gt; d[tword]:</span><br><span class="line">                subd[s[left:left+l]] -= <span class="number">1</span></span><br><span class="line">                left += l</span><br><span class="line">                count -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> count == len(words):</span><br><span class="line">                ans.append(left)</span><br><span class="line">        <span class="comment"># not valid</span></span><br><span class="line">        <span class="comment">#遇到无效的单词时，需要重新计算left的位置，默认下一个单词有效，同时需要重新统计字典</span></span><br><span class="line">        <span class="comment">#这种解法关键在于使用两个字典分别统计words中各个单词出现的次数，当统计到的有效单词数目一致时则认为是有效的连续子串</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            left = j + l</span><br><span class="line">            subd = &#123;&#125;</span><br><span class="line">            count = <span class="number">0</span></span><br><span class="line"><span class="keyword">return</span> ans</span><br></pre></td></tr></table></figure><p>一开始的想法是通过寻找words中所有可能的单词搭配，组合数为n!，苦于寻找快速找到所有组合的方法，在看了discussion后发现自己其实没有很好的理解题目，其实问题的关键是找到由words中单词随意组成的子串的位置，并不要求返回具体的子串是什么，同样不关注子串是否重复，所以比较理想的方法是不用字符串匹配，毕竟倘若要字符串匹配，需要做到每一个字符进行比较；使用字典的形式统计单词出现的次数，同样能实现题目的要求。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;26-remove-duplicates-from-sorted-array&quot;&gt;&lt;a class=&quot;markdownIt-Anchor
      
    
    </summary>
    
      <category term="leetcode" scheme="http://aier02.com/categories/leetcode/"/>
    
    
      <category term="array" scheme="http://aier02.com/tags/array/"/>
    
      <category term="two pointers" scheme="http://aier02.com/tags/two-pointers/"/>
    
      <category term="backtracking" scheme="http://aier02.com/tags/backtracking/"/>
    
  </entry>
  
  <entry>
    <title>leetcode summary 20-25</title>
    <link href="http://aier02.com/2018/10/24/leetcode_summary_20-25/"/>
    <id>http://aier02.com/2018/10/24/leetcode_summary_20-25/</id>
    <published>2018-10-24T01:07:58.319Z</published>
    <updated>2018-10-29T08:04:24.604Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h3 id="20-valid-parentheses"><a class="markdownIt-Anchor" href="#20-valid-parentheses"></a> 20 Valid Parentheses</h3><p>Given a string containing just the characters <code>'('</code>, <code>')'</code>, <code>'{'</code>, <code>'}'</code>, <code>'['</code> and <code>']'</code>, determine if the input string is valid.</p><p>An input string is valid if:</p><ol><li>Open brackets must be closed by the same type of brackets.</li><li>Open brackets must be closed in the correct order.</li></ol><p>Note that an empty string is also considered valid.</p><p>利用stack的后进新出的特性对输入尽心匹配</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">isValid</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type s: str</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line">        mdict = &#123;<span class="string">')'</span>:<span class="string">'('</span>,<span class="string">'&#125;'</span>:<span class="string">'&#123;'</span>,<span class="string">']'</span>:<span class="string">'['</span>&#125;</span><br><span class="line">        my_stack = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> s:</span><br><span class="line">            <span class="keyword">if</span> mdict.get(item):</span><br><span class="line">    <span class="comment">#check if item is closing parentheses</span></span><br><span class="line">                <span class="keyword">if</span> my_stack == []: </span><br><span class="line"><span class="comment"># if it is closing and stack is empty - return false</span></span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> mdict.get(item) != my_stack.pop():</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>: </span><br><span class="line">    <span class="comment"># if not - add to stack </span></span><br><span class="line">                my_stack.append(item)        </span><br><span class="line">        <span class="keyword">return</span> my_stack == []</span><br></pre></td></tr></table></figure><p>关键在于创建一个stack用于存储所有开括号，python中list自带pop函数使得list成为一个栈；依次读取s的值，每当遇到一个闭括号，则检查stack非空且顶端的符号是否为对应的开括号，是则正确关闭，否则error；每遇到一个开括号则放入栈中，等待匹配；开闭括号的对应关系通过dict存储。</p><p>###21 Generate Parentheses</p><p>Given <em>n</em> pairs of parentheses, write a function to generate all combinations of well-formed parentheses.</p><p>For example, given <em>n</em> = 3, a solution set is:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[</span><br><span class="line">  <span class="string">"((()))"</span>,</span><br><span class="line">  <span class="string">"(()())"</span>,</span><br><span class="line">  <span class="string">"(())()"</span>,</span><br><span class="line">  <span class="string">"()(())"</span>,</span><br><span class="line">  <span class="string">"()()()"</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>回溯算法的定义：回溯算法也叫试探法，它是一种系统地搜索问题的解的方法。回溯算法的基本思想是：从一条路往前走，能进则进，不能进则退回来，换一条路再试。</p><p>回溯算法实际上一个类似枚举的搜索尝试过程，主要是在搜索尝试过程中寻找问题的解，当发现已不满足求解条件时，就“回溯”返回，尝试别的路径。回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法。适用于求解组合数较大的问题。</p><p>对于回溯问题，总结出一个递归函数模板，包括以下三点</p><p>递归函数的开头写好跳出条件，满足条件才将当前结果加入总结果中<br>已经拿过的数不再拿 if(s.contains(num)){continue;}<br>遍历过当前节点后，为了回溯到上一步，要去掉已经加入到结果list中的当前节点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generateParenthesis</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type n: int</span></span><br><span class="line"><span class="string">        :rtype: List[str]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        path,result=<span class="string">''</span>,[]</span><br><span class="line">        self.backtrack(n,<span class="number">0</span>,<span class="number">0</span>,path,result)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backtrack</span><span class="params">(self,n,num_open,num_close,path,result)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> num_close==n:</span><br><span class="line">            result.append(path)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> num_open&lt;n:</span><br><span class="line">                path+=<span class="string">'('</span></span><br><span class="line">                num_open+=<span class="number">1</span></span><br><span class="line">                self.backtrack(n,num_open,num_close,path,result)</span><br><span class="line">                <span class="comment">#回退到前一步</span></span><br><span class="line">                path=path[:<span class="number">-1</span>]</span><br><span class="line">                num_open-=<span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> num_close&lt;num_open:</span><br><span class="line">                path+=<span class="string">')'</span></span><br><span class="line">                num_close+=<span class="number">1</span></span><br><span class="line">                self.backtrack(n,num_open,num_close,path,result)</span><br><span class="line">                <span class="comment">#path=path[:-1]</span></span><br><span class="line">                <span class="comment">#num_close-=1</span></span><br><span class="line">            <span class="keyword">return</span></span><br></pre></td></tr></table></figure><p>backtrack函数体现了回溯算法的思想，实际上类似于dfs，当“不满足条件时则回退到上一步”,在代码实现中则是通过回退num_open或者num_close的值和path实现这一操作，因为添加path的操作实际只有两种，添加“(”或者“)”，故根据if的顺序，在python中如果为顺序执行if的话，只用回退第一个if即可，若两个if的顺序不确定，更加规范的操作是都得回退。</p><p>在运行过程中，“不满足条件”编码实现为“满足条件则加入到result中”，这种做法实际上更加符合枚举的想法;该代码实际上是每一个位置先加“(”进行深度优先搜索，直到找到满足条件的path，然后回退到该位置，添加“)”继续进行深度优先搜索。</p><h3 id="23-merge-k-sorted-lists"><a class="markdownIt-Anchor" href="#23-merge-k-sorted-lists"></a> 23. Merge k Sorted Lists</h3><p>Merge <em>k</em> sorted linked lists and return it as one sorted list. Analyze and describe its complexity.</p><p><strong>Example:</strong></p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Input:</span><br><span class="line">[</span><br><span class="line">  1-&gt;4-&gt;5,</span><br><span class="line">  1-&gt;3-&gt;4,</span><br><span class="line">  2-&gt;6</span><br><span class="line">]</span><br><span class="line">Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mergeKLists</span><span class="params">(self, lists)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type lists: List[ListNode]</span></span><br><span class="line"><span class="string">        :rtype: ListNode</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        ls=[]</span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> lists:</span><br><span class="line">            <span class="keyword">while</span> t!=<span class="keyword">None</span>:</span><br><span class="line">                ls.append(t.val)</span><br><span class="line">                t=t.next</span><br><span class="line">        ls.sort()</span><br><span class="line">        head=fhead=ListNode(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(ls)):</span><br><span class="line">            head.next=ListNode(ls[i])</span><br><span class="line">            head=head.next</span><br><span class="line">        <span class="keyword">return</span> fhead.next</span><br></pre></td></tr></table></figure><p>没有使用递归依次计算两个sorted lists，而是直接获取所有链表中的数值，然后sort()，根据得到的已经排好序的序列创建新的链表，明显时间复杂度为O(n)（忽略list自带的sort()函数的时间复杂度）</p><h3 id="24-swap-nodes-in-pairs"><a class="markdownIt-Anchor" href="#24-swap-nodes-in-pairs"></a> 24. Swap Nodes in Pairs</h3><p>Given a linked list, swap every two adjacent nodes and return its head.</p><p><strong>Example:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Given 1-&gt;2-&gt;3-&gt;4, you should return the list as 2-&gt;1-&gt;4-&gt;3.</span><br></pre></td></tr></table></figure><p><strong>Note:</strong></p><ul><li>Your algorithm should use only constant extra space.</li><li>You may <strong>not</strong> modify the values in the list’s nodes, only nodes itself may be changed.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">swapPairs</span><span class="params">(self, head)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type head: ListNode</span></span><br><span class="line"><span class="string">        :rtype: ListNode</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> head==<span class="keyword">None</span> <span class="keyword">or</span> head.next==<span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> head</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            temp=head</span><br><span class="line">            cur=head.next</span><br><span class="line">            prev=<span class="keyword">None</span></span><br><span class="line">            root=cur</span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">                node=cur.next</span><br><span class="line">                cur.next=temp</span><br><span class="line">                temp.next=node</span><br><span class="line">                <span class="keyword">if</span> prev==<span class="keyword">None</span>:</span><br><span class="line">                    prev=temp</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    prev.next=cur</span><br><span class="line">                    prev=temp</span><br><span class="line">                <span class="keyword">if</span> node==<span class="keyword">None</span> <span class="keyword">or</span> node.next==<span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                cur=node.next</span><br><span class="line">                temp=node</span><br><span class="line">            <span class="keyword">return</span> root</span><br></pre></td></tr></table></figure><p>每两个节点互换并且不能只更改node的val，则只需处理节点之间的next关系，每次对两个节点进行操作，比如1-&gt;2-&gt;3-&gt;4,将2的next指向1，1的next指向3，同时注意另存一个节点记录前一组的最后一个节点，即该例子中的1，3，用于连接前一组转换后的的最后一个节点和后一组转换后的第一个节点，即例子中的1.next指向4.翻转的关键在于指定节点prev。</p><h3 id="25-reverse-nodes-in-k-group"><a class="markdownIt-Anchor" href="#25-reverse-nodes-in-k-group"></a> 25. Reverse Nodes in k-Group</h3><p>Given a linked list, reverse the nodes of a linked list <em>k</em> at a time and return its modified list.</p><p><em>k</em> is a positive integer and is less than or equal to the length of the linked list. If the number of nodes is not a multiple of <em>k</em> then left-out nodes in the end should remain as it is.</p><p><strong>Example:</strong></p><p>Given this linked list: <code>1-&gt;2-&gt;3-&gt;4-&gt;5</code></p><p>For <em>k</em> = 2, you should return: <code>2-&gt;1-&gt;4-&gt;3-&gt;5</code></p><p>For <em>k</em> = 3, you should return: <code>3-&gt;2-&gt;1-&gt;4-&gt;5</code></p><p><strong>Note:</strong></p><ul><li>Only constant extra memory is allowed.</li><li>You may not alter the values in the list’s nodes, only nodes itself may be changed.</li></ul><p>针对整个链表的翻转实现,注意每次只要处理一个节点cur和该节点的前向节点prev，同时在改变cur的next之前另存nxt指向原来的next节点，以便于完成一对节点的翻转后cur指向cur的next。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseList</span><span class="params">(self, head)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> head <span class="keyword">or</span> <span class="keyword">not</span> head.next:</span><br><span class="line">            <span class="keyword">return</span> head</span><br><span class="line">        <span class="comment">#prev,cur,nxt分别记录前一个节点，正在处理的节点和后一个节点</span></span><br><span class="line">        prev, cur, nxt = <span class="keyword">None</span>, head, head</span><br><span class="line">        <span class="keyword">while</span> cur:</span><br><span class="line">            nxt = cur.next</span><br><span class="line">            cur.next = prev</span><br><span class="line">            prev = cur</span><br><span class="line">            cur = nxt</span><br><span class="line">        <span class="keyword">return</span> prev</span><br></pre></td></tr></table></figure><p>扩展到k个节点的翻转，每次翻转k个节点的链表:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Definition for singly-linked list.</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="comment">#处理分组和组间的连接关系</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverseKGroup</span><span class="params">(self, head, k)</span>:</span></span><br><span class="line">        count, node = <span class="number">0</span>, head</span><br><span class="line">        <span class="keyword">while</span> node <span class="keyword">and</span> count &lt; k:</span><br><span class="line">            node = node.next</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> count &lt; k: <span class="keyword">return</span> head</span><br><span class="line">        new_head, prev = self.reverse(head, count)</span><br><span class="line">        head.next = self.reverseKGroup(new_head, k)</span><br><span class="line">        <span class="keyword">return</span> prev</span><br><span class="line">    <span class="comment">#处理每一个组的k个节点的翻转</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reverse</span><span class="params">(self, head, count)</span>:</span></span><br><span class="line">        prev, cur, nxt = <span class="keyword">None</span>, head, head</span><br><span class="line">        <span class="keyword">while</span> count &gt; <span class="number">0</span>:</span><br><span class="line">            nxt = cur.next</span><br><span class="line">            cur.next = prev</span><br><span class="line">            prev = cur</span><br><span class="line">            cur = nxt</span><br><span class="line">            count -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> (cur, prev)</span><br></pre></td></tr></table></figure><p>关键在于reverse函数返回值cur指示连续的k个节点翻转后的下一个节点，最为新的head节点，prev指示前一组翻转后的节点的最后一个节点，用于和前前一组的原头节点(即翻转后的尾节点)进行连接。实际问题的关键在于将整条链表以k个节点为一组进行划分，每次对k个节点的子链表进行翻转，对于某个子链表，翻转后要注意确定他的下一组的头节点new_head，从而获得他的下一组翻转后的头节点prev，将改子链表翻转后的尾节点，即原来的头节点和他的下一组的翻转后的头节点连接起来，同时要返回该子链表翻转后的头节点prev，供他的前一组使用。总而言之就是要注意每一组翻转前后的头尾节点，处理相邻组的连接关系。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;20-valid-parentheses&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#20-valid-
      
    
    </summary>
    
      <category term="leetcode" scheme="http://aier02.com/categories/leetcode/"/>
    
    
      <category term="backtracking" scheme="http://aier02.com/tags/backtracking/"/>
    
      <category term="linked list" scheme="http://aier02.com/tags/linked-list/"/>
    
      <category term="string" scheme="http://aier02.com/tags/string/"/>
    
      <category term="stack" scheme="http://aier02.com/tags/stack/"/>
    
  </entry>
  
  <entry>
    <title>common knowledge</title>
    <link href="http://aier02.com/2018/10/13/common_knowledge/"/>
    <id>http://aier02.com/2018/10/13/common_knowledge/</id>
    <published>2018-10-13T02:28:06.184Z</published>
    <updated>2018-10-30T02:57:03.946Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h3 id="np问题"><a class="markdownIt-Anchor" href="#np问题"></a> NP问题</h3><ul><li>NP问题:首先需要介绍P(Polynomial,多项式)问题.P问题是可以在多项式时间内被确定机(通常意义的计算机)<a href="http://xn--67q801g9inollphd.NP" target="_blank" rel="noopener">解决的问题.NP</a>(Non-Deterministic Polynomial, 非确定多项式)问题,是指可以在多项式时间内被非确定机(他可以猜,他总是能猜到最能满足你需要的那种选择,如果你让他解决n皇后问题,他只要猜n次就能完成----每次都是那么幸运)解决的问题.这里有一个著名的问题----千禧难题之首,是说P问题是否等于NP问题,也即是否所有在非确定机上多项式可解的问题都能在确定机上用多项式时间求解</li></ul><p>###L1-norm和L2-norm</p><ul><li><p>l1-norm是指曼哈顿距离，即向量的各个元素的绝对值之和；l2-norm是指欧几里得距离，即向量的各个元素的平方和。Lp范数是指向量的各元素的p次方之和开p次方</p></li><li><p><img src="/images/181013/l1&amp;l2.png" alt=""></p><ul><li><p>鲁棒性（Robustness）：最小绝对值偏差的方法应用领域很广，相比最小均方的方法，它的鲁棒性更好，LAD能对数据中的异常点有很好的抗干扰能力，异常点可以安全的和高效的忽略，这对研究帮助很大。如果异常值对研究很重要，最小均方误差则是更好的选择。</p><p>对于L2-norm，由于是均方误差，如果误差&gt;1的话，那么平方后，相比L1-norm而言，误差就会被放大很多。因此模型会对样例更敏感。如果样例是一个异常值，模型会调整最小化异常值的情况，以牺牲其它更一般样例为代价，因为相比单个异常样例，那些一般的样例会得到更小的损失误差</p></li><li><p>内置的特征选择（Built-in feature selection）：这是L1-norm经常被提及的一个优点，而L2-norm没有。这实际上是L1-norm的一个结果，L1-norm往往会使系数变得稀疏（sparse coefficients）。假设模型有100个系数，但是有10个非零的系数，这就是说，其它90个预测器在预测目标值上是没有用的。L2-norm往往会有非稀疏的系数（non-sparse coefficients），没有这个特点。</p></li><li><p>稀疏性（Sparsity）：这主要是一个向量或矩阵中只有很少的非零（non-zero）条目（entries）。L1-norm有能产生许多零值或非常小的值的系数的属性，很少有大的系数。L1-norm得到的稀疏矩阵能用于选择特征，稀疏矩阵中大多数为0，即对loss没用的特征，故只需关注少数非0的特征；</p></li><li><p>稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0. 通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系</p></li></ul></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h3 id=&quot;np问题&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#np问题&quot;&gt;&lt;/a&gt; NP问题&lt;/h3&gt;&lt;ul&gt;&lt;
      
    
    </summary>
    
      <category term="cs231n" scheme="http://aier02.com/categories/cs231n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>CS231n-2017-Summary</title>
    <link href="http://aier02.com/2018/10/08/CS231n_2017_Summary/"/>
    <id>http://aier02.com/2018/10/08/CS231n_2017_Summary/</id>
    <published>2018-10-08T14:46:19.006Z</published>
    <updated>2018-11-02T09:33:11.448Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><p>Something new to me when I read such a good notebook about <a href="https://github.com/mbadry1/CS231n-2017-Summary" target="_blank" rel="noopener">CS231n-2017-Summary</a></p><h3 id="cnns"><a class="markdownIt-Anchor" href="#cnns"></a> CNNs</h3><p>常用same策略同时保存图像边缘信息</p><ul><li>Padding strategy:in order to maintain our full size of the input. If we didn’t do padding zero the input will be shrinking too fast and we will lose a lot of data.Give a stride of <code>1</code> its common to pad to this equation: <code>(F-1)/2</code> where F is the filter size, zero padding from both sides.If we pad this way we call this same convolution.<ul><li>If we have input of shape (32,32,3) and ten filters with shape is (5,5) with stride 1 and pad 2;Output size will be (32,32,10) # We maintain the size.</li><li>Size of parameters per filter = 5x5x3 + 1 = 76(+1 for bias)</li><li>All parameters 76x10=760</li></ul></li><li>So here are the parameters for the Conv layer:<ul><li>Number of filters K.<ul><li>Usually a power of 2.</li></ul></li><li>Spatial content size F.<ul><li>3,5,7 …</li></ul></li><li>The stride S.<ul><li>Usually 1 or 2 (If the stride is big there will be a downsampling but different of pooling)</li></ul></li><li>Amount of Padding<ul><li>If we want the input shape to be as the output shape, based on the F if 3 its 1, if F is 5 the 2 and so on</li></ul></li></ul></li></ul><p>一般而言pooling层是不可(用)学习的</p><ul><li>Pooling makes the representation smaller and more manageable.</li><li>Pooling Operates over each activation map independently.</li><li>Example of pooling is the maxpooling.<ul><li>Parameters of max pooling is the size of the filter and the stride&quot;<ul><li>Example <code>2x2</code> with stride <code>2</code> <code># Usually the two parameters are the same 2 , 2</code></li></ul></li></ul></li><li>Also example of pooling is average pooling.<ul><li>In this case it might be learnable.</li></ul></li></ul><h3 id="training-neural-networks-i"><a class="markdownIt-Anchor" href="#training-neural-networks-i"></a> Training neural networks I</h3><ul><li><p>As a revision here are the Mini batch stochastic gradient descent algorithm steps,小批量的随机梯度下降算法的步骤:</p><ul><li>Loop:<ol><li>Sample a batch of data.</li><li>Forward prop it through the graph (network) and get loss.(define loss)</li><li>Backprop to calculate the gradients.(chain rule)</li><li>Update the parameters using the gradients(learning rate)</li></ol></li></ul></li><li><p>Activation functions,用于引入非线性因素，单纯的线性模型表达能录不足。</p><p><img src="/images/181008/activation.png" alt=""></p><ul><li><p>Sigmoid:</p><ul><li>Squashes the numbers between [0,1]</li><li>Used as a firing rate like human brains.</li><li><code>Sigmoid(x) = 1 / (1 + e^-x)</code></li><li>Problems with sigmoid:<ul><li>big values neurons kill the gradients.</li><li>Gradients are in most cases near 0 (Big values/small values), that kills the updates if the graph/network are large.</li><li>Not Zero-centered.<ul><li>Didn’t produce zero-mean data.</li></ul></li><li>exp() is a bit compute expensive.<ul><li>just to mention. We have a more complex operations in deep learning like convolution.</li></ul></li></ul></li></ul></li><li><p>Tanh:</p><ul><li>Squashes the numbers between [-1,1]</li><li>Zero centered.</li><li>Still big values neurons “kill” the gradients.</li><li><code>Tanh(x)</code> is the equation.<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mi>s</mi><mi>i</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">tanh(x)=\frac {sinh(x)}{cosh(x)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.01em"></span><span class="strut bottom" style="height:1.53em;vertical-align:-.52em"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit">a</span><span class="mord mathit">n</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.34500000000000003em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">s</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.485em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">s</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>,<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>i</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><mi>x</mi></mrow></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><mn>2</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">sinh(x)=\frac {e^{x}-e^{-x}}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.939765em"></span><span class="strut bottom" style="height:1.284765em;vertical-align:-.345em"></span><span class="base textstyle uncramped"><span class="mord mathit">s</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.345em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.394em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord">−</span><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>,<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><mi>x</mi></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><mn>2</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">cosh(x)=\frac {e^{x}+e^{-x}}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.939765em"></span><span class="strut bottom" style="height:1.284765em;vertical-align:-.345em"></span><span class="base textstyle uncramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">s</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.345em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.394em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord">−</span><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>,</li><li>Proposed by Yann Lecun in 1991</li></ul></li><li><p>RELU (Rectified linear unit):</p><ul><li><code>RELU(x) = max(0,x)</code></li><li>Doesn’t kill the gradients.<ul><li>Only small values that are killed. Killed the gradient in the half</li></ul></li><li>Computationally efficient.</li><li>Converges much faster than Sigmoid and Tanh <code>(6x)</code></li><li>More biologically plausible than sigmoid.</li><li>Proposed by Alex Krizhevsky in 2012 Toronto university. (AlexNet)</li><li>Problems:<ul><li>Not zero centered.</li></ul></li><li>If weights aren’t initialized good, maybe 75% of the neurons will be dead and thats a waste computation. But its still works. This is an active area of research to optimize this.</li><li>To solve the issue mentioned above, people might initialize all the biases by 0.01</li></ul></li><li><p>Leaky RELU:</p><ul><li><code>leaky_RELU(x) = max(0.01x,x)</code></li><li>Doesn’t kill the gradients from both sides.</li><li>Computationally efficient.</li><li>Converges much faster than Sigmoid and Tanh (6x)</li><li>Will not die.</li><li>PRELU is placing the 0.01 by a variable alpha which is learned as a parameter.</li></ul></li><li><p>Exponential linear units (ELU):</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ELU(x) = &#123; x                                           if x &gt; 0</span><br><span class="line">   alpah *(exp(x) -1)                   if x &lt;= 0</span><br><span class="line">           # alpah are a learning parameter</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>It has all the benefits of RELU</p></li><li><p>Closer to zero mean outputs and adds some robustness to noise.</p></li><li><p>problems</p><ul><li><code>exp()</code> is a bit compute expensive.</li></ul></li></ul></li><li><p>Maxout activations:</p><ul><li><code>maxout(x) = max(w1.T*x + b1, w2.T*x + b2)</code></li><li>Generalizes RELU and Leaky RELU</li><li>Doesn’t die!</li><li>Problems:<ul><li>oubles the number of parameters per neuron</li></ul></li></ul></li><li><p>In practice:</p><ul><li>Use RELU. Be careful for your learning rates.</li><li>Try out Leaky RELU/Maxout/ELU</li><li>Try out tanh but don’t expect much.</li><li>Don’t use sigmoid!</li></ul></li></ul><p><strong>Data preprocessing</strong>:</p><ul><li><p>Normalize the data:减去均值后除以标准差</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Zero centered data. (Calculate the mean for every input).</span></span><br><span class="line"><span class="comment"># On of the reasons we do this is because we need data to be between positive and negative and not all the be negative or positive. </span></span><br><span class="line">X -= np.mean(X, axis = <span class="number">1</span>)</span><br><span class="line"><span class="comment">#np.mean()中的参数axis指定了哪个维度被压缩成1，例如axis=0,则输出的结果为一行，即求得输入x的每一列的平均，压缩成一行，同理axis=1，则输出为一列，该结果中的每一行为按照行进行平均的值。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Then apply the standard deviation. Hint: in images we don't do this.</span></span><br><span class="line">X /= np.std(X, axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p>To normalize images:对图像进行标准化</p><ul><li>Subtract the mean image (E.g. Alexnet)<ul><li>Mean image shape is the same as the input images.</li></ul></li><li>Or Subtract per-channel mean<ul><li>Means calculate the mean for each channel of all images. Shape is 3 (3 channels)</li></ul></li></ul></li><li><p>First idea is to initialize the w’s with small random numbers:</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.rand(D, H)</span><br><span class="line"><span class="comment"># Works OK for small networks but it makes problems with deeper networks!</span></span><br></pre></td></tr></table></figure></li><li><p>The standard deviations is going to zero in deeper networks. and the gradient will vanish sooner in deep networks.使用任意小的数字进行对w初始化，随着网络的加深可能导致梯度消失问题(每一层的蔬输入很小)。</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">1</span> * np.random.rand(D, H) </span><br><span class="line"><span class="comment"># Works OK for small networks but it makes problems with deeper networks!</span></span><br></pre></td></tr></table></figure></li><li><p>The network will explode with big numbers!，使用大于一的初值可能会导致深层网络中梯度爆炸问题。</p></li></ul></li><li><p>Xavier initialization:</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.rand(in, out) / np.sqrt(in)</span><br></pre></td></tr></table></figure></li><li><p>It works because we want the variance of the input to be as the variance of the output.</p></li><li><p>But it has an issue, It breaks when you are using RELU.</p></li></ul></li><li><p>He initialization(Solution for the RELU issue):</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.rand(in, out) / np.sqrt(in/2)</span><br></pre></td></tr></table></figure></li><li><p>Solves the issue with RELU. Its recommended when you are using RELU</p></li></ul></li></ul><p><strong>Batch normalization</strong>:</p><ul><li><p>is a technique to provide any layer in a Neural Network with inputs that are zero mean/unit variance.</p></li><li><p>It speeds up the training. You want to do this a lot.</p><ul><li>Made by Sergey Ioffe and Christian Szegedy at 2015.</li></ul></li><li><p>We make a Gaussian activations in each layer. by calculating the mean and the variance.</p></li><li><p>Usually inserted after (fully connected or Convolutional layers) and (before nonlinearity).</p></li><li><p>Steps (For each output of a layer)</p><ol><li><p>First we compute the mean and variance^2 of the batch for each feature.</p></li><li><p>We normalize by subtracting the mean and dividing by square root of (variance^2 + epsilon)</p><ul><li>epsilon to not divide by zero</li></ul></li><li><p>Then we make a scale and shift variables:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Result = gamma * normalizedX + beta</span><br></pre></td></tr></table></figure><ul><li>gamma and beta are learnable parameters.</li><li>it basically possible to say “Hey!! I don’t want zero mean/unit variance input, give me back the raw input - it’s better for me.”</li><li>Hey shift and scale by what you want not just the mean and variance!</li></ul></li></ol></li></ul><p><strong>Baby sitting the learning process</strong></p><ol><li>Preprocessing of data.</li><li>Choose the architecture.</li><li>Make a forward pass and check the loss (Disable regularization). Check if the loss is reasonable.</li><li>Add regularization, the loss should go up!</li><li>Disable the regularization again and take a small number of data and try to train the loss and reach zero loss.<ul><li>You should overfit perfectly for small datasets.</li></ul></li><li>Take your full training data, and small regularization then try some value of learning rate.<ul><li>If loss is barely changing, then the learning rate is small.</li><li>If you got <code>NAN</code> then your NN exploded and your learning rate is high.</li><li>Get your learning rate range by trying the min value (That can change) and the max value that doesn’t explode the network.</li></ul></li><li>Do Hyperparameters optimization to get the best hyperparameters values.</li></ol><p>Hyperparameter Optimization</p><ul><li>Try Cross validation strategy.<ul><li>Run with a few ephocs, and try to optimize the ranges.</li></ul></li><li>Its best to optimize in log space.</li><li>Adjust your ranges and try again.</li><li>Its better to try random search instead of grid searches (In log space)</li></ul><h3 id="training-neural-networks-ii"><a class="markdownIt-Anchor" href="#training-neural-networks-ii"></a> Training neural networks II</h3><p><strong>Optimization algorithms</strong>:</p><ul><li><p>Problems with stochastic gradient descent:随机梯度下降算法的问题</p><ul><li><p>if loss quickly in one direction and slowly in another (For only two variables), you will get very slow progress along shallow dimension, jitter along steep direction. Our NN will have a lot of parameters then the problem will be more.</p></li><li><p>Local minimum or saddle points；极小值或者鞍点问题</p><ul><li>何为鞍点？鞍点（Saddle point）在微分方程中，沿着某一方向是稳定的，另一条方向是不稳定的奇点，叫做鞍点。在泛函中，既不是极大值点也不是极小值点的临界点，叫做鞍点。在矩阵中，一个数在所在行中是最大值，在所在列中是最小值，则被称为鞍点。在物理上要广泛一些，指在一个方向是极大值，另一个方向是极小值的点</li></ul><ul><li>If SGD went into local minimum we will stuck at this point because the gradient is zero.遇到极小值点会stuck</li><li>Also in saddle points the gradient will be zero so we will stuck.</li><li>Saddle points says that at some point:鞍点在gradient上的表现<ul><li>Some gradients will get the loss up.</li><li>Some gradients will get the loss down.</li><li>And that happens more in high dimensional (100 million dimension for example)</li></ul></li><li>The problem of deep NN is more about saddle points than about local minimum because deep NN has high dimensions (Parameters)</li><li>Mini batches are noisy because the gradient is not taken for the whole batch.</li></ul></li></ul></li><li><p><strong>SGD + momentum</strong>:引入momentum，解决在鞍点或者极小值点处出现gradient为0而无法及继续更新参数的情况</p><ul><li><p>Build up velocity as a running mean of gradients:</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Computing weighted average. rho best is in range [0.9 - 0.99]</span></span><br><span class="line">V[t+<span class="number">1</span>] = rho * v[t] + dx</span><br><span class="line">x[t+<span class="number">1</span>] = x[t] - learningRate * V[t+<span class="number">1</span>]</span><br></pre></td></tr></table></figure></li><li><p><code>V[0]</code> is zero.</p></li><li><p>Solves the saddle point and local minimum problems.</p></li><li><p>It overshoots the problem and returns to it back.</p></li></ul></li><li><p><strong>Nestrov momentum</strong>:</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dx = compute_gradient(x)</span><br><span class="line">old_v = v</span><br><span class="line">v = rho * v - learning_rate * dx</span><br><span class="line">x+= -rho * old_v + (<span class="number">1</span>+rho) * v</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>AdaGrad</strong></p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(<span class="keyword">True</span>):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># here is a problem, the grad_squared isn't decayed (gets so large)</span></span><br><span class="line">  grad_squared += dx * dx</span><br><span class="line">  </span><br><span class="line">  x -= (learning_rate*dx) / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>RMSProp</strong></p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(<span class="keyword">True</span>):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#Solved ADAgra</span></span><br><span class="line">  grad_squared = decay_rate * grad_squared + (<span class="number">1</span>-grad_squared) * dx * dx  </span><br><span class="line">  </span><br><span class="line">  x -= (learning_rate*dx) / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure></li><li><p>People uses this instead of AdaGrad</p></li></ul></li><li><p><strong>Adam</strong>,最常用的优化器，结合了momentum和RMSProp</p><ul><li>Calculates the momentum and RMSProp as the gradients.</li><li>It need a Fixing bias to fix starts of gradients.</li><li>Is the best technique so far runs best on a lot of problems.</li><li>With <code>beta1 = 0.9</code> and <code>beta2 = 0.999</code> and <code>learning_rate = 1e-3</code> or <code>5e-4</code> is a great starting point for many models!</li></ul></li><li><p><strong>Learning decay</strong>学习率退火，避免因为网络的不断加深而导致学习率相对参数而言过大</p><ul><li>Ex. decay learning rate by half every few epochs.</li><li>To help the learning rate not to bounce out.</li><li>Learning decay is common with SGD+momentum but not common with Adam.</li><li>Dont use learning decay from the start at choosing your hyperparameters. Try first and check if you need decay or not.</li></ul></li></ul><p><strong>Regularization</strong>:损失函数中引入正则化项；集成学习；drop out修改网络结构；数据增强</p><ul><li>So far we have talked about reducing the training error, but we care about most is how our model will handle unseen data!上述优化更多的是在做如何更新参数使得error减少，但我们更加关心的是模型的泛化能力</li><li>What if the gab of the error between training data and validation data are too large?</li><li>This error is called high variance.</li><li>Model Ensemble:<ul><li>Algorithm:<ul><li>Train multiple independent models of the same architecture with different initializations.</li><li>At test time average their results.</li></ul></li><li>It can get you extra 2% performance.</li><li>It reduces the generalization error.</li><li>You can use some snapshots of your NN at the training ensembles them and take the results.</li></ul></li><li>Regularization solves the high variance problem. We have talked about L1, L2 Regularization.</li><li>L0-norm用于统计向量中非零元素的个数</li><li>Some Regularization techniques are designed for only NN and can do better.</li><li>Drop out:使得activation 函数失效，即让其输出在任何输入下都为0<ul><li>In each forward pass, randomly set some of the neurons to zero. Probability of dropping is a hyperparameter that are 0.5 for almost cases.训练过程中随机将部分神经元设置为失活</li><li>So you will chooses some activation and makes them zero.</li><li>It works because:<ul><li>It forces the network to have redundant representation; prevent co-adaption of features!</li><li>If you think about this, It ensemble some of the models in the same model!相当于集成学习，在一个模型中将他的多个不同的子模型进行集成</li></ul></li><li>At test time we might multiply each dropout layer by the probability of the dropout.</li><li>Sometimes at test time we don’t multiply anything and leave it as it is.</li><li>With drop out it takes more time to train.</li><li>Dropout是一种在深度学习环境中应用的正规化手段。它是这样运作的：在一次循环中我们先随机选择神经层中的一些单元并将其临时隐藏，然后再进行该次循环中神经网络的训练和优化过程。在下一次循环中，我们又将隐藏另外一些神经元，如此直至训练结束。<br>在训练时，每个神经单元以概率p被保留(dropout丢弃率为1-p)；在测试阶段，每个神经单元都是存在的，权重参数w要乘以p，成为：pw。测试时需要乘上p的原因：考虑第一隐藏层的一个神经元在dropout之前的输出是x，那么dropout之后的期望值是E=px+(1−p)0 ，在测试时该神经元总是激活，为了保持同样的输出期望值并使下一层也得到同样的结果，需要调整x→pxx→px. 其中p是Bernoulli分布（0-1分布）中值为1的概率</li></ul></li><li>Data augmentation:<ul><li>Another technique that makes Regularization.</li><li>Change the data!</li><li>For example flip the image, or rotate it.</li><li>Example in ResNet:<ul><li>Training: Sample random crops and scales:<ol><li>Pick random L in range [256,480]</li><li>Resize training image, short side = L</li><li>Sample random 224x244 patch.</li></ol></li><li>Testing: average a fixed set of crops<ol><li>Resize image at 5 scales: {224, 256, 384, 480, 640}</li><li>For each size, use 10 224x224 crops: 4 corners + center + flips</li></ol></li><li>Apply Color jitter or PCA</li><li>Translation, rotation, stretching.</li></ul></li></ul></li><li>Drop connect<ul><li>Like drop out idea it makes a regularization.</li><li>Instead of dropping the activation, we randomly zeroing the weights.</li></ul></li></ul><p><strong>Transfer learning</strong>:</p><ul><li>Some times your data is overfitted by your model because the data is small not because of regularization.自己的数据集太小</li><li>You need a lot of data if you want to train/use CNNs.</li><li>Steps of transfer learning<ol><li>Train on a big dataset that has common features with your dataset. Called pretraining.找到一个和你的小的数据集特征类似的打的数据集，并用你的模型在该数据集中进行训练</li><li>Freeze the layers except the last layer and feed your small dataset to learn only the last layer.将模型中除了最后一层外的所有层结构进行冻结，然后在小的数据集中进行训练，以学习最后一层</li><li>Not only the last layer maybe trained again, you can fine tune any number of layers you want based on the number of data you have</li></ol></li></ul></li></ul><h3 id="deep-learning-software"><a class="markdownIt-Anchor" href="#deep-learning-software"></a> Deep learning software</h3><ul><li><p>CPU vs GPU</p><ul><li><p>GPU The graphics card was developed to render graphics to play games or make 3D media,. etc.</p><ul><li>NVIDIA vs AMD<ul><li>Deep learning choose NVIDIA over AMD GPU because NVIDIA is pushing research forward deep learning also makes it architecture more suitable for deep learning.</li></ul></li></ul></li><li><p>CPU has fewer cores but each core is much faster and much more capable; great at sequential tasks. While GPUs has more cores but each core is much slower “dumber”; great for parallel tasks.CPU的核心更少，但是更快，胜任串行任务；GPU的核心更多，但是更慢，胜任并行任务</p></li><li><p>GPU cores needs to work together. and has its own memory.GPU各个核心需要并行工作，而且GPU有自己的内存，称为显存</p></li><li><p>Matrix multiplication is from the operations that are suited for GPUs. It has MxN independent operations that can be done on parallel.矩阵乘法适用于GPU中</p></li><li><p>Convolution operation also can be paralyzed because it has independent operations.卷积操作也能并行化</p></li><li><p>Programming GPUs frameworks:</p><ul><li><p>CUDA</p><p>(NVIDIA only)</p><ul><li>Write c-like code that runs directly on the GPU.</li><li>Its hard to build a good optimized code that runs on GPU. Thats why they provided high level APIs.</li><li>Higher level APIs: cuBLAS, cuDNN, etc</li><li><strong>CuDNN</strong> has implemented back prop. , convolution, recurrent and a lot more for you!</li><li>In practice you won’t write a parallel code. You will use the code implemented and optimized by others!</li></ul></li></ul></li><li><p>If you aren’t careful, training can bottleneck on reading data and transferring to GPU. So the solutions are:训练过程中在读取数据和迁移到gpu的过程中可能出现瓶颈</p><ul><li>Read all the data into RAM. # If possible将所有数据读如到内存</li><li>Use SSD instead of HDD使用固态硬盘</li><li>Use multiple CPU threads to prefetch data!使用多条CPU线程去预读取数据<ul><li>While the GPU are computing, a CPU thread will fetch the data for you.</li><li>A lot of frameworks implemented that for you because its a little bit painful!</li></ul></li></ul></li></ul></li><li><p><strong>Deep learning Frameworks</strong></p><ul><li>Its super fast moving!</li><li>Currently available frameworks:<ul><li>Tensorflow (Google)</li><li>Caffe (UC Berkeley)</li><li>Caffe2 (Facebook)</li><li>Torch (NYU / Facebook)</li><li>PyTorch (Facebook)</li><li>Theano (U monteral)</li><li>Paddle (Baidu)</li><li>CNTK (Microsoft)</li><li>MXNet (Amazon)</li></ul></li><li>The instructor thinks that you should focus on Tensorflow and PyTorch.</li><li>The point of deep learning frameworks:<ul><li>Easily build big computational graphs.方便地构建计算图</li><li>Easily compute gradients in computational graphs.方便地通过计算图计算梯度</li><li>Run it efficiently on GPU (cuDNN - cuBLAS)支持GPU</li></ul></li><li>Numpy doesn’t run on GPU.Numpy不能使用GPU</li><li>Most of the frameworks tries to be like NUMPY in the forward pass and then they compute the gradients for you.很多框架尽力去靠拢NUMPY，同时又能支持GPU</li></ul></li><li><p>**Tensorflow (Google)**静态架构</p><ul><li><p>Code are two parts:</p><ol><li>Define computational graph.定义好计算图</li><li>Run the graph and reuse it many times.运行计算图并多次使用</li></ol></li><li><p>Tensorflow uses a static graph architecture.静态的图结构</p></li><li><p>Tensorflow variables live in the graph. while the placeholders are feed each run.variable在计算图中生存，placeholders用于在计算图中占位，运行时填入数据</p></li><li><p>Global initializer function initializes the variables that lives in the graph.</p></li><li><p>Use predefined optimizers and losses.使用预先定义的优化器和损失函数</p></li><li><p>You can make a full layers with layers.dense function.</p></li><li><p>Keras</p><p>(High level wrapper):</p><ul><li>Keras is a layer on top pf Tensorflow, makes common things easy to do.</li><li>So popular!</li><li>Trains a full deep NN in a few lines of codes.</li></ul></li><li><p>There are a lot high level wrappers:</p><ul><li>Keras</li><li>TFLearn</li><li>TensorLayer</li><li>tf.layers <code>#Ships with tensorflow</code></li><li>tf-Slim <code>#Ships with tensorflow</code></li><li>tf.contrib.learn <code>#Ships with tensorflow</code></li><li>Sonnet <code># New from deep mind</code></li></ul></li><li><p>Tensorflow has pretrained models that you can use while you are using transfer learning.迁移学习的时候可以使用多个预训练模型</p></li><li><p>Tensorboard adds logging to record loss, stats. Run server and get pretty graphs! Tensorboard添加了日志用于跟踪损失</p></li><li><p>It has distributed code if you want to split your graph on some nodes.</p></li><li><p>Tensorflow is actually inspired from Theano. It has the same inspirations and structure.</p></li></ul></li><li><p><strong>PyTorch (Facebook)</strong></p><ul><li>Has three layers of abstraction:<ul><li>Tensor: ndarraybut runs on GPU,Like numpy arrays in tensorflow<ul><li>Variable: Node in a computational graphs; stores data and gradient <code>#Like Tensor, Variable, Placeholders</code></li></ul></li><li>Module: A NN layer; may store state or learnable weights<code>#Like tf.layers in tensorflow</code></li></ul></li><li>In PyTorch the graphs runs in the same loop you are executing which makes it easier for debugging. This is called a dynamic graph.动态图架构，容易进行debugging</li><li>In PyTorch you can define your own autograd functions by writing forward and backward for tensors. Most of the times it will implemented for you.一般只用重写forward函数</li><li>Torch.nn is a high level api like keras in tensorflow. You can create the models and go on and on.<ul><li>You can define your own nn module!</li></ul></li><li>Also Pytorch contains optimizers like tensorflow.</li><li>It contains a data loader that wraps a Dataset and provides minbatches, shuffling and multithreading.自己编写数据加载器很重要</li><li>PyTorch contains the best and super easy to use pretrained models</li><li>PyTorch contains Visdom that are like tensorboard. but Tensorboard seems to be more powerful.提供Visdom用于记录日志</li><li>PyTorch is new and still evolving compared to Torch. Its still in beta state.</li><li>PyTorch is best for research.对于research更加常用</li></ul></li><li><p>Tensorflow builds the graph once, then run them many times (Called static graph)定义一次网络即可多次使用，有专门的保存方式,在工业中更常用</p></li><li><p>In each PyTorch iteration we build a new graph (Called dynamic graph)每次使用都要重新搭建网络,在研究中更常用</p></li><li><p>Tensorflow fold make dynamic graphs easier in Tensorflow through dynamic batching.</p></li><li><p>Dynamic graph applications include: recurrent networks and recursive networks.</p></li><li><p>Caffe2 uses static graphs and can train model in python also works on IOS and Android</p></li><li><p>Tensorflow/Caffe2 are used a lot in production especially on mobile.</p></li></ul><h3 id="cnn-architectures"><a class="markdownIt-Anchor" href="#cnn-architectures"></a> CNN architectures</h3><ul><li>Focuses on CNN architectures that won ImageNet competition since 2012.</li></ul><p><img src="/images/181008/43.png" alt=""></p><ul><li><p>These architectures includes: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlexNet</a>, <a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">VGG</a>, <a href="https://research.google.com/pubs/pub43022.html" target="_blank" rel="noopener">GoogLeNet</a>, and <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">ResNet</a>.</p></li><li><p>Also we will discuss some interesting architectures as we go.</p></li><li><p>The first ConvNet that was made was <a href="http://ieeexplore.ieee.org/document/726791/" target="_blank" rel="noopener">LeNet-5</a> architectures are:by Yann Lecun at 1998.</p><ul><li>Architecture are: <code>CONV-POOL-CONV-POOL-FC-FC-FC</code></li></ul><p><img src="/images/181008/02.jpg" alt=""></p></li><li><p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener"><strong>AlexNet</strong></a> (2012):</p><ul><li>ConvNet that started the evolution and wins the ImageNet at 2012.</li><li>Architecture are: <code>CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MAXPOOL3-FC6-FC7-FC8</code></li><li>Contains exactly <strong>8</strong> layers the first 5 are Convolutional and the last 3 are fully connected layers.</li><li>Some other details:<ul><li>First use of RELU.</li><li>Norm layers but not used any more.</li><li>heavy data augmentation</li><li>Dropout <code>0.5</code></li><li>batch size <code>128</code></li><li>SGD momentum <code>0.9</code></li><li>Learning rate <code>1e-2</code> reduce by 10 at some iterations</li><li>7 CNN ensembles!</li></ul></li><li>AlexNet was trained on GTX 580 GPU with only 3 GB which wasn’t enough to train in one machine so they have spread the feature maps in half. The first AlexNet was distributed!</li><li>Its still used in transfer learning in a lot of tasks.</li><li>Total number of parameters are <code>60 million</code></li></ul></li><li><p><a href="https://arxiv.org/pdf/1409.1556" target="_blank" rel="noopener"><strong>VGGNet</strong></a> (2014) (Oxford)</p><ul><li>Deeper network with more layers.</li><li>Contains 19 layers.</li><li>Won on 2014 with GoogleNet with error 7.3%</li><li>Smaller filters with deeper layers.</li><li>The great advantage of VGG was the insight that multiple 3 × 3 convolution in sequence can emulate the effect of larger receptive fields, for examples 5 × 5 and 7 × 7.</li><li>Used the simple 3 x 3 Conv all through the network.</li></ul><p><img src="/images/181008/03.png" alt=""></p><ul><li><p>Has a similar details in training like AlexNet. Like using momentum and dropout.</p></li><li><p>在卷积神经网络中，感受野的定义是 卷积神经网络每一层输出的特征图（feature map）上的像素点在<strong>原始图像</strong>上映射的区域大小，即每一层feature的感受野都是对于原始输入图像而言的，而不是上一层的输入,但是在计算过程中，需要逐层计算该层在往上的每一层的感受野;公式 (N-1)_RF = f(N_RF, stride, ksize) = (N_RF - 1) * stride(convN) +ksize(convN)，其中，RF是感受野。N_RF和RF有点像，<strong>N代表 neighbour</strong>，指的是第n层的 a feature在n-1层的RF,显然第N层feature map在第N层的RF=1,在N-1层的RF=ksize__convN，计算RF时不需要考虑padding的影响。</p></li><li><p><strong>Feature Map的尺寸=(input_size + 2 * padding_size − ksize)/stride+1</strong></p><p><strong>根据定义 感受野是决定某一层输出结果中一个元素所对应的输入层的区域大小</strong></p><p><strong>这里指的是要求解的那层的一个元素也就是最初输入的out=1:</strong></p><p><strong>rfsize = f(out, stride, ksize) = (out - 1) * stride + ksize</strong></p><p><strong>感受野近似于用feature map为1时反推input_size ，只是不考虑padding</strong></p></li></ul></li><li><p><a href="https://research.google.com/pubs/pub43022.html" target="_blank" rel="noopener"><strong>GoogleNet</strong></a> (2014)</p><ul><li><p>Deeper network with more layers.</p></li><li><p>Contains 22 layers.</p></li><li><p>It has Efficient <strong>Inception</strong> module.</p></li><li><p>Only 5 million parameters! 12x less than AlexNet</p></li><li><p>Won on 2014 with VGGNet with error 6.7%</p></li><li><p>Inception module:内含并行操作的多种conv,将他们各自的输出在最后按depth堆叠成一个输出.</p><ul><li>Design a good local network topology (network within a network (NiN)) and then stack these modules on top of each other.</li><li>It consists of:<ul><li>Apply parallel filter operations on the input from previous layer<ul><li>Multiple convs of sizes (1 x 1, 3 x 3, 5 x 5)<ul><li>Adds padding to maintain the sizes.</li></ul></li><li>Pooling operation. (Max Pooling)<ul><li>Adds padding to maintain the sizes.</li></ul></li></ul></li><li>Concatenate all filter outputs together depth-wise.</li></ul></li><li>For example:<ul><li>Input for inception module is 28 x 28 x 256</li><li>Then the parallel filters applied:<ul><li>(1 x 1), 128 filter <code># output shape (28,28,128)</code></li><li>(3 x 3), 192 filter <code># output shape (28,28,192)</code></li><li>(5 x 5), 96 filter <code># output shape (28,28,96)</code></li><li>(3 x 3) Max pooling <code># output shape (28,28,256)</code></li></ul></li><li>After concatenation this will be <code>(28,28,672)</code></li></ul></li><li>By this design -We call Naive- it has a big computation complexity.<ul><li>The last example will make:<ul><li>[1 x 1 conv, 128] ==&gt; 28 * 28 * 128 * 1 * 1 * 256 = 25 Million approx</li><li>[3 x 3 conv, 192] ==&gt; 28 * 28 * 192 *3 *3 * 256 = 346 Million approx</li><li>[5 x 5 conv, 96] ==&gt; 28 * 28 * 96 * 5 * 5 * 256 = 482 Million approx</li><li>In total around 854 Million operation!</li></ul></li></ul></li><li>Solution:bottleneck layers that use 1x1 convolutions to reduce feature depth.<ul><li>Inspired from NiN (<a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="noopener">Network in network</a>)</li></ul></li></ul><p><img src="/images/181008/05.png" alt=""></p></li><li><p>So GoogleNet stacks this Inception module multiple times to get a full architecture of a network that can solve a problem without the Fully connected layers.</p></li><li><p>Just to mention, it uses an average pooling layer at the end before the classification step.</p></li><li><p>Full architecture:</p></li></ul><p><img src="/images/181008/43.png" alt=""></p><ul><li>In February 2015 Batch-normalized Inception was introduced as Inception V2. Batch-normalization computes the mean and standard-deviation of all feature maps at the output of a layer, and normalizes their responses with these values.</li></ul></li><li><p><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener"><strong>ResNet</strong></a> (2015) (Microsoft Research)</p><ul><li>152-layer model for ImageNet. Winner by 3.57% which is more than human level error.</li><li>This is also the very first time that a network of &gt; hundred, even 1000 layers was trained.</li><li>Swept all classification and detection competitions in ILSVRC’15 and COCO’15!</li><li>What happens when we continue stacking deeper layers on a “plain” Convolutional neural network?<ul><li>The deeper model performs worse, but it’s not caused by overfitting!网络变深,准确率反而下降</li><li>The learning stops performs well somehow because deeper NN are harder to optimize!网络越深，越难优化，容易出现vanish gradient</li></ul></li><li>The deeper model should be able to perform at least as well as the shallower model.</li><li>A solution by construction is copying the learned layers from the shallower model and setting additional layers to identity mapping.如何保证加深的网络能够学到新的知识，并保证旧的知识也保存了下来？identity mapping的存在就是这个价值。</li><li>Residual block:<ul><li>Microsoft came with the Residual block which has this architecture:</li></ul></li></ul></li></ul><p><img src="/images/181008/45.png" alt=""></p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Instead of us trying To learn a new representation, We learn only Residual</span></span><br><span class="line">Y = (W2* RELU(W1x+b1) + b2) + X</span><br></pre></td></tr></table></figure><ul><li>Say you have a network till a depth of N layers. You only want to add a new layer if you get something extra out of adding that layer.</li><li>One way to ensure this new (N+1)th layer learns something new about your network is to also provide the input(x) without any transformation to the output of the (N+1)th layer. This essentially drives the new layer to learn something different from what the input has already encoded.这样可以驱使新的网络层学习与前N层已经学到的东西不同的内容</li><li>The other advantage is such connections help in handling the Vanishing gradient problem in very deep networks.这样的结构有助于解决在很深的网络中导致的梯度消失问题</li><li>With the Residual block we can now have a deep NN of any depth without the fearing that we can’t optimize the network.</li><li>ResNet with a large number of layers started to use a bottleneck layer similar to the Inception bottleneck to reduce the dimensions.Resnet中包含了大量的类似inception bottleneck设计的bottlenect layer用于减少维度</li><li><img src="/images/181008/07.jpg" alt=""></li><li><strong>Full ResNet architecture</strong>:<ul><li>Stack residual blocks.</li><li><img src="/images/181008/08.png" alt=""></li><li>Every residual block has two 3 x 3 conv layers.</li><li>Additional conv layer at the beginning.</li><li>No FC layers at the end (only FC 1000 to output classes)</li><li>Periodically, double number of filters and downsample spatially using stride 2 (/2 in each dimension)</li><li>Training ResNet in practice:<ul><li>Batch Normalization after every CONV layer.</li><li>Xavier/2 initialization from He et al.</li><li>SGD + Momentum (<code>0.9</code>)</li><li>Learning rate: 0.1, divided by 10 when validation error plateaus</li><li>Mini-batch size <code>256</code></li><li>Weight decay of <code>1e-5</code></li><li>No dropout used.</li></ul></li></ul></li></ul></li><li><p><strong>ResNets Improvements</strong>:</p><ul><li>(2016) Identity Mappings in Deep Residual Networks<ul><li>From the creators of ResNet.</li><li>Gives better performance.</li></ul></li><li>(2016) Wide Residual Networks<ul><li>Argues that residuals are the important factor, not depth</li><li>50-layer wide ResNet outperforms 152-layer original ResNet</li><li>Increasing width instead of depth more computationally efficient (parallelizable)</li></ul></li><li>(2016) Deep Networks with Stochastic Depth<ul><li>Motivation: reduce vanishing gradients and training time through short networks during training.</li><li>Randomly drop a subset of layers during each training pass</li><li>Use full deep network at test time.</li></ul></li></ul></li><li><p><strong>Beyond ResNets</strong>:</p><ul><li>(2017) FractalNet: Ultra-Deep Neural Networks without Residuals<ul><li>Argues that key is transitioning effectively from shallow to deep and residual representations are not necessary.</li><li>Trained with dropping out sub-paths</li><li>Full network at test time.</li></ul></li><li>(<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">2017</a>) Densely Connected Convolutional Networks</li><li>(2017) SqueezeNet: AlexNet-level Accuracy With 50x Fewer Parameters and &lt;0.5Mb Model Size<ul><li>Good for production.</li><li>It is a re-hash of many concepts from ResNet and Inception, and show that after all, a better design of architecture will deliver small network sizes and parameters without needing complex compression algorithms.</li></ul></li></ul></li><li><p>Conclusion:</p><ul><li>ResNet current best default.</li><li>Trend towards extremely deep networks</li><li>In the last couple of years, some models all using the shortcuts like “ResNet” to eaisly flow the gradients.</li></ul></li></ul><h2 id="recurrent-neural-networks"><a class="markdownIt-Anchor" href="#recurrent-neural-networks"></a> Recurrent Neural networks</h2><ul><li><p>Recurrent Neural Networks RNN Models:</p></li><li><p><img src="/images/181008/46.jpg" alt=""></p><ul><li>One to many<ul><li>Example: Image Captioning<ul><li>image ==&gt; sequence of words</li></ul></li></ul></li><li>Many to One<ul><li>Example: Sentiment Classification<ul><li>sequence of words ==&gt; sentiment</li></ul></li></ul></li><li>Many to many<ul><li>Example: Machine Translation<ul><li>seq of words in one language ==&gt; seq of words in another language</li></ul></li><li>Example: Video classification on frame level</li></ul></li></ul></li><li><p>So what is a recurrent neural network?</p><ul><li><p>Recurrent core cell that take an input x and that cell has an internal state that are updated each time it reads an input.</p></li><li><p><img src="/images/181008/47.jpg" alt=""></p></li><li><p>The RNN block should return a vector.</p></li><li><p>We can process a sequence of vectors x by applying a recurrence formula at every time step:</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h[t] = fw (h[t-1], x[t])# Where fw is some function with parameters W</span><br></pre></td></tr></table></figure></li><li><p>The same function and the same set of parameters are used at every time step.</p></li></ul></li><li><p>(Vanilla) Recurrent Neural Network:</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">h[t] = tanh (W[h,h]*h[t-1] + W[x,h]*x[t])    # Then we save h[t]</span><br><span class="line">y[t] = W[h,y]*h[t]</span><br></pre></td></tr></table></figure></li><li><p>This is the simplest example of a RNN.</p></li></ul></li><li><p>RNN works on a sequence of related data.</p></li></ul></li><li><p>Recurrent NN Computational graph:</p></li><li><p><img src="/images/181008/10.jpg" alt=""></p><ul><li><code>h0</code> are initialized to zero.</li><li>Gradient of <code>W</code> is the sum of all the <code>W</code> gradients that has been calculated!</li><li>A many to many graph:</li></ul></li></ul><p>​</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Something new to me when I read such a good notebook about &lt;a href=&quot;http
      
    
    </summary>
    
      <category term="cs231n" scheme="http://aier02.com/categories/cs231n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>optimization</title>
    <link href="http://aier02.com/2018/10/05/optimization/"/>
    <id>http://aier02.com/2018/10/05/optimization/</id>
    <published>2018-10-05T12:44:14.537Z</published>
    <updated>2018-10-05T15:46:13.106Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h1 id="optimization"><a class="markdownIt-Anchor" href="#optimization"></a> Optimization</h1><ul><li>Optimization is the process of finding the set of parameters W that minimize the loss function,如何改变w使得损失函数不断减小</li></ul><h4 id="strategy-1-random-search"><a class="markdownIt-Anchor" href="#strategy-1-random-search"></a> Strategy #1 random search</h4><ul><li>核心思想:一次性找到使得损失函数最小的w取值貌似很难？(random search,a bad idea），但是iterative refinement直观上是可行的，故可以给予w一个random 值，然后迭代的改进，使得loss每次都比上次要小。Our strategy will be to start with random weights and iteratively refine them over time to get lower loss</li><li>类似于一个戴眼罩的hiker，在多个角度下尝试往下走，直到山底。如在CIFAR-10中，hills are 30730 dimensional.</li></ul><h4 id="strategy-2-random-local-search"><a class="markdownIt-Anchor" href="#strategy-2-random-local-search"></a> Strategy #2 random local search</h4><ul><li>策略一相当于随意选定某条路径前进，若根据该路径直接走到的地方更低，则选择该方向最优；而策略二相当于先随意选定某条路径，然后循环地在这个路径上进行随意的方向修改，若走到的地方更低，则进行路径的修改，直到循环结束。</li></ul><h4 id="strategy-3-follow-gradient"><a class="markdownIt-Anchor" href="#strategy-3-follow-gradient"></a> Strategy #3 follow gradient</h4><ul><li>不用刻意规划路线，而是要找到每一步的最优方向，即loss下降的方向gradinet，也就是当时脚下的hills的斜坡方向</li><li>所谓gradient就是a generalization of slope for functions that don’t take a single number but a vector of numbers，由在所求函数对整个输入空间中的各个维度的derivative，即偏导数组成。一维导数的定义:</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>d</mi><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mo>=</mo><msub><mi>lim</mi><mrow><mi>h</mi><mtext></mtext><mo>→</mo><mn>0</mn></mrow></msub><mfrac><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo>)</mo><mo>−</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>h</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em"></span><span class="strut bottom" style="height:2.179108em;vertical-align:-.7521079999999999em"></span><span class="base displaystyle textstyle uncramped"><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit">d</span><span class="mord mathit">x</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit">d</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mrel">=</span><span class="mop op-limits"><span class="vlist"><span style="top:.6521079999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">h</span><span class="mord mspace"> </span><span class="mrel">→</span><span class="mord mathrm">0</span></span></span></span><span style="top:-2.7755575615628914e-17em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="mop">lim</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit">h</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mbin">+</span><span class="mord mathit">h</span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><ul><li>When the functions of interest take a vector of numbers instead of a single number,(即自变量不止一个,对不同的变量进行求导) we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension.</li><li>gradient是函数增长速率最快的方向,The gradient tells us the slope of the loss function along every dimension, which we can use to make an update。</li></ul><h1 id="computing-gradient"><a class="markdownIt-Anchor" href="#computing-gradient"></a> Computing gradient</h1><p>两种方式:数值解和分析解</p><h4 id="numerical-gradient"><a class="markdownIt-Anchor" href="#numerical-gradient"></a> Numerical gradient</h4><ul><li>实质是根据一维导数的公式进行计算，每次在输入x的一个维度上进行数值求解，即让该维度上旧的值加上指定的h(尽可能小，公式中是goes toward zero)，算得f(x+h)后再算(f(x+h)-f(x))/h，以求得在该维度上的偏导数的近似值,计算完某个维度后注意要设置回原来的输入再进行下个维度的计算.</li><li><code>it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])</code>创建了一个numpy数组的迭代器，这里的flags表示对数组进行多重索引，op_flags表示迭代器对数组x可执行读写操作。<code>print it.multi_index</code>可以得到数组元素所有的索引(以元组形式返回)，如(0,0),(0,1)</li><li>在实际应用中，计算梯度的时候常用centered difference formula</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo>)</mo><mo>−</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>−</mo><mi>h</mi><mo>)</mo></mrow><mrow><mn>2</mn><mi>h</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{f(x+h)-f(x-h)}{2h}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em"></span><span class="strut bottom" style="height:2.113em;vertical-align:-.686em"></span><span class="base displaystyle textstyle uncramped"><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">2</span><span class="mord mathit">h</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mbin">+</span><span class="mord mathit">h</span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mbin">−</span><span class="mord mathit">h</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><ul><li>根据gradient，在每个维度上朝着loss funcitn增长速率最快的方向的负方向进行step_size的更新。Update in negative gradient direction. In the code above, notice that to compute W_new we are making an update in the negative direction of the gradient df since we wish our loss function to decrease, not increase.</li><li>step_size的作用:the gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step</li><li>效率问题:显然对于每个维度(数组索引)进行数值求解，则对于gradient的求解是O(n)，n为w的维度，或者<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.716668em;vertical-align:-.286108em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:.05724em">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>的个数；对于神经网络而言，参数个数太多，数值解扩展性太差。</li></ul><h4 id="analytic-gradient"><a class="markdownIt-Anchor" href="#analytic-gradient"></a> Analytic gradient</h4><p>通过微积分进行导数的推导,得到导数确切的值，而不是近似解。</p><ul><li>求导数容易出错，所以经常讲分析解和数值解进行比较，称为gradient check。</li><li>举例SVM loss function，对于单个示例<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.58056em;vertical-align:-.15em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>:</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub><mo>=</mo><msub><mo>∑</mo><mrow><mi>j</mi><mo>≠</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mrow><mo fence="true">[</mo><mi>max</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><msubsup><mi>w</mi><mi>j</mi><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>w</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mo>)</mo><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.050005em"></span><span class="strut bottom" style="height:2.51771em;vertical-align:-1.467705em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mop op-limits"><span class="vlist"><span style="top:1.2172049999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span><span class="mrel">≠</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.000005000000000032756em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0"><span class="delimsizing size1">[</span></span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord mathrm">Δ</span><span class="mclose">)</span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0"><span class="delimsizing size1">]</span></span></span></span></span></span></span></p><ul><li>对于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub></mrow><annotation encoding="application/x-tex">w_{y_i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.716668em;vertical-align:-.286108em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>,</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mrow><msub><mi>w</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub></mrow></msub><msub><mi>L</mi><mi>i</mi></msub><mo>=</mo><mo>−</mo><mrow><mo fence="true">(</mo><msub><mo>∑</mo><mrow><mi>j</mi><mo>≠</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mrow><mn mathvariant="normal">1</mn></mrow><mo>(</mo><msubsup><mi>w</mi><mi>j</mi><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>w</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mo>&gt;</mo><mn>0</mn><mo>)</mo><mo fence="true">)</mo></mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:2.05002em"></span><span class="strut bottom" style="height:3.60004em;vertical-align:-1.55002em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:.14999999999999997em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.31472em;margin-right:.1em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptscriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord">−</span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped"><span class="delimsizing mult"><span class="vlist"><span style="top:.9049999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="delimsizinginner delim-size4"><span>⎝</span></span></span><span style="top:-.89502em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="delimsizinginner delim-size4"><span>⎛</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.2172049999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span><span class="mrel">≠</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.000005000000000032756em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">1</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord mathrm">Δ</span><span class="mrel">&gt;</span><span class="mord mathrm">0</span><span class="mclose">)</span><span class="style-wrap reset-textstyle textstyle uncramped"><span class="delimsizing mult"><span class="vlist"><span style="top:.9049999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="delimsizinginner delim-size4"><span>⎠</span></span></span><span style="top:-.89502em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="delimsizinginner delim-size4"><span>⎞</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></span></p><p>where 𝟙 is the indicator function that is one if the condition inside is true or zero otherwise<br>对于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">w_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.716668em;vertical-align:-.286108em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>,</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mrow><msub><mi>w</mi><mi>j</mi></msub></mrow></msub><msub><mi>L</mi><mi>i</mi></msub><mo>=</mo><mrow><mn mathvariant="normal">1</mn></mrow><mo>(</mo><msubsup><mi>w</mi><mi>j</mi><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>w</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mo>&gt;</mo><mn>0</mn><mo>)</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.891331em"></span><span class="strut bottom" style="height:1.274439em;vertical-align:-.383108em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:.14999999999999997em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">1</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord mathrm">Δ</span><span class="mrel">&gt;</span><span class="mord mathrm">0</span><span class="mclose">)</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></span></p><ul><li>Once you derive the expression for the gradient it is straight-forward to implement the expressions and use them to perform the gradient update</li></ul><h1 id="gradient-descent"><a class="markdownIt-Anchor" href="#gradient-descent"></a> Gradient descent</h1><ul><li>the procedure of repeatedly evaluating the gradient and then performing a parameter update is called Gradient Descent</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">while True:</span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data, weights)</span><br><span class="line">  weights += - step_size * weights_grad # perform parameter update</span><br></pre></td></tr></table></figure><ul><li>Mini-batch gradient descent.对于输入数据量庞大的training set，为了更新w的某个索引下的值而对庞大的data进行操作有点浪费，常用的方法是compute the gradient over batches of the training data</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">while True:</span><br><span class="line">  data_batch = sample_training_data(data, 256) # sample 256 examples</span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)</span><br><span class="line">  weights += - step_size * weights_grad # perform parameter update</span><br></pre></td></tr></table></figure><ul><li>为何在一小部分的数据中更新w也是可行的？The reason this works well is that the examples in the training data are correlated</li><li>The extreme case of this is a setting where the mini-batch contains only a single example. This process is called Stochastic Gradient Descent (SGD) (or also sometimes on-line gradient descent)比较少见</li></ul><h1 id="summary"><a class="markdownIt-Anchor" href="#summary"></a> Summary</h1><p><img src="/images/181005/dataflow.jpeg" alt=""></p><ul><li>两种求gradient的方法:We discussed the tradeoffs between computing the numerical and analytic gradient. The numerical gradient is simple but it is approximate and expensive to compute. The analytic gradient is exact, fast to compute but more error-prone since it requires the derivation of the gradient with math. Hence, in practice we always use the analytic gradient and then perform a gradient check, in which its implementation is compared to the numerical gradient</li><li>梯度下降方法:We introduced the Gradient Descent algorithm which iteratively computes the gradient and performs a parameter update in loop</li><li>总的来说优化就是通过寻找loss function下降速度最快的方向(gradient的反方向)，对w进行不断的适当幅度(learning rate)的更新，使得loss不断减少。</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;optimization&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#optimization&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="cs231n" scheme="http://aier02.com/categories/cs231n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
      <category term="image classification" scheme="http://aier02.com/tags/image-classification/"/>
    
  </entry>
  
  <entry>
    <title>Linear classification</title>
    <link href="http://aier02.com/2018/10/03/linear_classification/"/>
    <id>http://aier02.com/2018/10/03/linear_classification/</id>
    <published>2018-10-03T05:31:44.756Z</published>
    <updated>2018-12-09T06:57:25.313Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h2 id="linear-classification"><a class="markdownIt-Anchor" href="#linear-classification"></a> Linear classification</h2><h3 id="multiclass-svm"><a class="markdownIt-Anchor" href="#multiclass-svm"></a> Multiclass SVM</h3><ul><li>基本形式为y=wx+b，此时的x为列向量，一列为一个样本，w的每一行为一个class的template。</li><li>loss function:Multiclass Support Vector Machine (SVM) loss,SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin Δ;Δ为超参，需要人为设定，它的存在说明多类svm关注的和普通的svm思想上是一致的，都是关注距离超平面一定范围内的误分类点，也就是间隔边界内的点，所以这里的损失函数和合页损失函数的设计是一样的；故第i张图像的损失函数为</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub><mo>=</mo><msub><mo>∑</mo><mrow><mi>j</mi><mo>≠</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mi>max</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><msub><mi>s</mi><mi>j</mi></msub><mo>−</mo><msub><mi>s</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.050005em"></span><span class="strut bottom" style="height:2.51771em;vertical-align:-1.467705em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mop op-limits"><span class="vlist"><span style="top:1.2172049999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span><span class="mrel">≠</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.000005000000000032756em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord mathrm">Δ</span><span class="mclose">)</span></span></span></span></span></p><p>注意这里的sj表示的是该图像在第j类的得分，而yi表示的是该图像的label，即只要计算其错误分类的所有分数和正确分类的分数的差值之和，当label类的得分没有大于某个非label类得分margin时，两者的差值会被算入loss中。<br><img src="/images/181002/margin.jpg" alt=""></p><ul><li>引入regularization</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mo>(</mo><mi>W</mi><mo>)</mo><mo>=</mo><msub><mo>∑</mo><mi>k</mi></msub><msub><mo>∑</mo><mi>l</mi></msub><msubsup><mi>W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>l</mi></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">R(W) = \sum_k\sum_l W_{k,l}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.0500050000000005em"></span><span class="strut bottom" style="height:2.3521180000000004em;vertical-align:-1.3021129999999999em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.00773em">R</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.13889em">W</span><span class="mclose">)</span><span class="mrel">=</span><span class="mop op-limits"><span class="vlist"><span style="top:1.2021129999999998em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.03148em">k</span></span></span><span style="top:-.000005000000000254801em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.2021129999999998em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.01968em">l</span></span></span><span style="top:-.000005000000000254801em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:.13889em">W</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:.03148em">k</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.01968em">l</span></span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></span></p><ul><li>完整的multicalss SVM loss:</li></ul>L = \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss}<ul><li>扩展形式为:</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mi>N</mi></mrow></mfrac><msub><mo>∑</mo><mi>i</mi></msub><msub><mo>∑</mo><mrow><mi>j</mi><mo>≠</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mrow><mo fence="true">[</mo><mi>max</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">;</mo><mi>W</mi><msub><mo>)</mo><mi>j</mi></msub><mo>−</mo><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">;</mo><mi>W</mi><msub><mo>)</mo><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mo>)</mo><mo fence="true">]</mo></mrow><mo>+</mo><mi>λ</mi><msub><mo>∑</mo><mi>k</mi></msub><msub><mo>∑</mo><mi>l</mi></msub><msubsup><mi>W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>l</mi></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.32144em"></span><span class="strut bottom" style="height:2.789145em;vertical-align:-1.467705em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">L</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span style="top:-.000005000000000143778em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.2172049999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span><span class="mrel">≠</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.000005000000000032756em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0">[</span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">;</span><span class="mord mathit" style="margin-right:.13889em">W</span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">;</span><span class="mord mathit" style="margin-right:.13889em">W</span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord mathrm">Δ</span><span class="mclose">)</span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0">]</span></span><span class="mbin">+</span><span class="mord mathit">λ</span><span class="mop op-limits"><span class="vlist"><span style="top:1.2021129999999998em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.03148em">k</span></span></span><span style="top:-.000005000000000254801em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.2021129999999998em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.01968em">l</span></span></span><span style="top:-.000005000000000254801em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:.13889em">W</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:.03148em">k</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.01968em">l</span></span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.69444em;vertical-align:0"></span><span class="base textstyle uncramped"><span class="mord mathit">λ</span></span></span></span>为超参，常用cross validation确定，表示模型的一种偏好。</p><ul><li>L2范数作为正则化项的好处是:The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself,即在wx+b得分一样的情况下，L2范数的模型偏向于选择smaller and diffuse weights，使得没有哪个维度影响很大。</li><li>setting delta:一般设置为1.0是安全的，在loss function中delta和lambda其实具有相同effect在tradeoff上，所以真正有意义的是对于lambda的控制</li><li>与二分类的svm比较:</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub><mo>=</mo><mi>C</mi><mi>max</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mi>w</mi><mi>T</mi></msup><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><mo>+</mo><mi>R</mi><mo>(</mo><mi>W</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">L_i = C \max(0, 1 - y_i w^Tx_i) + R(W)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.8913309999999999em"></span><span class="strut bottom" style="height:1.1413309999999999em;vertical-align:-.25em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord mathit" style="margin-right:.07153em">C</span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mbin">−</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:-.413em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:.00773em">R</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.13889em">W</span><span class="mclose">)</span></span></span></span></span></p><p>这里的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mo>{</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo>}</mo></mrow><annotation encoding="application/x-tex">y_i \in \{ -1,1 \}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">∈</span><span class="mopen">{</span><span class="mord">−</span><span class="mord mathrm">1</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mclose">}</span></span></span></span></p><h3 id="softmax-classifier"><a class="markdownIt-Anchor" href="#softmax-classifier"></a> Softmax classifier</h3><ul><li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">;</mo><mi>W</mi><mo>)</mo><mo>=</mo><mi>W</mi><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f(x_i; W) = W x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">;</span><span class="mord mathit" style="margin-right:.13889em">W</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:.13889em">W</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>和svm保持一致，但经过softmax层用于指示概率的大小，损失函数由hinge loss变成cross-entropy loss</li></ul>L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}<ul><li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>f</mi><mi>j</mi></msub><mo>(</mo><mi>z</mi><mo>)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>z</mi><mi>j</mi></msub></mrow></msup></mrow><mrow><msub><mo>∑</mo><mi>k</mi></msub><msup><mi>e</mi><mrow><msub><mi>z</mi><mi>k</mi></msub></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.939285em"></span><span class="strut bottom" style="height:1.4942920000000002em;vertical-align:-.555007em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.04398em">z</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.34500000000000003em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mop"><span class="op-symbol small-op mop" style="top:.074995em">∑</span><span class="vlist"><span style="top:.30001em;margin-right:.07142857142857144em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.03148em">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.3574928571428571em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.04398em">z</span><span class="vlist"><span style="top:.34963999999999995em;margin-right:.1em;margin-left:-.04398em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptscriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.03148em">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.394em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.47143571428571435em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.04398em">z</span><span class="vlist"><span style="top:.31472000000000006em;margin-right:.1em;margin-left:-.04398em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptscriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>称为softmax function,这里的z即为wx+b，整个softmax function estimated class probabilities，即unnormalized log probabilities</li><li>交叉熵:p为true distibution，q为estimated distribution</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mo>(</mo><mi>p</mi><mo separator="true">,</mo><mi>q</mi><mo>)</mo><mo>=</mo><mo>−</mo><msub><mo>∑</mo><mi>x</mi></msub><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo><mi>log</mi><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">H(p,q) = - \sum_x p(x) \log q(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.050005em"></span><span class="strut bottom" style="height:2.3000100000000003em;vertical-align:-1.250005em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.08125em">H</span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.03588em">q</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord">−</span><span class="mop op-limits"><span class="vlist"><span style="top:1.150005em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">x</span></span></span><span style="top:-.000005000000000032756em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mord mathit" style="margin-right:.03588em">q</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p><ul><li>Practical issues: Numeric stability:</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>f</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub></mrow></msup></mrow><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><mrow><msub><mi>f</mi><mi>j</mi></msub></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>C</mi><msup><mi>e</mi><mrow><msub><mi>f</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub></mrow></msup></mrow><mrow><mi>C</mi><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><mrow><msub><mi>f</mi><mi>j</mi></msub></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>f</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mo>+</mo><mi>log</mi><mi>C</mi></mrow></msup></mrow><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><mrow><msub><mi>f</mi><mi>j</mi></msub><mo>+</mo><mi>log</mi><mi>C</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{e^{f_{y_i}}}{\sum_j e^{f_j}} = \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}} = \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.533218em"></span><span class="strut bottom" style="height:2.655414em;vertical-align:-1.122196em"></span><span class="base displaystyle textstyle uncramped"><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686078em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mop"><span class="op-symbol small-op mop" style="top:-.0000050000000000050004em">∑</span><span class="vlist"><span style="top:.30001em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.30997em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.37011em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.31472em;margin-right:.1em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptscriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686078em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:.07153em">C</span><span class="mop"><span class="op-symbol small-op mop" style="top:-.0000050000000000050004em">∑</span><span class="vlist"><span style="top:.30001em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.30997em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:.07153em">C</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.37011em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.31472em;margin-right:.1em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptscriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686078em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mop"><span class="op-symbol small-op mop" style="top:-.0000050000000000050004em">∑</span><span class="vlist"><span style="top:.30001em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.30997em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mord mathit" style="margin-right:.07153em">C</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.37011em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.31472em;margin-right:.1em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptscriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mord mathit" style="margin-right:.07153em">C</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><p>常用的设置方法是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mi>C</mi><mo>=</mo><mo>−</mo><msub><mi>max</mi><mi>j</mi></msub><msub><mi>f</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\log C = -\max_j f_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.980548em;vertical-align:-.286108em"></span><span class="base textstyle uncramped"><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mord mathit" style="margin-right:.07153em">C</span><span class="mrel">=</span><span class="mord">−</span><span class="mop"><span class="mop">max</span><span class="vlist"><span style="top:.15em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></p><ul><li>The Softmax classifier gets its name from the softmax function, which is used to squash the raw class scores into normalized positive values that sum to one, so that the cross-entropy loss can be applied.</li></ul><h3 id="svm-vs-softmax"><a class="markdownIt-Anchor" href="#svm-vs-softmax"></a> SVM vs. Softmax</h3><ul><li>主要区别在有loss function:</li></ul><p><img src="/images/181003/svmvssoftmax.png" alt=""></p><p>两者的分数向量f都是一样的，不同的在于对f的解释，svm认为f是对应种类的得分，hinge loss鼓励正确的class得分比所有错误的class score都高出一个margin；而softmax认为f是在没有标准化之前表示的是属于某个种类的log概率，并且cross entropy鼓励正确的正确分类的概率大(-log§变小)</p><ul><li>Hence, the probabilities computed by the Softmax classifier are better thought of as confidences where, similar to the SVM</li></ul><h3 id="further-reading"><a class="markdownIt-Anchor" href="#further-reading"></a> Further Reading</h3><p><a href="https://arxiv.org/abs/1306.0239" target="_blank" rel="noopener">Deep Learning using Linear Support Vector Machines</a> from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;linear-classification&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#linear-c
      
    
    </summary>
    
      <category term="cs231n" scheme="http://aier02.com/categories/cs231n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
      <category term="linear classification" scheme="http://aier02.com/tags/linear-classification/"/>
    
      <category term="SVM" scheme="http://aier02.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>pytorch cookbook U2&amp;U3</title>
    <link href="http://aier02.com/2018/10/02/pytorch_cookbook-U2&amp;U3/"/>
    <id>http://aier02.com/2018/10/02/pytorch_cookbook-U2&amp;U3/</id>
    <published>2018-10-02T08:27:32.398Z</published>
    <updated>2018-11-07T07:21:58.471Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h1 id="pytorch-cookbook"><a class="markdownIt-Anchor" href="#pytorch-cookbook"></a> pytorch-cookbook</h1><h2 id="第二章"><a class="markdownIt-Anchor" href="#第二章"></a> 第二章</h2><ul><li><p>函数名后带下划线会修改函数本身如y.add_(x)会直接修改y</p></li><li><p>pytorch的tensor和numpy的对象共享内存，两者同时改变;对于tensor不支持的操作，可以先转为numpy进行操作在转为tensor（tensor支持gpu）</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=t.ones(5)</span><br><span class="line">b = a.numpy() # Tensor -&gt; Numpy</span><br><span class="line">a = np.ones(5)</span><br><span class="line">b = t.from_numpy(a) # Numpy-&gt;Tensor</span><br></pre></td></tr></table></figure><ul><li>tensor[idx]得到的为0-dim的tensor，scalar.item()获取tensor的单个元素对象</li><li>t.Tensor(5,3)创建5行3列的tensor，t.tensor([3,4])创建包含3，4两个元素的tensor</li><li>t.tensor()会进行数据拷贝，新的tensor和旧的不共享内存，而torch.from_numpy（）或者tensor.detach()则相反</li><li>使用gpu</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = t.device(&quot;cuda:0&quot; if t.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line">x = x.to(device)</span><br><span class="line">y = y.to(device)</span><br></pre></td></tr></table></figure><ul><li>autograd: 自动微分;要想使得Tensor使用autograd功能，只需要设置<code>tensor.requries_grad=True</code>.如：x = t.ones(2, 2, requires_grad=True)</li><li>注意：<code>grad</code>在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以反向传播之前需把梯度清零。# 以下划线结束的函数是inplace操作，会修改自身的值，就像add__</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad.data.zero_()</span><br></pre></td></tr></table></figure><ul><li>一起求导的过程示例</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x=t.ones(2,2,requires_grad=True)#生成tensor</span><br><span class="line">y=x.sum()#定义表达式</span><br><span class="line">y.grad_fn#查看求导函数</span><br><span class="line">y.backward()#back propagation</span><br><span class="line">x.grad#查看y对x的导数</span><br><span class="line">x.grad.data_zero_()#清空导数缓存空间</span><br></pre></td></tr></table></figure><ul><li>nerual network的定义主要是对torch.nn模块的使用,<br>定义网络时，需要继承<code>nn.Module</code>，并实现它的forward方法，把网络中具有可学习参数的层放在构造函数<code>__init__</code>中。如果某一层(如ReLU)不具有可学习的参数，则既可以放在构造函数中，也可以不放，但建议不放在其中，而在forward中使用<code>nn.functional</code>代替,forwar的输入和输出都是tensor,input = t.randn(1, 1, 32, 32),需要注意的是，torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用 <code>input.unsqueeze(0)</code>将batch_size设为１,size形式为nSamples x nChannels x height x weight</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # nn.Module子类的函数必须在构造函数中执行父类的构造函数</span><br><span class="line">        # 下式等价于nn.Module.__init__(self)</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        </span><br><span class="line">        # 卷积层 &apos;1&apos;表示输入图片为单通道, &apos;6&apos;表示输出通道数，&apos;5&apos;表示卷积核为5*5</span><br><span class="line">        self.conv1 = nn.Conv2d(1, 6, 5) </span><br><span class="line">        # 卷积层</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 5) </span><br><span class="line">        # 仿射层/全连接层，y = Wx + b</span><br><span class="line">        self.fc1   = nn.Linear(16*5*5, 120) </span><br><span class="line">        self.fc2   = nn.Linear(120, 84)</span><br><span class="line">        self.fc3   = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x): </span><br><span class="line">        # 卷积 -&gt; 激活 -&gt; 池化 </span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), 2) </span><br><span class="line">        # reshape，‘-1’表示自适应</span><br><span class="line">        x = x.view(x.size()[0], -1) </span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)        </span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure><ul><li>conv layer主要特征是局部连接和权重共享</li><li></li></ul><p>局部连接：每个神经元仅与输入神经元的一块区域连接，这块局部区域称作感受野（receptive field）。在图像卷积操作中，即神经元在空间维度（spatial dimension，即上图示例H和W所在的平面）是局部连接，但在深度上是全部连接。对于二维图像本身而言，也是局部像素关联较强。这种局部连接保证了学习后的过滤器能够对于局部的输入特征有最强的响应。局部连接的思想，也是受启发于生物学里面的视觉系统结构，视觉皮层的神经元就是局部接受信息的。<br>*<br>权重共享：计算同一个深度切片的神经元时采用的滤波器是共享的。例上图中计算o[:,:,0]的每个每个神经元的滤波器均相同，都为W0，这样可以很大程度上减少参数。共享权重在一定程度上讲是有意义的，例如图片的底层边缘特征与特征在图中的具体位置无关。但是在一些场景中是无意的，比如输入的图片是人脸，眼睛和头发位于不同的位置，希望在不同的位置学到不同的特征 。请注意权重只是对于同一深度切片的神经元是共享的，在卷积层，通常采用多组卷积核提取不同特征，即对应不同深度切片的特征，不同深度切片的神经元权重是不共享。另外，偏重对同一深度切片的所有神经元都是共享的。</p><ul><li>池化是非线性下采样的一种形式，主要作用是通过减少网络的参数来减小计算量，并且能够在一定程度上控制过拟合。</li><li>网络的可学习参数通过<code>net.parameters()</code>返回,<code>net.named_parameters</code>可同时返回可学习的参数及名称。</li><li>nn.MSELoss()实现均方误差，nn.CrossEntropyLoss()实现交叉熵损失</li><li>优化器更新参数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> import torch.optim as optim</span><br><span class="line">#新建一个优化器，指定要调整的参数和学习率</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr = 0.01)</span><br><span class="line"></span><br><span class="line"># 在训练过程中</span><br><span class="line"># 先梯度清零(与net.zero_grad()效果一样)</span><br><span class="line">optimizer.zero_grad() </span><br><span class="line"></span><br><span class="line"># 计算损失</span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line">#反向传播</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">#更新参数</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><ul><li>数据加载和预处理：使用torchvision</li><li>示例：下面我们来尝试实现对CIFAR-10数据集的分类，步骤如下:</li></ul><ol><li>使用torchvision加载并预处理CIFAR-10数据集,得到dataset和dataloader</li><li>定义网络,继承nn.Module,init中写入可学习的参数函数，forward定义好前向传播的过程</li><li>定义损失函数和优化器，criterion和optimizer</li><li>训练网络并更新网络参数，在每个ephco中加载数据，传入net，算loss，loss.backward，optimizer.step</li><li>测试网络</li></ol><ul><li></li></ul><p>定义对数据的预处理:将两种转化合并一起；ToTensor()将shape为(H, W, C)的nump.ndarray或img转为shape为(C, H, W)的tensor，其将每一个数值归一化到[0,1]，其归一化方法比较简单，直接除以255即可，加入normalize则其作用就是先将输入归一化到(0,1)，再使用公式”(x-mean)/std”，将每个元素分布到(-1,1),函数normalize（std,mean）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">        transforms.ToTensor(), # 转为Tensor</span><br><span class="line">        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化</span><br><span class="line">                             ])</span><br></pre></td></tr></table></figure><ul><li>Dataset对象是一个数据集，可以按下标访问，返回形如(data, label)的数据。Dataloader是一个可迭代的对象，它将dataset返回的每一条数据拼接成一个batch，并提供多线程加速优化和数据打乱等操作。当程序对dataset的所有数据遍历完一遍之后，相应的对Dataloader也完成了一次迭代，先定义好dataset，然后定义dataloader对指定的dataset进行操作</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> # 训练集</span><br><span class="line">trainset = tv.datasets.CIFAR10(</span><br><span class="line">                    root=&apos;/home/cy/tmp/data/&apos;, </span><br><span class="line">                    train=True, </span><br><span class="line">                    download=True,</span><br><span class="line">                    transform=transform)</span><br><span class="line"></span><br><span class="line">trainloader = t.utils.data.DataLoader(</span><br><span class="line">                    trainset, </span><br><span class="line">                    batch_size=4,</span><br><span class="line">                    shuffle=True, </span><br><span class="line">                    num_workers=2)</span><br></pre></td></tr></table></figure><ul><li>进行normaliza的必要性：每个样本图像减去数据集图像的均值后除以方差，保证了所有图像的分布相似，使得model训练的时候更快的收敛.</li><li>训练网络的示例</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">t.set_num_threads(8)</span><br><span class="line">for epoch in range(2):  </span><br><span class="line">    </span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    for i, data in enumerate(trainloader, 0):</span><br><span class="line">        </span><br><span class="line">        # 输入数据</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        </span><br><span class="line">        # 梯度清零</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        # forward + backward </span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()   </span><br><span class="line">        </span><br><span class="line">        # 更新参数 </span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        # 打印log信息</span><br><span class="line">        # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0]</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        if i % 2000 == 1999: # 每2000个batch打印一下训练状态</span><br><span class="line">            print(&apos;[%d, %5d] loss: %.3f&apos; \</span><br><span class="line">                  % (epoch+1, i+1, running_loss / 2000))</span><br><span class="line">            running_loss = 0.0</span><br><span class="line">print(&apos;Finished Training&apos;)</span><br></pre></td></tr></table></figure><h2 id="第三章"><a class="markdownIt-Anchor" href="#第三章"></a> 第三章</h2><ul><li>表3-1: 常见新建tensor的方法</li></ul><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">Tensor(*sizes)</td><td style="text-align:center">基础构造函数</td></tr><tr><td style="text-align:center">tensor(data,)</td><td style="text-align:center">类似np.array的构造函数</td></tr><tr><td style="text-align:center">ones(*sizes)</td><td style="text-align:center">全1Tensor</td></tr><tr><td style="text-align:center">zeros(*sizes)</td><td style="text-align:center">全0Tensor</td></tr><tr><td style="text-align:center">eye(*sizes)</td><td style="text-align:center">对角线为1，其他为0</td></tr><tr><td style="text-align:center">arange(s,e,step</td><td style="text-align:center">从s到e，步长为step</td></tr><tr><td style="text-align:center">linspace(s,e,steps)</td><td style="text-align:center">从s到e，均匀切分成steps份</td></tr><tr><td style="text-align:center">rand/randn(*sizes)</td><td style="text-align:center">均匀/标准分布</td></tr><tr><td style="text-align:center">normal(mean,std)/uniform(from,to)</td><td style="text-align:center">正态分布/均匀分布</td></tr><tr><td style="text-align:center">randperm(m)</td><td style="text-align:center">随机排列</td></tr></tbody></table><ul><li>除了<code>tensor.size()</code>，还可以利用<code>tensor.shape</code>直接查看tensor的形状，<code>tensor.shape</code>等价于<code>tensor.size()</code></li><li>tensor = t.Tensor(1,2)创建了一个size为【1，2】的张量</li><li>vector = t.tensor([1, 2])创建了一个值为（1，2）的向量，size为2</li><li>scalar = t.tensor(3.14159) 创建了一个值为3.14159的标量，<br>size为【】,区别于size【0】，empty_tensor = t.tensor([])，size存在即为tensor,否则为scalar</li><li>通过<code>tensor.view</code>方法可以调整tensor的形状，但必须保证调整前后元素总数一致。<code>view</code>不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。<code>b = a.view(-1, 3)</code>当某一维为-1的时候，会自动计算它的大小,</li><li>torch.squeeze() 这个函数主要对数据的维度进行压缩，去掉维数为1的的维度，比如是一行或者一列这种，一个一行三列（1,3）的数去掉第一个维数为一的维度之后就变成（3）行。squeeze(a)就是将a中所有为1的维度删掉。不为1的维度没有影响。a.squeeze(N) 就是去掉a中指定的维数为一的维度。还有一种形式就是b=torch.squeeze(a，N) a中去掉指定的定的维数为一的维度。</li><li>torch.unsqueeze()这个函数主要是对数据维度进行扩充。给指定位置加上维数为一的维度，比如原本有个三行的数据（3），在0的位置加了一维就变成一行三列（1,3）。a.unsqueeze(N) 就是在a中指定位置N加上一个维数为1的维度。还有一种形式就是b=torch.unsqueeze(a，N) a就是在a中指定位置N加上一个维数为1的维度</li><li><code>resize</code>是另一种可用来调整<code>size</code>的方法，但与<code>view</code>不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存，当再次扩展时其值为当时缩小保存的值</li><li>tensor的索引操作和tensor共享内存，即更改其中一个，另一个也会更改。</li><li>a[None]:None类似于np.newaxis, 为a新增了一个轴；等价于a.view(1, a.shape[0], a.shape[1])</li><li>a &gt; 1 # 返回一个ByteTensor,即满足条件的位置值为1，否则为0</li><li>对tensor的任何索引操作仍是一个tensor，想要获取标准的python对象数值，需要调用<code>tensor.item()</code>, 这个方法只对包含一个元素的tensor适用</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;pytorch-cookbook&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#pytorch-cookb
      
    
    </summary>
    
      <category term="pytorch" scheme="http://aier02.com/categories/pytorch/"/>
    
    
      <category term="basic knowledge" scheme="http://aier02.com/tags/basic-knowledge/"/>
    
      <category term="pytorch cookbook" scheme="http://aier02.com/tags/pytorch-cookbook/"/>
    
  </entry>
  
  <entry>
    <title>first time in Kaggle-summary</title>
    <link href="http://aier02.com/2018/10/02/rsna_summary/"/>
    <id>http://aier02.com/2018/10/02/rsna_summary/</id>
    <published>2018-10-01T16:08:39.573Z</published>
    <updated>2018-11-20T02:30:44.452Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h1 id="项目存在的问题和解决方案"><a class="markdownIt-Anchor" href="#项目存在的问题和解决方案"></a> 项目存在的问题和解决方案</h1><h2 id="rsna"><a class="markdownIt-Anchor" href="#rsna"></a> RSNA</h2><h3 id="如何开始比赛"><a class="markdownIt-Anchor" href="#如何开始比赛"></a> 如何开始比赛</h3><ul><li>完全是新手，很早以前就有学长介绍过kaggle，最近看完了cs231n，然后打算试试手，但是一开始并不知道应该做什么，于是就上知乎直接搜了如何打kaggle比赛，找到的很多的都是ml的，但是个人更喜欢cv，添加了关键词后，找到了一篇和我类似经历的blog，知道了kaggle比赛到底是什么，他的每个板块表示的是什么，常用的步骤是什么，有什么需要注意的（这个时候开始意识到我没有gpu，这点必定是后期的瓶颈）</li></ul><h3 id="选择平台"><a class="markdownIt-Anchor" href="#选择平台"></a> 选择平台</h3><ul><li>没有gpu怎么办，没有资源只能去租用云平台跑model，开头抱有侥幸心理，有没有什么免费的平台呢，一开始找到的是google 的colab，听说是免费的，然后我就直接尝试使用，跑了一下mnist的基本模型发现经常自动断开连接，而且他给的免费的硬盘空间只有15g，当时我首先打算打的比赛其实是有关air ship的detection比赛。果然天底下没有免费的午餐，只能另找平台，后来在知乎上看到不少人推荐极客云，他自动帮你搭建好了深度学习的环境，（其实这是个坑，创建的dl环境是不能直接操作整个系统的，任何系统的指令都无法操作，这使得我后来无法进行端口的查询，visdom启动的时候总是提示端口被占用），运行的速度和价钱的确很吸引人</li></ul><h3 id="eda"><a class="markdownIt-Anchor" href="#eda"></a> EDA</h3><ul><li>什么是lung opacities，什么是pneumonia，他们两者有什么关系么？由于缺乏领域知识，得益于知乎，我直接奔向了discussion和kernel，果然找到了一篇名为what r lung opacities的kernel，这篇文章由一个具备radiologist领域知识的kaggler提供，他直接介绍了怎么看chest x-ray，黑色的为air，白色的为bone，grey为tissue或者fluid，这个kernel对于我整个项目影响最大，他补充了我xray的领域知识。做eda主要是看各个文件中的数量、样本中是否存在重复，是否有缺失，class_label.csv文件中有多少中不同的class，各自的数量如何，他与train数据集是否一一对应，open（）函数，创建一个reader，next跳过header之后就循环读取reader实现按行读取数据；也可用pd.read_csv实现，value_counts()统计重复的次数，groupby（keyword）可以根据keyword分组；主要用到了matplotlib.pyplot，numpy，pandas，pydicom，glob进行数据可视化</li></ul><h3 id="detection还是segmentation"><a class="markdownIt-Anchor" href="#detection还是segmentation"></a> detection？还是segmentation？</h3><ul><li>诚然我一开始看到官方的比赛介绍的时候，我主观上是认为要根据dicom图像提供的病人的个人信息以及图像中的信息进行classification，根据我个人对于部分pneumonia的xray的观察，以及提供的数据集中的bboxes的信息和之前提到的那篇kernel的引导，本次比赛更多的是做能指示pneumonia的lung opacities，而且明显是难以完全sgment出来的（opacities有一种是模糊了心脏和lung的边界。于是决定做detection</li></ul><h3 id="数据准备"><a class="markdownIt-Anchor" href="#数据准备"></a> 数据准备</h3><ul><li>前期所做的数据准备，我的想法是直接把pneumonia的bbox的x,y,w,h装入内存，很明显这中做法有一定的风险，遇到内存不足恐怕直接爆了，后期我选择了使用npy文件先预先读取train_lable的信息，然后每次使用的时候再进行读取，这样的做法个人认为可以避免每次训练的时候进行pydicom数据的转换，也能减少内存的使用，后果用了更多的磁盘空间，而且每次读取npy文件都是存在时间成本的。而且有个严重的问题，就是后期如何做数据增强，是对image进行操作，pydicom.dcmread()和cv2.imread读都是hwc，而且是BGR上的0-255为了使后期进行读取方便，我把使用sitk.ReadImage（dcm）将所有的dcm图像转为png格式，sitk.GetArrayFromImage返回的是（hwc），同时存储患有肺炎的图像的numpy数组和bbox的位置信息（ymin,xmin,ymax,xmax），以npy文件的格式保存，实际上在这次操作中图片都是单通道的，而且都是1024*1024的。然后通过对patientid进行shuffle，创建train和valdation，大概9:1</li></ul><h3 id="数据增强"><a class="markdownIt-Anchor" href="#数据增强"></a> 数据增强</h3><ul><li>经过eda后，lung opacities，no lung opacities/not normal,normal,比例是1:1:1，把lung opacities作为positive，则dataset中存在样例不平衡，考虑进行图像的增强，旋转，平移，亮度改变之类的，由于是x-ray图像，考虑进行图像的仿射变换，但是旋转会涉及bbox的改变，因为之前在数字图像课程实验中学过仿射变换（旋转，缩放，平移）然后就直接实现了rotate_img_bbox(img, bboxes, degree=-45, scale=1.)，<br>mat = cv2.getRotationMatrix2D(center,angle=degree,scale=scale) #affine matrix，仿射矩阵为（2，3）的矩阵，以水平进行划分，前（2，2）子矩阵为线性变换矩阵，（2，1）子矩阵为平移矩阵，用标量的形式来看就是ax+b；T=M【x,y,1】,对原来的bbox的四条边的四个中点进行相同的矩阵变化，然后合并为一个矩阵，表示一个仿射后的矩阵，矩形边框（Bounding Rectangle）是说，用一个最小的矩形，把找到的形状包起来。还有一个带旋转的矩形，面积会更小，即使用rx, ry, rw, rh = cv2.boundingRect(concat)得到摆正后的经过仿射变换的新的bbox，整个图像的变换out_image = cv2.warpAffine(img,mat,(width,height))</li><li>做各种变换的时候由于数据集较大，而且操作多，经常出现等待时间较长的情况，缺乏可视化，不能确定是完成了操作还是仍然在等待，后来在循环中使用了tqdm进行进度条的显示，可视化了进程的进度。</li></ul><h3 id="准备dataset"><a class="markdownIt-Anchor" href="#准备dataset"></a> 准备dataset</h3><p>创建类generator(keras.utils.Sequence)，实现的函数分别有</p><ul><li>内置init()，初始化文件夹路径，文件名，bboxing box，batch_size=32,image_size=256(原图为1024)，shuffle，augment，predict为三者为真还是假</li><li>内置len（），返回filenames中数据量的大小</li><li>内置getitem（），以batchz_size为数据单位，根据index确定数据的位置，当predict为真时，返回对应文件名的图像和文件名；否则，返回对应文件名的img图像和msk，注意msks即为bboxes的列表</li><li>内置load（），通过filename确定patientid，然后读取npy文件获取img_array和bboxes列表（每一项为对应图片的bbox的ymin,xmin,ymax,xmax），创建和img等大小的全0msk，根据npy中的‘bboxes’项将对应的标注框区域设置为1；然后resize图片，特别注意由于训练的时候进行的是batch_train,所以对于img和msk都必须添加一个dimension，作为训练的维度</li><li>内置loadpredict（），基本操作和load函数一致，不同在与该函数不用获取bboxes信息</li></ul><h3 id="搭建神经网络"><a class="markdownIt-Anchor" href="#搭建神经网络"></a> 搭建神经网络</h3><ul><li>create_downsample(channels, inputs)下采样函数，channels指示filter的大小，inputs指示images，<a href="http://xn--keras-2b3h3g252orkjqiznnd.layers.BN" target="_blank" rel="noopener">依次使用的是keras.layers.BN</a>(momentum=0.9)-&gt;leakyrelu-&gt;conv2d(padding=same)-&gt;maxpool2d。padding：补0策略，为“valid”, “same” 。“valid”代表只进行有效的卷积，即对边界数据不处理。“same”代表保留边界处的卷积结果，通常会导致输出shape与输入shape相同</li><li>create_resblock(channels, inputs)resblock函数，一个resblock包含了BN-&gt;LEAKYRELU-&gt;CONV2D-&gt;BN-&gt;LEAKYRELU-&gt;CON2D-&gt;add([x, inputs])(channels用于conv层中filters的数目，即输出的维度，kernel_size指定filter的大小)</li><li>create_network(input_size, channels, n_blocks=2, depth=4)搭建整个网络，inputs = keras.Input(shape=(input_size, input_size, 1))对输入进行规范化-&gt;conv2d-&gt;创建depth层结构，每个结构中包括了一个downsample和n_block个resblock-&gt;ouput layer依次为BN-&gt;LEAKYRELU-&gt;CONV2D-&gt;UpSampling2D(2**depth)(x)-&gt;将input和output包装为一个model即model = keras.Model(inputs=inputs, outputs=outputs)</li></ul><h3 id="resnet"><a class="markdownIt-Anchor" href="#resnet"></a> Resnet</h3><ul><li>背景:随着网络的加深，出现了训练集准确率下降的现象，排除overfitting，针对该问题提出了resnet，以允许实现尽可能地加深网络</li><li>resnet中提出了两种mapping，identity mapping 和 residual mapping，输出为y=F(x)+x,显然x为前者，F(x)为后者；理论上，对于“随着网络加深，准确率下降”的问题，Resnet提供了两种选择方式，也就是identity mapping和residual mapping，如果网络已经到达最优，继续加深网络，residual mapping将被push为0，只剩下identity mapping，这样理论上网络一直处于最优状态了，网络的性能也就不会随着深度增加而降低了。即真正学习的是残差，而形式上y保证了不会出现网络加深而经验误差增大的现象</li><li>在resnet中加入1*1的conv layer就是bottleneck layer，目的是为了降维，降低计算量和参数的数目，最后又升维是为了保持和输入x的dimensions一致</li></ul><h3 id="batch-normalization"><a class="markdownIt-Anchor" href="#batch-normalization"></a> Batch Normalization</h3><ul><li>BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。(IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障</li><li>在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。然后提出了BatchNorm的基本思想：能不能让每个隐层节点的激活输入分布固定下来呢？这样就避免了“Internal Covariate Shift”问题了。</li><li>启发式思考：对输入图像进行白化（Whiten）操作的话——所谓白化，就是对输入数据分布变换到0均值，单位方差的正态分布——那么神经网络会较快收敛；BN所做的可以理解为对深层神经网络每个隐层神经元的激活值做简化版本的白化操作</li><li>简而言之，对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。将输入x的分布强制转换到均值为0，方差为1的正态分布。</li><li>经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。</li><li>BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者右移一点并长胖一点或者变瘦一点</li><li>①不仅仅极大提升了训练速度，收敛过程大大加快；②还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等</li></ul><h3 id="定义loss-function"><a class="markdownIt-Anchor" href="#定义loss-function"></a> 定义loss function</h3><ul><li>定义iou损失函数:iou_loss(y_true, y_pred),这里的label(即y)是mask，即bboxes为1的0-1图像，intersection = tf.reduce_sum(y_true * y_pred)求的共同区域的1的个数，score = (intersection + 1.) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)，注意+1.是为了保证score不为0，公式为inserction/union。reduce_sum返回该矩阵所有元素之和</li><li>合并损失函数:0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * iou_loss(y_true, y_pred)，binary_crossentropy为对数损失函数</li><li>损失函数是训练的关键，他表达了模型的目标，通过求原图像经过resnet后的feature map和图像的label，即bboxes的iou，使用adam进行优化，使得网络参数朝iou减少的方向进行更新。</li></ul><h3 id="训练参数的设置"><a class="markdownIt-Anchor" href="#训练参数的设置"></a> 训练参数的设置</h3><ul><li>定义tf.metric算子，即评估指标算子，用于计算accuracy</li><li>model.compile(optimizer=‘adam’,<br>loss=iou_bce_loss,<br>metrics=[‘accuracy’, mean_iou])指定优化器，损失函数和accuracy</li><li>初始化lr=0.01，epoch=25，lr的更新策略:lrx(np.cos(np.pi*x/epochs)+1.)/2,x为动态变化的epoch</li><li>keras.callbacks.LearningRateScheduler(schedule)<br>该回调函数是用于动态设置学习率，其中schedule函数以epoch号为参数（从0算起的整数），返回一个新学习率（浮点数）</li></ul><h3 id="创建train和validation-generator"><a class="markdownIt-Anchor" href="#创建train和validation-generator"></a> 创建train和validation generator</h3><ul><li>指定图片文件夹:folder = ‘./input/stage_1_train_png’</li><li>训练生成器:train_gen = generator(folder, train_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=True, augment=True, predict=False)</li><li>测试生成器:valid_gen = generator(folder, valid_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=False, predict=False)</li></ul><h3 id="训练模型"><a class="markdownIt-Anchor" href="#训练模型"></a> 训练模型</h3><ul><li>history = model.fit_generator(train_gen, validation_data=valid_gen, callbacks=[learning_rate], epochs=25, workers=4, use_multiprocessing=True)用于指定callback内容，即learning rate，训练集，验证集，epoch数量，线程数量</li></ul><h3 id="测试模型"><a class="markdownIt-Anchor" href="#测试模型"></a> 测试模型</h3><ul><li>创建测试数据生成器:test_gen = generator(folder, test_filenames, None, batch_size=25, image_size=256, shuffle=False, predict=True)</li><li>threshold predicted mask,pred中大于0.5才为mask<br>comp = pred[:, :, 0] &gt; 0.5<br>apply connected components<br>comp = measure.label(comp),measure.label作用是给comp标记连通区域，用于确定不同的bbox，</li><li>measure.regionprops(comp)获取comp的不同连通区域，返回的region列表，每一个分别为ymin,xmin,ymax,xmax</li><li>计算置信度proxy for confidence score:conf = np.mean(pred[y:y+height, x:x+width])</li></ul><h3 id="提交结果"><a class="markdownIt-Anchor" href="#提交结果"></a> 提交结果</h3><ul><li>保存结果到csv文件中:以字典的形式保存到csv文件中save dictionary as csv file<br>sub = pd.DataFrame.from_dict(submission_dict,orient=‘index’)</li><li>指定索引名称:sub.index.names = [‘patientId’]</li><li>指定列名:sub.columns = [‘PredictionString’]</li><li>写入指定目录文件:sub.to_csv(’/input/submission.csv’)</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;项目存在的问题和解决方案&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#项目存在的问题和解决方案&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="Kaggle" scheme="http://aier02.com/categories/Kaggle/"/>
    
    
      <category term="segmentation" scheme="http://aier02.com/tags/segmentation/"/>
    
      <category term="keras" scheme="http://aier02.com/tags/keras/"/>
    
      <category term="ResNet" scheme="http://aier02.com/tags/ResNet/"/>
    
      <category term="jikecloud" scheme="http://aier02.com/tags/jikecloud/"/>
    
  </entry>
  
  <entry>
    <title>python rubbish collection</title>
    <link href="http://aier02.com/2018/09/28/python_rubbish_collection/"/>
    <id>http://aier02.com/2018/09/28/python_rubbish_collection/</id>
    <published>2018-09-28T07:32:02.260Z</published>
    <updated>2018-10-03T08:08:16.164Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h2 id="python垃圾回收机制"><a class="markdownIt-Anchor" href="#python垃圾回收机制"></a> python垃圾回收机制</h2><h3 id="引用计数"><a class="markdownIt-Anchor" href="#引用计数"></a> 引用计数</h3><p>每个对象维护一个ob_ref字段，每次被别的对象引用的ob_ref加1，若引用失效，则减1，当ob_ref为0则该对象被回收，占用的内存空间被释放。但是该方法不能解决循环引用问题，即两个对象相互引用，当他们的外部引用都失效时，ob_ref仍为1，非零，但是他们实质是要被回收的，而python却不能将其回收</p><h3 id="标记清除"><a class="markdownIt-Anchor" href="#标记清除"></a> 标记清除</h3><p>对活动对象进行标记，将非活动对象进行回收。对象之间通过引用（指针）连在一起，构成一个有向图，对象构成这个有向图的节点，而引用关系构成这个有向图的边。从根对象（root object）出发，沿着有向边遍历对象，可达的（reachable）对象标记为活动对象，不可达的对象就是要被清除的非活动对象。根对象就是全局变量、调用栈、寄存器。</p><h3 id="分代回收"><a class="markdownIt-Anchor" href="#分代回收"></a> 分代回收</h3><p>根据对象存活时间的不同对内存进行了划分，时间由短到长分别划分为年轻代，中年代，老年代，新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发，把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，依此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;python垃圾回收机制&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#python垃圾回收机制&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="python" scheme="http://aier02.com/categories/python/"/>
    
    
      <category term="basic knowledge" scheme="http://aier02.com/tags/basic-knowledge/"/>
    
  </entry>
  
  <entry>
    <title>faster RCNN</title>
    <link href="http://aier02.com/2018/09/18/faster_RCNN/"/>
    <id>http://aier02.com/2018/09/18/faster_RCNN/</id>
    <published>2018-09-18T10:55:07.479Z</published>
    <updated>2018-10-03T08:08:10.513Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h2 id="faster-rcnn"><a class="markdownIt-Anchor" href="#faster-rcnn"></a> faster RCNN</h2><ul><li>感受野：在卷积神经网络CNN中，决定某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野receptive field</li><li>fatal error: numpy/arrayobject.h: No such file or directory”​<br>这个错误的出现可以如下解决，将setup.py内容加入一条include_dirs=[numpy.get_include()]​就可以了。<br>示例setup.py文件如下：<br>from distutils.core import setup<br>from distutils.extension import Extension<br>from Cython.Distutils import build_ext<br>import numpy as np<br>ext_modules=[Extension(“test03”,[“test03.pyx”])]<br>setup(<br>name=‘gravity_cy’,<br>cmdclass={‘build_ext’:build_ext},<br>include_dirs = [np.get_include()],<br>ext_modules=ext_modules<br>) 主要问题是要指定numpy的路径</li><li>过程记录：1）下载faster-rcnn的预训练模型并在demo中进行调试<br>2）运行python misc/convert_caffe_pretrain.py把预训练的vgg16下载并保存到checkpoint中，作为cnn提取器<br>3）</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;faster-rcnn&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#faster-rcnn&quot;&gt;&lt;/a&gt; 
      
    
    </summary>
    
      <category term="pytorch" scheme="http://aier02.com/categories/pytorch/"/>
    
    
      <category term="CNN" scheme="http://aier02.com/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>analysis kernel demo</title>
    <link href="http://aier02.com/2018/09/18/analysis_kernel_demo/"/>
    <id>http://aier02.com/2018/09/18/analysis_kernel_demo/</id>
    <published>2018-09-18T10:55:03.519Z</published>
    <updated>2018-10-30T02:57:12.890Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h2 id="analysis-kernel-demo"><a class="markdownIt-Anchor" href="#analysis-kernel-demo"></a> analysis kernel demo</h2><ul><li><p>BatchNormalization层：该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1<a href="https://arxiv.org/pdf/1502.03167v3.pdf" target="_blank" rel="noopener">BN</a></p></li><li><p>leakyRelu层,大于0为x，小于0为ax(a常为0.01)</p></li><li><p>pytorch自定义dataset和dataloader</p></li><li><p>torch.utils.data.Dataset是表示数据集的抽象类。您自定义的数据集应该继承Dataset并重写以下方法：len使用len(dataset)将返回数据集的大小。getitem 支持索引，dataset[i]可以获取第i个样本让我们为我们的人脸标记点数据集创建一个数据集类。我们将在init中读取csv，而将在getitem存放读取图片的任务。因为所有的图像不是一次性存储在内存中，而是根据需要进行读取，这样可以高效的使用内存。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomDataset</span><span class="params">(data.Dataset)</span>:</span><span class="comment">#需要继承data.Dataset</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        <span class="comment"># 1. Initialize file path or list of file names.</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        <span class="comment"># 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).</span></span><br><span class="line">        <span class="comment"># 2. Preprocess the data (e.g. torchvision.Transform).</span></span><br><span class="line">        <span class="comment"># 3. Return a data pair (e.g. image and label).</span></span><br><span class="line">        <span class="comment">#这里需要注意的是，第一步：read one data，是一个data</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># You should change 0 to the total size of your dataset.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><p>将numpy ndarry转为tensor：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ToTensor</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Convert ndarrays in sample to Tensors."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># swap color axis because</span></span><br><span class="line">        <span class="comment"># numpy image: H x W x C</span></span><br><span class="line">        <span class="comment"># torch image: C X H X W </span></span><br><span class="line">        image = image.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: torch.from_numpy(image),</span><br><span class="line">                <span class="string">'landmarks'</span>: torch.from_numpy(landmarks)&#125;</span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;analysis-kernel-demo&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#analysis-
      
    
    </summary>
    
      <category term="pytorch" scheme="http://aier02.com/categories/pytorch/"/>
    
    
      <category term="demo" scheme="http://aier02.com/tags/demo/"/>
    
  </entry>
  
  <entry>
    <title>first time in Kaggle-preparation</title>
    <link href="http://aier02.com/2018/09/06/rsna_pneumonia_detection/"/>
    <id>http://aier02.com/2018/09/06/rsna_pneumonia_detection/</id>
    <published>2018-09-06T09:08:00.932Z</published>
    <updated>2018-10-03T08:15:40.619Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h2 id="question-description"><a class="markdownIt-Anchor" href="#question-description"></a> Question description</h2><ul><li>In this competition, you’re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs.(CXR)</li><li>胸部X线片上肺部阴影的定位以跟踪可能的肺炎病况</li><li>They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review.</li><li>But the title “Pneumonia Detection” for the competition is misleading because you actually have to do “Lung Opacities Detection”, and lung opacities are not the same as pneumonia. Lung opacities are vague, fuzzy clouds of white in the darkness of the lungs, which makes detecting them a real challenge.-from a kernal</li><li>如何确认x照片中的阴影？</li></ul><h2 id="basic-information"><a class="markdownIt-Anchor" href="#basic-information"></a> Basic information</h2><ul><li>in CXR:black-air;white-bones;grey-fluid or tissue</li><li>Usually the lungs are full of air. When someone has pneumonia, the air in the lungs is replaced by other material - fluids, bacteria, immune system cells, etc. That’s why areas of opacities are areas that are grey but should be more black.(找出原来是黑但是变灰了的区域</li><li>As you can see, lung opacities are not homogenoues and they do not have a clear center or clear boundaries. I don’t think you can properly segment opacities out of the entire picture because there are no clear boundries.-直接找阴影貌似很难，可以先segment lungs</li><li>in fact, there was only a moderate level of agreement between radiologists about the presence of infiltrates, which are opacities by definition</li><li>预测任务为病人的dcm图像找到lung opacities,以bbox的形式给出，在submission_file中，一张图片中只能有一条bbox信息，每4个信息项为一个bbox，即(x,y,weight,height)</li></ul><h2 id="difficulties"><a class="markdownIt-Anchor" href="#difficulties"></a> Difficulties</h2><ul><li>there is a mass of tissue surrounding the lungs and between the lungs. These areas contain skin, muscles, fat, bones, and also the heart and big blood vessels. That translates into a lot of information on the chest radiograph that is not useful for this competition.</li><li>阴影有多种，怎样的阴影才和肺炎相关The main difference in the types of opacities between these two patients is the borders and the shape of the opacity, Patient 3 has multiple round and clearly defined opacities. Patient 2 has this poorly defined haziness which obscures the margins of the lungs and heart. This haziness is termed consolidation.</li><li>Exclude: obvious mass(es), nodule(s), lobar collapse, linear atelectasis要找的阴影是模糊的，难懂的，不明显的</li><li>In the cases labeled Not Normal/No Lung Opacity, no lung opacity refers to no opacity suspicious for pneumonia. 在数据集中，不正常/没有肺部阴影，是指没有与肺炎相关的阴影，但会有与其他病症相关的阴影，故是不正常。</li><li>两种肺炎阴影：Ground-Glass Opacities；Consolidations前者We can see that the lungs are “whiter” than they should be, but we can see most of the borders of the lungs and hear后者There are fuzzy areas in the lungs and the borders of the lungs and heart cannot be seen.玻璃粉状的阴影不会影响心脏和肺部的边界，而合并类的阴影会模糊两者的边界</li><li>预测pneumoni和opacities应该独立预测？lung opacities并不是是pneumonia的充要条件</li><li>实际上该比赛为Lung Opacities Detectio，而不是Pneumonia Detection</li></ul><h2 id="exprolatory-data-analysis"><a class="markdownIt-Anchor" href="#exprolatory-data-analysis"></a> exprolatory data analysis</h2><ul><li>stage_1_detailed_class_info里面包括了3种class，共有28989条记录，每条信息为patientid：class，标注了病人id对应的正常、不正常/不是肺炎，肺炎3种情况</li><li>stage_1_train_labels记录了bbox的位置，同样有28989条记录，每条记录分别是patientid：x,y,width,height,target;x,y表示bbox的左上角的坐标，width是宽度，即x的范围，height是高度，即y的范围，target=0表示没有肺炎，=1表示有肺炎，对应class_info，normal则bbox信息empty，target=0；No Lung Opacity / Not Normal则bbox信息empty，target=0；因此lung opacities in data is associated with pneumonia；lung opacities 则包含了bbox的信息同时target=1</li><li>注意数据集存在一名病人对应多条记录的情况，并不是有28989名病人，经过eda可知存在25684名病人（patientid）(training data 中有25684张dcm)实际上因为bbox信息表是一条记录只能表示一个bbox，故存在一个病人的cxr中有多个bbox，而class表和label表相一致，所以保持对应关系class存在重复数据</li><li>训练集中dcm文件不只是image，还有meta information，如sex，age等，是否需要添加此类属性，从而考虑相关的内容进行判断？</li><li>patientId - A patientId. Each patientId corresponds to a unique image.</li><li>x_ - the upper-left x coordinate of the bounding box.</li><li>y_ - the upper-left y coordinate of the bounding box.</li><li>width_ - the width of the bounding box.</li><li>height_ - the height of the bounding box.</li><li>Target_ - the binary Target, indicating whether this sample has evidence of pneumonia.</li><li>No Lung Opacity / Not Normal and Normal have together the same percent (69.077%) as the percent of missing values for target window in class details information.显然存在正样本偏少的情况，需要做数据增强，过采样？角度偏转？</li></ul><h2 id="models"><a class="markdownIt-Anchor" href="#models"></a> models</h2><h3 id="chexnet"><a class="markdownIt-Anchor" href="#chexnet"></a> ChexNet</h3><ul><li>The CheXNet algorithm is a 121-layer deep 2D Convolutional Neural Network; a Densenet after Huang &amp; Liu. The Densenet’s multiple residual connections reduce parameters and training time, allowing a deeper, more powerful model. The model accepts a vectorized two-dimensional image of size 224 pixels by 224 pixels.</li><li>CheXNet is a 121-layer Dense Convolutional Network (DenseNet) (Huang et al., 2016) trained on the ChestX-ray 14 dataset. DenseNets improve flow of information and gradients through the network, making the optimization of very deep networks tractable. We replace the final fully connected layer with one that has a single output, after which we apply a sigmoid nonlinearity. The weights of the network are initialized with weights from a model pretrained on ImageNet (Deng et al., 2009). The network is trained end-to-end using Adam with standard parameters (ß1 = 0.9 and ß2 = 0.999) (Kingma &amp; Ba, 2014). We train the model using minibatches of size 16. We use an initial learning rate of 0.001 that is decayed by a factor of 10 each time the validation loss plateaus after an epoch, and pick the model with the lowest validation loss.</li></ul><p><img src="/images/180906/chexnet.jpg" alt=""></p><p><img src="/images/180906/densenet.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;question-description&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#question-
      
    
    </summary>
    
      <category term="Kaggle" scheme="http://aier02.com/categories/Kaggle/"/>
    
    
      <category term="segmentation" scheme="http://aier02.com/tags/segmentation/"/>
    
      <category term="chest X-ray" scheme="http://aier02.com/tags/chest-X-ray/"/>
    
      <category term="ChexNet" scheme="http://aier02.com/tags/ChexNet/"/>
    
      <category term="EDA" scheme="http://aier02.com/tags/EDA/"/>
    
  </entry>
  
  <entry>
    <title>experience from a Kaggler</title>
    <link href="http://aier02.com/2018/09/05/kaggle_expericence/"/>
    <id>http://aier02.com/2018/09/05/kaggle_expericence/</id>
    <published>2018-09-05T02:19:10.278Z</published>
    <updated>2018-10-03T08:08:35.464Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h1 id="kaggle比赛步骤-经验"><a class="markdownIt-Anchor" href="#kaggle比赛步骤-经验"></a> kaggle比赛步骤-经验</h1><p>1.仔细阅读比赛介绍和数据描述；</p><p>2.查找相似的Kaggle比赛。作为一个接触不久的Kaggler，我已经完成对所有Kaggle比赛基本分析的收集；</p><p>3.研究相似比赛的解决方案；</p><p>4.阅读有关论文，以确保不错过该领域的最新进展；</p><p>5.分析数据，并构建可靠的交叉验证结果；</p><p>6.数据预处理、特征工程和模型训练。</p><p>7.结果分析，包括如预测分布、错误分析和困难样本等；</p><p>8.根据分析来改进模型或设计新模型；</p><p>9.基于数据分析和结果分析来设计模型以增加多样性或解决困难样本；</p><p>10.模型集成；</p><p>11.必要时返回到前面的某个步骤。</p><p>Q：你觉得，赢得比赛的关键是什么？</p><p>可靠的验证方式，借鉴其他比赛并阅读相关论文，以及良好的自制力和心理素质。</p><p>Q：你认为你最具竞争力的比赛技巧或方法是什么？</p><p>我认为应该是在比赛开始时准备解决方案的文档。我会强迫自己写出一份清单，包括面临的挑战、应该阅读的解决方案和论文、可能的风险、可用的验证方式、可能的数据增强方法以及增加模型多样性的方式。而且，我不断更新这个文档。幸运地，这些文档为我后面在很多比赛中取得不错成绩提供了支持</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;kaggle比赛步骤-经验&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#kaggle比赛步骤-经验&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="Kaggle" scheme="http://aier02.com/categories/Kaggle/"/>
    
    
      <category term="experience" scheme="http://aier02.com/tags/experience/"/>
    
  </entry>
  
  <entry>
    <title>a failed experience in Kaggle</title>
    <link href="http://aier02.com/2018/09/05/airbus_ship_detection/"/>
    <id>http://aier02.com/2018/09/05/airbus_ship_detection/</id>
    <published>2018-09-05T02:18:30.433Z</published>
    <updated>2018-10-03T08:08:00.121Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><h1 id="airbus-ship-detection-challenge"><a class="markdownIt-Anchor" href="#airbus-ship-detection-challenge"></a> airbus-ship-detection challenge</h1><h2 id="问题描述"><a class="markdownIt-Anchor" href="#问题描述"></a> 问题描述：</h2><p>从卫星图像中找到ships，并用bbox分割出来</p><h2 id="难点所在"><a class="markdownIt-Anchor" href="#难点所在"></a> 难点所在</h2><ul><li>并不是所有图片都有ships</li><li>在图片中ships的大小不一致</li><li>图像分割不允许存在部分重叠，但数据集中的ships当两者直接相邻时，存在轻微的overlap，重叠部分将会被作为背景而移除</li><li>部分带有人工标注的图片中的bbox可能存在边界像素的丢失</li><li>train-ship-segmentations.csv提供了人工标注的bbox作为训练的数据图片，其中bbox以run-length encoding 表示。</li><li>数据集很大，需要用到gpu，训练模型可能要几天时间</li><li>run-length-encoding</li><li>评价方式为IoU,即人工标注的bbox和预测的bbox的相交部分与两者合并的部分的占比。</li></ul><h2 id="edaexploratory-data-analysis"><a class="markdownIt-Anchor" href="#edaexploratory-data-analysis"></a> EDA(exploratory data analysis)</h2><ul><li>The images (at least many of them) are slices of the same image, not separate frames taken at different times.</li><li>data leak:The images in test are just shifted versions of images in train. The problem also happens in train, it looks like airbus had bigger images and then the 768x768 are random crops of the bigger images; but it looks they didn’t check whether there were any overlaps.<br>How to find:<br>Run nearest neighbors on all images<br>For each image take N closest neighbors and find where it overlaps</li><li>Cutting all the images 256x256 they can be found easier because they match almost exactly (except for compression artifacts).</li></ul><h2 id="up-to-now"><a class="markdownIt-Anchor" href="#up-to-now"></a> up to now</h2><p>due to data leak,large data set,few computation source and time limit,I decided to pause the competition.</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;airbus-ship-detection-challenge&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=
      
    
    </summary>
    
      <category term="Kaggle" scheme="http://aier02.com/categories/Kaggle/"/>
    
    
      <category term="segmentation" scheme="http://aier02.com/tags/segmentation/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to statistical learning method U1</title>
    <link href="http://aier02.com/2018/08/28/statistic_1/"/>
    <id>http://aier02.com/2018/08/28/statistic_1/</id>
    <published>2018-08-27T16:58:59.803Z</published>
    <updated>2018-10-03T08:31:04.196Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --><p>李航老师的《统计学习方法》基本已经过一遍了，感觉只是略懂皮毛，现为了加强知识点的认识和部分课后题的实现，有必要进行个人总结。</p><h1 id="统计学习"><a class="markdownIt-Anchor" href="#统计学习"></a> 统计学习</h1><h2 id="定义"><a class="markdownIt-Anchor" href="#定义"></a> 定义</h2><p>statistical learning 是关于计算机基于数据构建构建统计模型并运用模型对数据进行预测和分析的一门学科，是概率论、统计学、信息论、计算理论、最优化理论和计算机科学等学科的交叉学科。</p><h2 id="学习对象和目的"><a class="markdownIt-Anchor" href="#学习对象和目的"></a> 学习对象和目的</h2><p>从数据出发，提取数据的特征，抽象出数据的模型（概率统计模型），并对数据进行预测和分析</p><h2 id="实现统计学习方法的一般步骤"><a class="markdownIt-Anchor" href="#实现统计学习方法的一般步骤"></a> 实现统计学习方法的一般步骤</h2><ul><li>得到一个有限的训练数据集合</li><li>确定包含所有可能的模型的假设空间，即学习的模型的集合</li><li>确定模型选择的准则，即学习的策略</li><li>实现求解最优模型的算法，即学习的算法</li><li>通过学习方法选择最优的模型</li><li>利用最优模型对新的数据进行预测或则分析</li></ul><p>简而言之：数据-&gt;假设空间-&gt;策略-&gt;算法-&gt;最优模型-&gt;预测分析</p><h1 id="监督学习"><a class="markdownIt-Anchor" href="#监督学习"></a> 监督学习</h1><p>统计学习包括了监督学习，非监督学习，半监督学习和强化学习。</p><h2 id="基本概念"><a class="markdownIt-Anchor" href="#基本概念"></a> 基本概念</h2><p>输入所有可能取值的集合称为输入空间，同理输出所有可能的取值集合称为输出空间，通常output space 远小于 input space；而特征空间则是所有特征向量所在的空间，特征向量用于表示一个输入实例，特征空间的每个维度对应一个特征，输入空间可以和特征空间相同，也可把输入空间映射到空间，模型实质是定义在特征空间上的（对特征向量进行学习）。X表示输入变量，Y表示输出变量，输入变量X中的第i个表示为</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mo>(</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo separator="true">,</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msubsup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msubsup><msup><mo>)</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.0448em"></span><span class="strut bottom" style="height:1.321664em;vertical-align:-.276864em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.276864em;margin-left:0;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span style="top:-.5198em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">1</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.276864em;margin-left:0;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span style="top:-.5198em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">2</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.276864em;margin-left:0;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span style="top:-.5198em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mopen">(</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:-.413em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></span></p><p>N个数据的训练数据集(labled)表示为</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi><mo>=</mo><mo>{</mo><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>)</mo><mo separator="true">,</mo><mo>(</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>)</mo><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mo>(</mo><msub><mi>x</mi><mi>N</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>N</mi></msub><mo>)</mo><mo>}</mo></mrow><annotation encoding="application/x-tex">T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span><span class="mrel">=</span><span class="mopen">{</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mpunct">,</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">}</span></span></span></span></span></p><p>输入到输出的映射关系由模型进行表示，所有可能的由输入空间（特征空间）到输出空间的映射的集合组成了假设空间，学习的范围局限在假设空间中。</p><h1 id="统计学习的三要素"><a class="markdownIt-Anchor" href="#统计学习的三要素"></a> 统计学习的三要素</h1><h2 id="模型"><a class="markdownIt-Anchor" href="#模型"></a> 模型</h2><p>统计学习首先考虑是学习什么样的模型，在监督学习中就是要学习的条件概率分布或者决策函数，由所有可能的模型构成的集合组成了假设空间F,通常是一个由参数决定的函数族.<br>决策函数模型的集合表示为$$F={f|Y=f_\theta(X),\theta\in R^n}$$<br>条件概率模型的集合表示为$$F={f|P=P_\theta(Y|X),\theta\in R^n}$$<br>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.69444em;vertical-align:0"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">θ</span></span></span></span>取值于n维的欧氏空间，称为参数空间</p><h2 id="策略"><a class="markdownIt-Anchor" href="#策略"></a> 策略</h2><p>有了假设空间，接着考虑按照什么样的准则学习最优的模型，称为策略。</p><h3 id="损失函数和风险函数"><a class="markdownIt-Anchor" href="#损失函数和风险函数"></a> 损失函数和风险函数</h3><p>损失函数度量模型预测的好坏（预测的结果和标记的差距），风险函数度量平均意义下的模型预测的好坏（在具体某次预测可能出错的期望）</p><h4 id="损失函数"><a class="markdownIt-Anchor" href="#损失函数"></a> 损失函数</h4><p>记作L(f(X),Y)&gt;=0,常见类型:</p><ul><li>0-1损失函数（0-1 loss fuction ）：</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>(</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo separator="true">,</mo><mi>Y</mi><mo>)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable><mtr><mtd><mrow><mn>1</mn><mo separator="true">,</mo></mrow></mtd><mtd><mrow><mi>Y</mi><mo>≠</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mrow><mn>0</mn><mo separator="true">,</mo></mrow></mtd><mtd><mrow><mi>Y</mi><mo>=</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">L(f(X),Y) = \begin{cases} 1, &amp; Y \neq f(X)\\ 0, &amp; Y = f(X) \end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.75em"></span><span class="strut bottom" style="height:3.0000299999999998em;vertical-align:-1.25003em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mclose">)</span><span class="mrel">=</span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist"><span style="top:-.6819999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">1</span><span class="mpunct">,</span></span></span><span style="top:.7579999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">0</span><span class="mpunct">,</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="arraycolsep" style="width:1em"></span><span class="col-align-l"><span class="vlist"><span style="top:-.6819999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mrel">≠</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span></span></span><span style="top:.7579999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><ul><li>平方损失函数(quadratic loss function):</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>(</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo separator="true">,</mo><mi>Y</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>Y</mi><mo>−</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo><msup><mo>)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L(f(X),Y) =(Y-f(X))^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.8641079999999999em"></span><span class="strut bottom" style="height:1.1141079999999999em;vertical-align:-.25em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mclose">)</span><span class="mrel">=</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:-.413em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></span></p><ul><li>绝对损失函数(absolute loss fuction):</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>(</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo separator="true">,</mo><mi>Y</mi><mo>)</mo><mo>=</mo><mi mathvariant="normal">∣</mi><mo>(</mo><mi>Y</mi><mo>−</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>)</mo><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">L(f(X),Y) =|(Y-f(X))|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathrm">∣</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mclose">)</span><span class="mord mathrm">∣</span></span></span></span></span></p><h3 id="经验风险和结构风险"><a class="markdownIt-Anchor" href="#经验风险和结构风险"></a> 经验风险和结构风险</h3><p>模型f(x)关于训练数据集的平均损失称为经验风险：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mrow><mi>e</mi><mi>m</mi><mi>p</mi></mrow></msub><mo>(</mo><mi>f</mi><mo>)</mo><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mi>L</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">R_{emp}(f) =\frac 1N\sum_{i=1}^NL(y_i,f(x_i))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.8283360000000002em"></span><span class="strut bottom" style="height:3.106005em;vertical-align:-1.277669em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.00773em">R</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.00773em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">e</span><span class="mord mathit">m</span><span class="mord mathit">p</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-.000005000000000143778em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span style="top:-1.2500050000000003em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p><p>结构风险简单而言就是经验风险加入了正则化项，正则化项用于表示模型复杂度，因此结构风险最小化是为了防止过拟合而提出的策略。</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mrow><mi>s</mi><mi>r</mi><mi>m</mi></mrow></msub><mo>(</mo><mi>f</mi><mo>)</mo><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mi>L</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><mo>)</mo><mo>+</mo><mi>λ</mi><mi>J</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">R_{srm}(f) =\frac 1N\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.8283360000000002em"></span><span class="strut bottom" style="height:3.106005em;vertical-align:-1.277669em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.00773em">R</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.00773em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">s</span><span class="mord mathit" style="margin-right:.02778em">r</span><span class="mord mathit">m</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-.000005000000000143778em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span style="top:-1.2500050000000003em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mbin">+</span><span class="mord mathit">λ</span><span class="mord mathit" style="margin-right:.09618em">J</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p><p>其中$\lambda $&gt;=0,用以权衡经验风险和模型复杂度</p><h2 id="算法"><a class="markdownIt-Anchor" href="#算法"></a> 算法</h2><p>算法是指学习模型的具体方法，一般而言就是求解模型f中的参数以达到模型的最优化，故很多时候统计学习的算法到最后基本都是最优化问题。</p><h1 id="模型评估和模型选择"><a class="markdownIt-Anchor" href="#模型评估和模型选择"></a> 模型评估和模型选择</h1><h2 id="训练误差"><a class="markdownIt-Anchor" href="#训练误差"></a> 训练误差</h2><p>训练误差是模型关于训练数据集的平均损失，即前面所说的经验风险；</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mrow><mi>e</mi><mi>x</mi><mi>p</mi></mrow></msub><mo>(</mo><mi>f</mi><mo>)</mo><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mi>L</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">R_{exp}(f) =\frac 1N\sum_{i=1}^NL(y_i,f(x_i))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.8283360000000002em"></span><span class="strut bottom" style="height:3.106005em;vertical-align:-1.277669em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.00773em">R</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.00773em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">e</span><span class="mord mathit">x</span><span class="mord mathit">p</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-.000005000000000143778em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span style="top:-1.2500050000000003em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p><h2 id="测试误差"><a class="markdownIt-Anchor" href="#测试误差"></a> 测试误差</h2><p>测试误差跟训练误差相似，只是前者是在测试数据集上的平均损失</p><h2 id="过拟合和模型选择"><a class="markdownIt-Anchor" href="#过拟合和模型选择"></a> 过拟合和模型选择</h2><p>一味最求模型对于训练数据的预测能力，所求得的模型往往会比真实模型更加复杂，这种现象叫做过拟合（模型参数过多，训练误差很小，但测试误差较大或者泛化能力很差），而模型的选择就是为了避免过拟合并提高模型对于未知数据的预测能力。</p><h2 id="正则化"><a class="markdownIt-Anchor" href="#正则化"></a> 正则化</h2><p>模型选择的典型方法是正则化，通过结构风险最小化实现，即在经验风险后加上一个正则化项，一般是模型复杂度的单调递增函数（比如L_2范数）<br>正则化的作用是选择经验风险和模型复杂度同时较小的模型；问题是为什么需要简单的模型？一方面是过拟合的存在使得模型有可能过度关注训练数据集的“个性”，导致模型的泛化能力下降；另一个解释是根据奥卡姆剃刀原理，“如无必要，勿增实体”，即在所有可能选择的模型中，能够很好地解释数据并且十分简单的模型才是最优的（尽管这个要求有点自相矛盾，此时<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.69444em;vertical-align:0"></span><span class="base textstyle uncramped"><span class="mord mathit">λ</span></span></span></span>往往起到折中的作用）；从贝叶斯估计的角度而言，正则化项对应于模型的先验概率，复杂的模型先验概率小，简单的模型先验概率大。</p><h2 id="交叉验证"><a class="markdownIt-Anchor" href="#交叉验证"></a> 交叉验证</h2><p>基本思想是重复地使用数据，对给定的数据进行划分，然后组合成训练集和测试集，反复进行训练、测试和模型的选择。</p><h3 id="简单交叉验证"><a class="markdownIt-Anchor" href="#简单交叉验证"></a> 简单交叉验证</h3><p>随机把数据集划分为训练集和测试集两部分（训练集更大），改变不同的条件使得在相同的训练集上也能得到不同的模型，然后在测试集进行模型的测试和选择。</p><h3 id="s折交叉验证"><a class="markdownIt-Anchor" href="#s折交叉验证"></a> S折交叉验证</h3><p>随机地把数据集划分为S个互不相交的子数据集，利用S-1个子数据集作为训练数据集，剩下的一个做为测试集；将这一过程对可能的S中选择重复进行，从得到的S个模型中选择测试误差最小的模型。</p><h3 id="留一交叉验证"><a class="markdownIt-Anchor" href="#留一交叉验证"></a> 留一交叉验证</h3><p>S=N时，用于数据缺乏的情况下，即每次只拿一个数据样本作为测试集，进行N次相同操作，从N个模型中选择最优者（平均测试误差最小）。</p><h1 id="泛化能力"><a class="markdownIt-Anchor" href="#泛化能力"></a> 泛化能力</h1><p>generalization alibity指由学习方法得到的模型对于未知数据的预测能力，常用测试误差评价学习方法的泛化能力，仅记测试数据集是十分宝贵的，在学习模型的过程中不能使用，要在训练完成后才能用于测试，保证对于学得的模型而言，测试数据是“未知的”。对于泛化能力的分析往往通过研究泛化误差的概率上界进行，常具有以下性质：样本容量增加，泛化上界趋于0；假设空间容量越大，模型就越难学，泛化误差上界就越大。<br>存在定理，对于二类分类问题，当假设空间是有限个函数的集合时F={f1,f2,f3…,fd},对任意一个函数f<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∈</mo></mrow><annotation encoding="application/x-tex">\in</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.5391em"></span><span class="strut bottom" style="height:.5782em;vertical-align:-.0391em"></span><span class="base textstyle uncramped"><span class="mrel">∈</span></span></span></span>F,至少以概率1-<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.69444em;vertical-align:0"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.03785em">δ</span></span></span></span>,以下不等式成立：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mo>(</mo><mi>f</mi><mo>)</mo><mo>&lt;</mo><mo>=</mo><mover accent="true"><mi>R</mi><mo>^</mo></mover><mo>(</mo><mi>f</mi><mo>)</mo><mo>+</mo><mi>ϵ</mi><mo>(</mo><mi>d</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>δ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">R(f)&lt;=\hat R(f)+\epsilon (d,N,\delta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.9467699999999999em"></span><span class="strut bottom" style="height:1.19677em;vertical-align:-.25em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.00773em">R</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mclose">)</span><span class="mrel">&lt;</span><span class="mrel">=</span><span class="mord accent"><span class="vlist"><span style="top:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord mathit" style="margin-right:.00773em">R</span></span><span style="top:-.25233em;margin-left:.16668em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mclose">)</span><span class="mbin">+</span><span class="mord mathit">ϵ</span><span class="mopen">(</span><span class="mord mathit">d</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.10903em">N</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.03785em">δ</span><span class="mclose">)</span></span></span></span></span></p><p>即泛化误差&lt;=训练误差+N的单调递减函数(右端即为泛化误差的上界)</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi><mo>(</mo><mi>d</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>δ</mi><mo>)</mo><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mi>N</mi></mrow></mfrac><mo>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>d</mi><mo>+</mo><mi>l</mi><mi>o</mi><mi>g</mi><mfrac><mn>1</mn><mi>δ</mi></mfrac><mo>)</mo></mrow></msqrt></mrow><annotation encoding="application/x-tex">\epsilon (d,N,\delta)=\sqrt {\frac 1{2N}(logd+log\frac 1\delta)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.65161em"></span><span class="strut bottom" style="height:2.44003em;vertical-align:-.78842em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">ϵ</span><span class="mopen">(</span><span class="mord mathit">d</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.10903em">N</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.03785em">δ</span><span class="mclose">)</span><span class="mrel">=</span><span class="sqrt mord"><span class="sqrt-sign" style="top:-.16161000000000003em"><span class="style-wrap reset-textstyle textstyle uncramped"><span class="delimsizing size3">√</span></span></span><span class="vlist"><span style="top:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord reset-textstyle displaystyle textstyle cramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">2</span><span class="mord mathit" style="margin-right:.10903em">N</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord mathit">d</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord reset-textstyle displaystyle textstyle cramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord mathit" style="margin-right:.03785em">δ</span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mclose">)</span></span></span><span style="top:-1.57161em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em">​</span></span>​</span></span></span></span></span></span></span></p><p>可见泛化误差上界和训练误差、样本容量均正相关，和模型复杂度d负相关。</p><h1 id="生成模型和判别模型"><a class="markdownIt-Anchor" href="#生成模型和判别模型"></a> 生成模型和判别模型</h1><h2 id="生成模型"><a class="markdownIt-Anchor" href="#生成模型"></a> 生成模型</h2><p>由生成方法学到的模型称为生成模型，生成方法由数据学习联合概率分布P(X,Y),然后求出条件概率分布P(Y|X)作为预测的模型，即：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(Y|X)=\frac {P(X,Y)}{P(X)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em"></span><span class="strut bottom" style="height:2.363em;vertical-align:-.936em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span></span></span></span><span style="top:-.2300000000000001em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><p>生成模型表示了给定输入X产生输出Y的生成关系，典型的生成模型由朴素贝叶斯法和隐马尔可夫模型。</p><h2 id="判别模型"><a class="markdownIt-Anchor" href="#判别模型"></a> 判别模型</h2><p>由判别方法学到的模型称为判别模型，判别方法由数据直接学习决策函数f(X)或者条件概率P(Y|X)作为预测的模型。判别模型表示给定输入X应该预测什么样的输出Y。典型的判别模型有k紧邻法，感知机，决策树，逻辑斯谛回归模型，最大熵模型，支持向量机，提升方法，条件随机场等。</p><p>存在隐变量时不能使用判别方法，但可以用生成方法；判别方法直接面对预测，准确率更高。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Sun Dec 09 2018 15:10:38 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;李航老师的《统计学习方法》基本已经过一遍了，感觉只是略懂皮毛，现为了加强知识点的认识和部分课后题的实现，有必要进行个人总结。&lt;/p&gt;&lt;h1 id
      
    
    </summary>
    
      <category term="statistical learning method" scheme="http://aier02.com/categories/statistical-learning-method/"/>
    
    
      <category term="basic knowledge" scheme="http://aier02.com/tags/basic-knowledge/"/>
    
      <category term="introduction" scheme="http://aier02.com/tags/introduction/"/>
    
  </entry>
  
</feed>
