<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Aier02</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://aier02.com/"/>
  <updated>2018-10-10T02:23:16.013Z</updated>
  <id>http://aier02.com/</id>
  
  <author>
    <name>易安明</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CS231n-2017-Summary</title>
    <link href="http://aier02.com/2018/10/08/CS231n_2017_Summary/"/>
    <id>http://aier02.com/2018/10/08/CS231n_2017_Summary/</id>
    <published>2018-10-08T14:46:19.006Z</published>
    <updated>2018-10-10T02:23:16.013Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --><p>Something new to me when I read such a good notebook about <a href="https://github.com/mbadry1/CS231n-2017-Summary" target="_blank" rel="noopener">CS231n-2017-Summary</a></p><h3 id="cnns"><a class="markdownIt-Anchor" href="#cnns"></a> CNNs</h3><p>常用same策略同时保存图像边缘信息</p><ul><li>Padding strategy:in order to maintain our full size of the input. If we didn’t do padding zero the input will be shrinking too fast and we will lose a lot of data.Give a stride of <code>1</code> its common to pad to this equation: <code>(F-1)/2</code> where F is the filter size, zero padding from both sides.If we pad this way we call this same convolution.<ul><li>If we have input of shape (32,32,3) and ten filters with shape is (5,5) with stride 1 and pad 2;Output size will be (32,32,10) # We maintain the size.</li><li>Size of parameters per filter = 5x5x3 + 1 = 76(+1 for bias)</li><li>All parameters 76x10=760</li></ul></li><li>So here are the parameters for the Conv layer:<ul><li>Number of filters K.<ul><li>Usually a power of 2.</li></ul></li><li>Spatial content size F.<ul><li>3,5,7 …</li></ul></li><li>The stride S.<ul><li>Usually 1 or 2 (If the stride is big there will be a downsampling but different of pooling)</li></ul></li><li>Amount of Padding<ul><li>If we want the input shape to be as the output shape, based on the F if 3 its 1, if F is 5 the 2 and so on</li></ul></li></ul></li></ul><p>一般而言pooling层是不可(用)学习的</p><ul><li>Pooling makes the representation smaller and more manageable.</li><li>Pooling Operates over each activation map independently.</li><li>Example of pooling is the maxpooling.<ul><li>Parameters of max pooling is the size of the filter and the stride&quot;<ul><li>Example <code>2x2</code> with stride <code>2</code> <code># Usually the two parameters are the same 2 , 2</code></li></ul></li></ul></li><li>Also example of pooling is average pooling.<ul><li>In this case it might be learnable.</li></ul></li></ul><h3 id="training-neural-networks-i"><a class="markdownIt-Anchor" href="#training-neural-networks-i"></a> Training neural networks I</h3><ul><li><p>As a revision here are the Mini batch stochastic gradient descent algorithm steps,小批量的随机梯度下降算法的步骤:</p><ul><li>Loop:<ol><li>Sample a batch of data.</li><li>Forward prop it through the graph (network) and get loss.(define loss)</li><li>Backprop to calculate the gradients.(chain rule)</li><li>Update the parameters using the gradients(learning rate)</li></ol></li></ul></li><li><p>Activation functions,用于引入非线性因素，单纯的线性模型表达能录不足。</p><p><img src="/images/181008/activation.png" alt=""></p><ul><li><p>Sigmoid:</p><ul><li>Squashes the numbers between [0,1]</li><li>Used as a firing rate like human brains.</li><li><code>Sigmoid(x) = 1 / (1 + e^-x)</code></li><li>Problems with sigmoid:<ul><li>big values neurons kill the gradients.</li><li>Gradients are in most cases near 0 (Big values/small values), that kills the updates if the graph/network are large.</li><li>Not Zero-centered.<ul><li>Didn’t produce zero-mean data.</li></ul></li><li>exp() is a bit compute expensive.<ul><li>just to mention. We have a more complex operations in deep learning like convolution.</li></ul></li></ul></li></ul></li><li><p>Tanh:</p><ul><li>Squashes the numbers between [-1,1]</li><li>Zero centered.</li><li>Still big values neurons “kill” the gradients.</li><li><code>Tanh(x)</code> is the equation.<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mi>s</mi><mi>i</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">tanh(x)=\frac {sinh(x)}{cosh(x)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.01em"></span><span class="strut bottom" style="height:1.53em;vertical-align:-.52em"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit">a</span><span class="mord mathit">n</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.34500000000000003em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">s</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.485em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">s</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>,<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>i</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><mi>x</mi></mrow></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><mn>2</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">sinh(x)=\frac {e^{x}-e^{-x}}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.939765em"></span><span class="strut bottom" style="height:1.284765em;vertical-align:-.345em"></span><span class="base textstyle uncramped"><span class="mord mathit">s</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.345em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.394em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord">−</span><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>,<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><mi>x</mi></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><mn>2</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">cosh(x)=\frac {e^{x}+e^{-x}}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.939765em"></span><span class="strut bottom" style="height:1.284765em;vertical-align:-.345em"></span><span class="base textstyle uncramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">s</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.345em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.394em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord">−</span><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>,</li><li>Proposed by Yann Lecun in 1991</li></ul></li><li><p>RELU (Rectified linear unit):</p><ul><li><code>RELU(x) = max(0,x)</code></li><li>Doesn’t kill the gradients.<ul><li>Only small values that are killed. Killed the gradient in the half</li></ul></li><li>Computationally efficient.</li><li>Converges much faster than Sigmoid and Tanh <code>(6x)</code></li><li>More biologically plausible than sigmoid.</li><li>Proposed by Alex Krizhevsky in 2012 Toronto university. (AlexNet)</li><li>Problems:<ul><li>Not zero centered.</li></ul></li><li>If weights aren’t initialized good, maybe 75% of the neurons will be dead and thats a waste computation. But its still works. This is an active area of research to optimize this.</li><li>To solve the issue mentioned above, people might initialize all the biases by 0.01</li></ul></li><li><p>Leaky RELU:</p><ul><li><code>leaky_RELU(x) = max(0.01x,x)</code></li><li>Doesn’t kill the gradients from both sides.</li><li>Computationally efficient.</li><li>Converges much faster than Sigmoid and Tanh (6x)</li><li>Will not die.</li><li>PRELU is placing the 0.01 by a variable alpha which is learned as a parameter.</li></ul></li><li><p>Exponential linear units (ELU):</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ELU(x) = &#123; x                                           if x &gt; 0</span><br><span class="line">   alpah *(exp(x) -1)                   if x &lt;= 0</span><br><span class="line">           # alpah are a learning parameter</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>It has all the benefits of RELU</p></li><li><p>Closer to zero mean outputs and adds some robustness to noise.</p></li><li><p>problems</p><ul><li><code>exp()</code> is a bit compute expensive.</li></ul></li></ul></li><li><p>Maxout activations:</p><ul><li><code>maxout(x) = max(w1.T*x + b1, w2.T*x + b2)</code></li><li>Generalizes RELU and Leaky RELU</li><li>Doesn’t die!</li><li>Problems:<ul><li>oubles the number of parameters per neuron</li></ul></li></ul></li><li><p>In practice:</p><ul><li>Use RELU. Be careful for your learning rates.</li><li>Try out Leaky RELU/Maxout/ELU</li><li>Try out tanh but don’t expect much.</li><li>Don’t use sigmoid!</li></ul></li></ul><p><strong>Data preprocessing</strong>:</p><ul><li><p>Normalize the data:减去均值后除以标准差</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Zero centered data. (Calculate the mean for every input).</span></span><br><span class="line"><span class="comment"># On of the reasons we do this is because we need data to be between positive and negative and not all the be negative or positive. </span></span><br><span class="line">X -= np.mean(X, axis = <span class="number">1</span>)</span><br><span class="line"><span class="comment">#np.mean()中的参数axis指定了哪个维度被压缩成1，例如axis=0,则输出的结果为一行，即求得输入x的每一列的平均，压缩成一行，同理axis=1，则输出为一列，该结果中的每一行为按照行进行平均的值。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Then apply the standard deviation. Hint: in images we don't do this.</span></span><br><span class="line">X /= np.std(X, axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p>To normalize images:对图像进行标准化</p><ul><li>Subtract the mean image (E.g. Alexnet)<ul><li>Mean image shape is the same as the input images.</li></ul></li><li>Or Subtract per-channel mean<ul><li>Means calculate the mean for each channel of all images. Shape is 3 (3 channels)</li></ul></li></ul></li><li><p>First idea is to initialize the w’s with small random numbers:</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.rand(D, H)</span><br><span class="line"><span class="comment"># Works OK for small networks but it makes problems with deeper networks!</span></span><br></pre></td></tr></table></figure></li><li><p>The standard deviations is going to zero in deeper networks. and the gradient will vanish sooner in deep networks.使用任意小的数字进行对w初始化，随着网络的加深可能导致梯度消失问题(每一层的蔬输入很小)。</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">1</span> * np.random.rand(D, H) </span><br><span class="line"><span class="comment"># Works OK for small networks but it makes problems with deeper networks!</span></span><br></pre></td></tr></table></figure></li><li><p>The network will explode with big numbers!，使用大于一的初值可能会导致深层网络中梯度爆炸问题。</p></li></ul></li><li><p>Xavier initialization:</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.rand(in, out) / np.sqrt(in)</span><br></pre></td></tr></table></figure></li><li><p>It works because we want the variance of the input to be as the variance of the output.</p></li><li><p>But it has an issue, It breaks when you are using RELU.</p></li></ul></li><li><p>He initialization(Solution for the RELU issue):</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.rand(in, out) / np.sqrt(in/2)</span><br></pre></td></tr></table></figure></li><li><p>Solves the issue with RELU. Its recommended when you are using RELU</p></li></ul></li></ul><p><strong>Batch normalization</strong>:</p><ul><li><p>is a technique to provide any layer in a Neural Network with inputs that are zero mean/unit variance.</p></li><li><p>It speeds up the training. You want to do this a lot.</p><ul><li>Made by Sergey Ioffe and Christian Szegedy at 2015.</li></ul></li><li><p>We make a Gaussian activations in each layer. by calculating the mean and the variance.</p></li><li><p>Usually inserted after (fully connected or Convolutional layers) and (before nonlinearity).</p></li><li><p>Steps (For each output of a layer)</p><ol><li><p>First we compute the mean and variance^2 of the batch for each feature.</p></li><li><p>We normalize by subtracting the mean and dividing by square root of (variance^2 + epsilon)</p><ul><li>epsilon to not divide by zero</li></ul></li><li><p>Then we make a scale and shift variables:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Result = gamma * normalizedX + beta</span><br></pre></td></tr></table></figure><ul><li>gamma and beta are learnable parameters.</li><li>it basically possible to say “Hey!! I don’t want zero mean/unit variance input, give me back the raw input - it’s better for me.”</li><li>Hey shift and scale by what you want not just the mean and variance!</li></ul></li></ol></li></ul><p><strong>Baby sitting the learning process</strong></p><ol><li>Preprocessing of data.</li><li>Choose the architecture.</li><li>Make a forward pass and check the loss (Disable regularization). Check if the loss is reasonable.</li><li>Add regularization, the loss should go up!</li><li>Disable the regularization again and take a small number of data and try to train the loss and reach zero loss.<ul><li>You should overfit perfectly for small datasets.</li></ul></li><li>Take your full training data, and small regularization then try some value of learning rate.<ul><li>If loss is barely changing, then the learning rate is small.</li><li>If you got <code>NAN</code> then your NN exploded and your learning rate is high.</li><li>Get your learning rate range by trying the min value (That can change) and the max value that doesn’t explode the network.</li></ul></li><li>Do Hyperparameters optimization to get the best hyperparameters values.</li></ol><p>Hyperparameter Optimization</p><ul><li>Try Cross validation strategy.<ul><li>Run with a few ephocs, and try to optimize the ranges.</li></ul></li><li>Its best to optimize in log space.</li><li>Adjust your ranges and try again.</li><li>Its better to try random search instead of grid searches (In log space)</li></ul><h3 id="training-neural-networks-ii"><a class="markdownIt-Anchor" href="#training-neural-networks-ii"></a> Training neural networks II</h3><p><strong>Optimization algorithms</strong>:</p><ul><li><p>Problems with stochastic gradient descent:随机梯度下降算法的问题</p><ul><li><p>if loss quickly in one direction and slowly in another (For only two variables), you will get very slow progress along shallow dimension, jitter along steep direction. Our NN will have a lot of parameters then the problem will be more.</p></li><li><p>Local minimum or saddle points；极小值或者鞍点问题</p><ul><li>何为鞍点？鞍点（Saddle point）在微分方程中，沿着某一方向是稳定的，另一条方向是不稳定的奇点，叫做鞍点。在泛函中，既不是极大值点也不是极小值点的临界点，叫做鞍点。在矩阵中，一个数在所在行中是最大值，在所在列中是最小值，则被称为鞍点。在物理上要广泛一些，指在一个方向是极大值，另一个方向是极小值的点</li></ul><ul><li>If SGD went into local minimum we will stuck at this point because the gradient is zero.遇到极小值点会stuck</li><li>Also in saddle points the gradient will be zero so we will stuck.</li><li>Saddle points says that at some point:鞍点在gradient上的表现<ul><li>Some gradients will get the loss up.</li><li>Some gradients will get the loss down.</li><li>And that happens more in high dimensional (100 million dimension for example)</li></ul></li><li>The problem of deep NN is more about saddle points than about local minimum because deep NN has high dimensions (Parameters)</li><li>Mini batches are noisy because the gradient is not taken for the whole batch.</li></ul></li></ul></li><li><p><strong>SGD + momentum</strong>:引入momentum，解决在鞍点或者极小值点处出现gradient为0而无法及继续更新参数的情况</p><ul><li><p>Build up velocity as a running mean of gradients:</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Computing weighted average. rho best is in range [0.9 - 0.99]</span></span><br><span class="line">V[t+<span class="number">1</span>] = rho * v[t] + dx</span><br><span class="line">x[t+<span class="number">1</span>] = x[t] - learningRate * V[t+<span class="number">1</span>]</span><br></pre></td></tr></table></figure></li><li><p><code>V[0]</code> is zero.</p></li><li><p>Solves the saddle point and local minimum problems.</p></li><li><p>It overshoots the problem and returns to it back.</p></li></ul></li><li><p><strong>Nestrov momentum</strong>:</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dx = compute_gradient(x)</span><br><span class="line">old_v = v</span><br><span class="line">v = rho * v - learning_rate * dx</span><br><span class="line">x+= -rho * old_v + (<span class="number">1</span>+rho) * v</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>AdaGrad</strong></p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(<span class="keyword">True</span>):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># here is a problem, the grad_squared isn't decayed (gets so large)</span></span><br><span class="line">  grad_squared += dx * dx</span><br><span class="line">  </span><br><span class="line">  x -= (learning_rate*dx) / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>RMSProp</strong></p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(<span class="keyword">True</span>):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#Solved ADAgra</span></span><br><span class="line">  grad_squared = decay_rate * grad_squared + (<span class="number">1</span>-grad_squared) * dx * dx  </span><br><span class="line">  </span><br><span class="line">  x -= (learning_rate*dx) / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure></li><li><p>People uses this instead of AdaGrad</p></li></ul></li><li><p><strong>Adam</strong>,最常用的优化器，结合了momentum和RMSProp</p><ul><li>Calculates the momentum and RMSProp as the gradients.</li><li>It need a Fixing bias to fix starts of gradients.</li><li>Is the best technique so far runs best on a lot of problems.</li><li>With <code>beta1 = 0.9</code> and <code>beta2 = 0.999</code> and <code>learning_rate = 1e-3</code> or <code>5e-4</code> is a great starting point for many models!</li></ul></li><li><p><strong>Learning decay</strong>学习率退火，避免因为网络的不断加深而导致学习率相对参数而言过大</p><ul><li>Ex. decay learning rate by half every few epochs.</li><li>To help the learning rate not to bounce out.</li><li>Learning decay is common with SGD+momentum but not common with Adam.</li><li>Dont use learning decay from the start at choosing your hyperparameters. Try first and check if you need decay or not.</li></ul></li></ul><p><strong>Regularization</strong>:损失函数中引入正则化项；集成学习；drop out修改网络结构；数据增强</p><ul><li>So far we have talked about reducing the training error, but we care about most is how our model will handle unseen data!上述优化更多的是在做如何更新参数使得error减少，但我们更加关心的是模型的泛化能力</li><li>What if the gab of the error between training data and validation data are too large?</li><li>This error is called high variance.</li><li>Model Ensemble:<ul><li>Algorithm:<ul><li>Train multiple independent models of the same architecture with different initializations.</li><li>At test time average their results.</li></ul></li><li>It can get you extra 2% performance.</li><li>It reduces the generalization error.</li><li>You can use some snapshots of your NN at the training ensembles them and take the results.</li></ul></li><li>Regularization solves the high variance problem. We have talked about L1, L2 Regularization.</li><li>Some Regularization techniques are designed for only NN and can do better.</li><li>Drop out:使得activation 函数失效，即让其输出在任何输入下都为0<ul><li>In each forward pass, randomly set some of the neurons to zero. Probability of dropping is a hyperparameter that are 0.5 for almost cases.训练过程中随机将部分神经元设置为失活</li><li>So you will chooses some activation and makes them zero.</li><li>It works because:<ul><li>It forces the network to have redundant representation; prevent co-adaption of features!</li><li>If you think about this, It ensemble some of the models in the same model!相当于集成学习，在一个模型中将他的多个不同的子模型进行集成</li></ul></li><li>At test time we might multiply each dropout layer by the probability of the dropout.</li><li>Sometimes at test time we don’t multiply anything and leave it as it is.</li><li>With drop out it takes more time to train.</li><li>Dropout是一种在深度学习环境中应用的正规化手段。它是这样运作的：在一次循环中我们先随机选择神经层中的一些单元并将其临时隐藏，然后再进行该次循环中神经网络的训练和优化过程。在下一次循环中，我们又将隐藏另外一些神经元，如此直至训练结束。<br>在训练时，每个神经单元以概率p被保留(dropout丢弃率为1-p)；在测试阶段，每个神经单元都是存在的，权重参数w要乘以p，成为：pw。测试时需要乘上p的原因：考虑第一隐藏层的一个神经元在dropout之前的输出是x，那么dropout之后的期望值是E=px+(1−p)0 ，在测试时该神经元总是激活，为了保持同样的输出期望值并使下一层也得到同样的结果，需要调整x→pxx→px. 其中p是Bernoulli分布（0-1分布）中值为1的概率</li></ul></li><li>Data augmentation:<ul><li>Another technique that makes Regularization.</li><li>Change the data!</li><li>For example flip the image, or rotate it.</li><li>Example in ResNet:<ul><li>Training: Sample random crops and scales:<ol><li>Pick random L in range [256,480]</li><li>Resize training image, short side = L</li><li>Sample random 224x244 patch.</li></ol></li><li>Testing: average a fixed set of crops<ol><li>Resize image at 5 scales: {224, 256, 384, 480, 640}</li><li>For each size, use 10 224x224 crops: 4 corners + center + flips</li></ol></li><li>Apply Color jitter or PCA</li><li>Translation, rotation, stretching.</li></ul></li></ul></li><li>Drop connect<ul><li>Like drop out idea it makes a regularization.</li><li>Instead of dropping the activation, we randomly zeroing the weights.</li></ul></li></ul><p><strong>Transfer learning</strong>:</p><ul><li>Some times your data is overfitted by your model because the data is small not because of regularization.自己的数据集太小</li><li>You need a lot of data if you want to train/use CNNs.</li><li>Steps of transfer learning<ol><li>Train on a big dataset that has common features with your dataset. Called pretraining.找到一个和你的小的数据集特征类似的打的数据集，并用你的模型在该数据集中进行训练</li><li>Freeze the layers except the last layer and feed your small dataset to learn only the last layer.将模型中除了最后一层外的所有层结构进行冻结，然后在小的数据集中进行训练，以学习最后一层</li><li>Not only the last layer maybe trained again, you can fine tune any number of layers you want based on the number of data you have</li></ol></li></ul></li></ul><h3 id="deep-learning-software"><a class="markdownIt-Anchor" href="#deep-learning-software"></a> Deep learning software</h3><ul><li><p>CPU vs GPU</p><ul><li><p>GPU The graphics card was developed to render graphics to play games or make 3D media,. etc.</p><ul><li>NVIDIA vs AMD<ul><li>Deep learning choose NVIDIA over AMD GPU because NVIDIA is pushing research forward deep learning also makes it architecture more suitable for deep learning.</li></ul></li></ul></li><li><p>CPU has fewer cores but each core is much faster and much more capable; great at sequential tasks. While GPUs has more cores but each core is much slower “dumber”; great for parallel tasks.CPU的核心更少，但是更快，胜任串行任务；GPU的核心更多，但是更慢，胜任并行任务</p></li><li><p>GPU cores needs to work together. and has its own memory.GPU各个核心需要并行工作，而且GPU有自己的内存，称为显存</p></li><li><p>Matrix multiplication is from the operations that are suited for GPUs. It has MxN independent operations that can be done on parallel.矩阵乘法适用于GPU中</p></li><li><p>Convolution operation also can be paralyzed because it has independent operations.卷积操作也能并行化</p></li><li><p>Programming GPUs frameworks:</p><ul><li><p>CUDA</p><p>(NVIDIA only)</p><ul><li>Write c-like code that runs directly on the GPU.</li><li>Its hard to build a good optimized code that runs on GPU. Thats why they provided high level APIs.</li><li>Higher level APIs: cuBLAS, cuDNN, etc</li><li><strong>CuDNN</strong> has implemented back prop. , convolution, recurrent and a lot more for you!</li><li>In practice you won’t write a parallel code. You will use the code implemented and optimized by others!</li></ul></li></ul></li><li><p>If you aren’t careful, training can bottleneck on reading data and transferring to GPU. So the solutions are:训练过程中在读取数据和迁移到gpu的过程中可能出现瓶颈</p><ul><li>Read all the data into RAM. # If possible将所有数据读如到内存</li><li>Use SSD instead of HDD使用固态硬盘</li><li>Use multiple CPU threads to prefetch data!使用多条CPU线程去预读取数据<ul><li>While the GPU are computing, a CPU thread will fetch the data for you.</li><li>A lot of frameworks implemented that for you because its a little bit painful!</li></ul></li></ul></li></ul></li><li><p><strong>Deep learning Frameworks</strong></p><ul><li>Its super fast moving!</li><li>Currently available frameworks:<ul><li>Tensorflow (Google)</li><li>Caffe (UC Berkeley)</li><li>Caffe2 (Facebook)</li><li>Torch (NYU / Facebook)</li><li>PyTorch (Facebook)</li><li>Theano (U monteral)</li><li>Paddle (Baidu)</li><li>CNTK (Microsoft)</li><li>MXNet (Amazon)</li></ul></li><li>The instructor thinks that you should focus on Tensorflow and PyTorch.</li><li>The point of deep learning frameworks:<ul><li>Easily build big computational graphs.方便地构建计算图</li><li>Easily compute gradients in computational graphs.方便地通过计算图计算梯度</li><li>Run it efficiently on GPU (cuDNN - cuBLAS)支持GPU</li></ul></li><li>Numpy doesn’t run on GPU.Numpy不能使用GPU</li><li>Most of the frameworks tries to be like NUMPY in the forward pass and then they compute the gradients for you.很多框架尽力去靠拢NUMPY，同时又能支持GPU</li></ul></li><li><p>**Tensorflow (Google)**静态架构</p><ul><li><p>Code are two parts:</p><ol><li>Define computational graph.定义好计算图</li><li>Run the graph and reuse it many times.运行计算图并多次使用</li></ol></li><li><p>Tensorflow uses a static graph architecture.静态的图结构</p></li><li><p>Tensorflow variables live in the graph. while the placeholders are feed each run.variable在计算图中生存，placeholders用于在计算图中占位，运行时填入数据</p></li><li><p>Global initializer function initializes the variables that lives in the graph.</p></li><li><p>Use predefined optimizers and losses.使用预先定义的优化器和损失函数</p></li><li><p>You can make a full layers with layers.dense function.</p></li><li><p>Keras</p><p>(High level wrapper):</p><ul><li>Keras is a layer on top pf Tensorflow, makes common things easy to do.</li><li>So popular!</li><li>Trains a full deep NN in a few lines of codes.</li></ul></li><li><p>There are a lot high level wrappers:</p><ul><li>Keras</li><li>TFLearn</li><li>TensorLayer</li><li>tf.layers <code>#Ships with tensorflow</code></li><li>tf-Slim <code>#Ships with tensorflow</code></li><li>tf.contrib.learn <code>#Ships with tensorflow</code></li><li>Sonnet <code># New from deep mind</code></li></ul></li><li><p>Tensorflow has pretrained models that you can use while you are using transfer learning.迁移学习的时候可以使用多个预训练模型</p></li><li><p>Tensorboard adds logging to record loss, stats. Run server and get pretty graphs! Tensorboard添加了日志用于跟踪损失</p></li><li><p>It has distributed code if you want to split your graph on some nodes.</p></li><li><p>Tensorflow is actually inspired from Theano. It has the same inspirations and structure.</p></li></ul></li><li><p><strong>PyTorch (Facebook)</strong></p><ul><li>Has three layers of abstraction:<ul><li>Tensor: ndarraybut runs on GPU,Like numpy arrays in tensorflow<ul><li>Variable: Node in a computational graphs; stores data and gradient <code>#Like Tensor, Variable, Placeholders</code></li></ul></li><li>Module: A NN layer; may store state or learnable weights<code>#Like tf.layers in tensorflow</code></li></ul></li><li>In PyTorch the graphs runs in the same loop you are executing which makes it easier for debugging. This is called a dynamic graph.动态图架构，容易进行debugging</li><li>In PyTorch you can define your own autograd functions by writing forward and backward for tensors. Most of the times it will implemented for you.一般只用重写forward函数</li><li>Torch.nn is a high level api like keras in tensorflow. You can create the models and go on and on.<ul><li>You can define your own nn module!</li></ul></li><li>Also Pytorch contains optimizers like tensorflow.</li><li>It contains a data loader that wraps a Dataset and provides minbatches, shuffling and multithreading.自己编写数据加载器很重要</li><li>PyTorch contains the best and super easy to use pretrained models</li><li>PyTorch contains Visdom that are like tensorboard. but Tensorboard seems to be more powerful.提供Visdom用于记录日志</li><li>PyTorch is new and still evolving compared to Torch. Its still in beta state.</li><li>PyTorch is best for research.对于research更加常用</li></ul></li><li><p>Tensorflow builds the graph once, then run them many times (Called static graph)定义一次网络即可多次使用，有专门的保存方式,在工业中更常用</p></li><li><p>In each PyTorch iteration we build a new graph (Called dynamic graph)每次使用都要重新搭建网络,在研究中更常用</p></li><li><p>Tensorflow fold make dynamic graphs easier in Tensorflow through dynamic batching.</p></li><li><p>Dynamic graph applications include: recurrent networks and recursive networks.</p></li><li><p>Caffe2 uses static graphs and can train model in python also works on IOS and Android</p></li><li><p>Tensorflow/Caffe2 are used a lot in production especially on mobile.</p></li></ul><h3 id="cnn-architectures"><a class="markdownIt-Anchor" href="#cnn-architectures"></a> CNN architectures</h3><ul><li>Focuses on CNN architectures that won ImageNet competition since 2012.</li></ul><p><img src="/images/181008/43.png" alt=""></p><ul><li><p>These architectures includes: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlexNet</a>, <a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">VGG</a>, <a href="https://research.google.com/pubs/pub43022.html" target="_blank" rel="noopener">GoogLeNet</a>, and <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">ResNet</a>.</p></li><li><p>Also we will discuss some interesting architectures as we go.</p></li><li><p>The first ConvNet that was made was <a href="http://ieeexplore.ieee.org/document/726791/" target="_blank" rel="noopener">LeNet-5</a> architectures are:by Yann Lecun at 1998.</p><ul><li>Architecture are: <code>CONV-POOL-CONV-POOL-FC-FC-FC</code></li></ul><p><img src="/images/181008/02.jpg" alt=""></p></li><li><p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener"><strong>AlexNet</strong></a> (2012):</p><ul><li>ConvNet that started the evolution and wins the ImageNet at 2012.</li><li>Architecture are: <code>CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MAXPOOL3-FC6-FC7-FC8</code></li><li>Contains exactly <strong>8</strong> layers the first 5 are Convolutional and the last 3 are fully connected layers.</li><li>Some other details:<ul><li>First use of RELU.</li><li>Norm layers but not used any more.</li><li>heavy data augmentation</li><li>Dropout <code>0.5</code></li><li>batch size <code>128</code></li><li>SGD momentum <code>0.9</code></li><li>Learning rate <code>1e-2</code> reduce by 10 at some iterations</li><li>7 CNN ensembles!</li></ul></li><li>AlexNet was trained on GTX 580 GPU with only 3 GB which wasn’t enough to train in one machine so they have spread the feature maps in half. The first AlexNet was distributed!</li><li>Its still used in transfer learning in a lot of tasks.</li><li>Total number of parameters are <code>60 million</code></li></ul></li><li><p><a href="https://arxiv.org/pdf/1409.1556" target="_blank" rel="noopener"><strong>VGGNet</strong></a> (2014) (Oxford)</p><ul><li>Deeper network with more layers.</li><li>Contains 19 layers.</li><li>Won on 2014 with GoogleNet with error 7.3%</li><li>Smaller filters with deeper layers.</li><li>The great advantage of VGG was the insight that multiple 3 × 3 convolution in sequence can emulate the effect of larger receptive fields, for examples 5 × 5 and 7 × 7.</li><li>Used the simple 3 x 3 Conv all through the network.</li></ul><p><img src="/images/181008/03.png" alt=""></p><ul><li>Has a similar details in training like AlexNet. Like using momentum and dropout.</li><li>在卷积神经网络中，感受野的定义是 卷积神经网络每一层输出的特征图（feature map）上的像素点在<strong>原始图像</strong>上映射的区域大小，即每一层feature的感受野都是对于原始输入图像而言的，而不是上一层的输入,但是在计算过程中，需要逐层计算该层在往上的每一层的感受野;公式 (N-1)_RF = f(N_RF, stride, ksize) = (N_RF - 1) * stride(convN) +ksize(convN)，其中，RF是感受野。N_RF和RF有点像，<strong>N代表 neighbour</strong>，指的是第n层的 a feature在n-1层的RF,显然第N层feature map在第N层的RF=1,在N-1层的RF=ksize__convN</li></ul></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;Something new to me when I read such a good notebook about &lt;a href=&quot;http
      
    
    </summary>
    
      <category term="cs231n" scheme="http://aier02.com/categories/cs231n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
  </entry>
  
  <entry>
    <title>optimization</title>
    <link href="http://aier02.com/2018/10/05/optimization/"/>
    <id>http://aier02.com/2018/10/05/optimization/</id>
    <published>2018-10-05T12:44:14.537Z</published>
    <updated>2018-10-05T15:46:13.106Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --><h1 id="optimization"><a class="markdownIt-Anchor" href="#optimization"></a> Optimization</h1><ul><li>Optimization is the process of finding the set of parameters W that minimize the loss function,如何改变w使得损失函数不断减小</li></ul><h4 id="strategy-1-random-search"><a class="markdownIt-Anchor" href="#strategy-1-random-search"></a> Strategy #1 random search</h4><ul><li>核心思想:一次性找到使得损失函数最小的w取值貌似很难？(random search,a bad idea），但是iterative refinement直观上是可行的，故可以给予w一个random 值，然后迭代的改进，使得loss每次都比上次要小。Our strategy will be to start with random weights and iteratively refine them over time to get lower loss</li><li>类似于一个戴眼罩的hiker，在多个角度下尝试往下走，直到山底。如在CIFAR-10中，hills are 30730 dimensional.</li></ul><h4 id="strategy-2-random-local-search"><a class="markdownIt-Anchor" href="#strategy-2-random-local-search"></a> Strategy #2 random local search</h4><ul><li>策略一相当于随意选定某条路径前进，若根据该路径直接走到的地方更低，则选择该方向最优；而策略二相当于先随意选定某条路径，然后循环地在这个路径上进行随意的方向修改，若走到的地方更低，则进行路径的修改，直到循环结束。</li></ul><h4 id="strategy-3-follow-gradient"><a class="markdownIt-Anchor" href="#strategy-3-follow-gradient"></a> Strategy #3 follow gradient</h4><ul><li>不用刻意规划路线，而是要找到每一步的最优方向，即loss下降的方向gradinet，也就是当时脚下的hills的斜坡方向</li><li>所谓gradient就是a generalization of slope for functions that don’t take a single number but a vector of numbers，由在所求函数对整个输入空间中的各个维度的derivative，即偏导数组成。一维导数的定义:</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>d</mi><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac><mo>=</mo><msub><mi>lim</mi><mrow><mi>h</mi><mtext></mtext><mo>→</mo><mn>0</mn></mrow></msub><mfrac><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo>)</mo><mo>−</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>h</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em"></span><span class="strut bottom" style="height:2.179108em;vertical-align:-.7521079999999999em"></span><span class="base displaystyle textstyle uncramped"><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit">d</span><span class="mord mathit">x</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit">d</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mrel">=</span><span class="mop op-limits"><span class="vlist"><span style="top:.6521079999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">h</span><span class="mord mspace"> </span><span class="mrel">→</span><span class="mord mathrm">0</span></span></span></span><span style="top:-2.7755575615628914e-17em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="mop">lim</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit">h</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mbin">+</span><span class="mord mathit">h</span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><ul><li>When the functions of interest take a vector of numbers instead of a single number,(即自变量不止一个,对不同的变量进行求导) we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension.</li><li>gradient是函数增长速率最快的方向,The gradient tells us the slope of the loss function along every dimension, which we can use to make an update。</li></ul><h1 id="computing-gradient"><a class="markdownIt-Anchor" href="#computing-gradient"></a> Computing gradient</h1><p>两种方式:数值解和分析解</p><h4 id="numerical-gradient"><a class="markdownIt-Anchor" href="#numerical-gradient"></a> Numerical gradient</h4><ul><li>实质是根据一维导数的公式进行计算，每次在输入x的一个维度上进行数值求解，即让该维度上旧的值加上指定的h(尽可能小，公式中是goes toward zero)，算得f(x+h)后再算(f(x+h)-f(x))/h，以求得在该维度上的偏导数的近似值,计算完某个维度后注意要设置回原来的输入再进行下个维度的计算.</li><li><code>it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])</code>创建了一个numpy数组的迭代器，这里的flags表示对数组进行多重索引，op_flags表示迭代器对数组x可执行读写操作。<code>print it.multi_index</code>可以得到数组元素所有的索引(以元组形式返回)，如(0,0),(0,1)</li><li>在实际应用中，计算梯度的时候常用centered difference formula</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi>f</mi><mo>(</mo><mi>x</mi><mo>+</mo><mi>h</mi><mo>)</mo><mo>−</mo><mi>f</mi><mo>(</mo><mi>x</mi><mo>−</mo><mi>h</mi><mo>)</mo></mrow><mrow><mn>2</mn><mi>h</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{f(x+h)-f(x-h)}{2h}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em"></span><span class="strut bottom" style="height:2.113em;vertical-align:-.686em"></span><span class="base displaystyle textstyle uncramped"><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">2</span><span class="mord mathit">h</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mbin">+</span><span class="mord mathit">h</span><span class="mclose">)</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mbin">−</span><span class="mord mathit">h</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><ul><li>根据gradient，在每个维度上朝着loss funcitn增长速率最快的方向的负方向进行step_size的更新。Update in negative gradient direction. In the code above, notice that to compute W_new we are making an update in the negative direction of the gradient df since we wish our loss function to decrease, not increase.</li><li>step_size的作用:the gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step</li><li>效率问题:显然对于每个维度(数组索引)进行数值求解，则对于gradient的求解是O(n)，n为w的维度，或者<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">w_{ij}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.716668em;vertical-align:-.286108em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mord mathit" style="margin-right:.05724em">j</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>的个数；对于神经网络而言，参数个数太多，数值解扩展性太差。</li></ul><h4 id="analytic-gradient"><a class="markdownIt-Anchor" href="#analytic-gradient"></a> Analytic gradient</h4><p>通过微积分进行导数的推导,得到导数确切的值，而不是近似解。</p><ul><li>求导数容易出错，所以经常讲分析解和数值解进行比较，称为gradient check。</li><li>举例SVM loss function，对于单个示例<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.58056em;vertical-align:-.15em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>:</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub><mo>=</mo><msub><mo>∑</mo><mrow><mi>j</mi><mo>≠</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mrow><mo fence="true">[</mo><mi>max</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><msubsup><mi>w</mi><mi>j</mi><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>w</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mo>)</mo><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">L_i = \sum_{j\neq y_i} \left[ \max(0, w_j^Tx_i - w_{y_i}^Tx_i + \Delta) \right]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.050005em"></span><span class="strut bottom" style="height:2.51771em;vertical-align:-1.467705em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mop op-limits"><span class="vlist"><span style="top:1.2172049999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span><span class="mrel">≠</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.000005000000000032756em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0"><span class="delimsizing size1">[</span></span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord mathrm">Δ</span><span class="mclose">)</span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0"><span class="delimsizing size1">]</span></span></span></span></span></span></span></p><ul><li>对于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub></mrow><annotation encoding="application/x-tex">w_{y_i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.716668em;vertical-align:-.286108em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>,</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mrow><msub><mi>w</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub></mrow></msub><msub><mi>L</mi><mi>i</mi></msub><mo>=</mo><mo>−</mo><mrow><mo fence="true">(</mo><msub><mo>∑</mo><mrow><mi>j</mi><mo>≠</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mrow><mn mathvariant="normal">1</mn></mrow><mo>(</mo><msubsup><mi>w</mi><mi>j</mi><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>w</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mo>&gt;</mo><mn>0</mn><mo>)</mo><mo fence="true">)</mo></mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\nabla_{w_{y_i}} L_i = - \left( \sum_{j\neq y_i} \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) \right) x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:2.05002em"></span><span class="strut bottom" style="height:3.60004em;vertical-align:-1.55002em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:.14999999999999997em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.31472em;margin-right:.1em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptscriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord">−</span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped"><span class="delimsizing mult"><span class="vlist"><span style="top:.9049999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="delimsizinginner delim-size4"><span>⎝</span></span></span><span style="top:-.89502em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="delimsizinginner delim-size4"><span>⎛</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.2172049999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span><span class="mrel">≠</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.000005000000000032756em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">1</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord mathrm">Δ</span><span class="mrel">&gt;</span><span class="mord mathrm">0</span><span class="mclose">)</span><span class="style-wrap reset-textstyle textstyle uncramped"><span class="delimsizing mult"><span class="vlist"><span style="top:.9049999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="delimsizinginner delim-size4"><span>⎠</span></span></span><span style="top:-.89502em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="delimsizinginner delim-size4"><span>⎞</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></span></p><p>where 𝟙 is the indicator function that is one if the condition inside is true or zero otherwise<br>对于<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">w_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.43056em"></span><span class="strut bottom" style="height:.716668em;vertical-align:-.286108em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>,</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mrow><msub><mi>w</mi><mi>j</mi></msub></mrow></msub><msub><mi>L</mi><mi>i</mi></msub><mo>=</mo><mrow><mn mathvariant="normal">1</mn></mrow><mo>(</mo><msubsup><mi>w</mi><mi>j</mi><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><msubsup><mi>w</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><mi>T</mi></msubsup><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mo>&gt;</mo><mn>0</mn><mo>)</mo><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\nabla_{w_j} L_i = \mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \Delta &gt; 0) x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.891331em"></span><span class="strut bottom" style="height:1.274439em;vertical-align:-.383108em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathrm">∇</span><span class="vlist"><span style="top:.14999999999999997em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.02691em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">1</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.02691em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord mathrm">Δ</span><span class="mrel">&gt;</span><span class="mord mathrm">0</span><span class="mclose">)</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></span></p><ul><li>Once you derive the expression for the gradient it is straight-forward to implement the expressions and use them to perform the gradient update</li></ul><h1 id="gradient-descent"><a class="markdownIt-Anchor" href="#gradient-descent"></a> Gradient descent</h1><ul><li>the procedure of repeatedly evaluating the gradient and then performing a parameter update is called Gradient Descent</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">while True:</span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data, weights)</span><br><span class="line">  weights += - step_size * weights_grad # perform parameter update</span><br></pre></td></tr></table></figure><ul><li>Mini-batch gradient descent.对于输入数据量庞大的training set，为了更新w的某个索引下的值而对庞大的data进行操作有点浪费，常用的方法是compute the gradient over batches of the training data</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">while True:</span><br><span class="line">  data_batch = sample_training_data(data, 256) # sample 256 examples</span><br><span class="line">  weights_grad = evaluate_gradient(loss_fun, data_batch, weights)</span><br><span class="line">  weights += - step_size * weights_grad # perform parameter update</span><br></pre></td></tr></table></figure><ul><li>为何在一小部分的数据中更新w也是可行的？The reason this works well is that the examples in the training data are correlated</li><li>The extreme case of this is a setting where the mini-batch contains only a single example. This process is called Stochastic Gradient Descent (SGD) (or also sometimes on-line gradient descent)比较少见</li></ul><h1 id="summary"><a class="markdownIt-Anchor" href="#summary"></a> Summary</h1><p><img src="/images/181005/dataflow.jpeg" alt=""></p><ul><li>两种求gradient的方法:We discussed the tradeoffs between computing the numerical and analytic gradient. The numerical gradient is simple but it is approximate and expensive to compute. The analytic gradient is exact, fast to compute but more error-prone since it requires the derivation of the gradient with math. Hence, in practice we always use the analytic gradient and then perform a gradient check, in which its implementation is compared to the numerical gradient</li><li>梯度下降方法:We introduced the Gradient Descent algorithm which iteratively computes the gradient and performs a parameter update in loop</li><li>总的来说优化就是通过寻找loss function下降速度最快的方向(gradient的反方向)，对w进行不断的适当幅度(learning rate)的更新，使得loss不断减少。</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;optimization&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#optimization&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="cs231n" scheme="http://aier02.com/categories/cs231n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
      <category term="image classification" scheme="http://aier02.com/tags/image-classification/"/>
    
  </entry>
  
  <entry>
    <title>Linear classification</title>
    <link href="http://aier02.com/2018/10/03/linear_classification/"/>
    <id>http://aier02.com/2018/10/03/linear_classification/</id>
    <published>2018-10-03T05:31:44.756Z</published>
    <updated>2018-10-05T16:09:16.104Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --><h2 id="linear-classification"><a class="markdownIt-Anchor" href="#linear-classification"></a> Linear classification</h2><h3 id="multiclass-svm"><a class="markdownIt-Anchor" href="#multiclass-svm"></a> Multiclass SVM</h3><ul><li>基本形式为y=wx+b，此时的x为列向量，一列为一个样本，w的每一行为一个class的template。</li><li>loss function:Multiclass Support Vector Machine (SVM) loss,SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin Δ;Δ为超参，需要人为设定，它的存在说明多类svm关注的和普通的svm思想上是一致的，都是关注距离超平面一定范围内的误分类点，也就是间隔边界内的点，所以这里的损失函数和合页损失函数的设计是一样的；故第i张图像的损失函数为</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub><mo>=</mo><msub><mo>∑</mo><mrow><mi>j</mi><mo>≠</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mi>max</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><msub><mi>s</mi><mi>j</mi></msub><mo>−</mo><msub><mi>s</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.050005em"></span><span class="strut bottom" style="height:2.51771em;vertical-align:-1.467705em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mop op-limits"><span class="vlist"><span style="top:1.2172049999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span><span class="mrel">≠</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.000005000000000032756em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit">s</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord mathrm">Δ</span><span class="mclose">)</span></span></span></span></span></p><p>注意这里的sj表示的是该图像在第j类的得分，而yi表示的是该图像的label，即只要计算其错误分类的所有分数和正确分类的分数的差值之和，当label类的得分没有大于某个非label类得分margin时，两者的差值会被算入loss中。<br><img src="/images/181002/margin.jpg" alt=""></p><ul><li>引入regularization</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mo>(</mo><mi>W</mi><mo>)</mo><mo>=</mo><msub><mo>∑</mo><mi>k</mi></msub><msub><mo>∑</mo><mi>l</mi></msub><msubsup><mi>W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>l</mi></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">R(W) = \sum_k\sum_l W_{k,l}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.0500050000000005em"></span><span class="strut bottom" style="height:2.3521180000000004em;vertical-align:-1.3021129999999999em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.00773em">R</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.13889em">W</span><span class="mclose">)</span><span class="mrel">=</span><span class="mop op-limits"><span class="vlist"><span style="top:1.2021129999999998em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.03148em">k</span></span></span><span style="top:-.000005000000000254801em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.2021129999999998em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.01968em">l</span></span></span><span style="top:-.000005000000000254801em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:.13889em">W</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:.03148em">k</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.01968em">l</span></span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></span></p><ul><li>完整的multicalss SVM loss:</li></ul>L = \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} + \underbrace{ \lambda R(W) }_\text{regularization loss}<ul><li>扩展形式为:</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mi>N</mi></mrow></mfrac><msub><mo>∑</mo><mi>i</mi></msub><msub><mo>∑</mo><mrow><mi>j</mi><mo>≠</mo><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mrow><mo fence="true">[</mo><mi>max</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">;</mo><mi>W</mi><msub><mo>)</mo><mi>j</mi></msub><mo>−</mo><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">;</mo><mi>W</mi><msub><mo>)</mo><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mo>)</mo><mo fence="true">]</mo></mrow><mo>+</mo><mi>λ</mi><msub><mo>∑</mo><mi>k</mi></msub><msub><mo>∑</mo><mi>l</mi></msub><msubsup><mi>W</mi><mrow><mi>k</mi><mo separator="true">,</mo><mi>l</mi></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">L = \frac{1}{N} \sum_i \sum_{j\neq y_i} \left[ \max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \Delta) \right] + \lambda \sum_k\sum_l W_{k,l}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.32144em"></span><span class="strut bottom" style="height:2.789145em;vertical-align:-1.467705em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">L</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span style="top:-.000005000000000143778em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.2172049999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span><span class="mrel">≠</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.000005000000000032756em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0">[</span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">;</span><span class="mord mathit" style="margin-right:.13889em">W</span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">;</span><span class="mord mathit" style="margin-right:.13889em">W</span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.07142857142857144em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord mathrm">Δ</span><span class="mclose">)</span><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0">]</span></span><span class="mbin">+</span><span class="mord mathit">λ</span><span class="mop op-limits"><span class="vlist"><span style="top:1.2021129999999998em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.03148em">k</span></span></span><span style="top:-.000005000000000254801em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.2021129999999998em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.01968em">l</span></span></span><span style="top:-.000005000000000254801em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:.13889em">W</span><span class="vlist"><span style="top:.24700000000000003em;margin-left:-.13889em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style="margin-right:.03148em">k</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.01968em">l</span></span></span></span><span style="top:-.41300000000000003em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></span></p><p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.69444em;vertical-align:0"></span><span class="base textstyle uncramped"><span class="mord mathit">λ</span></span></span></span>为超参，常用cross validation确定，表示模型的一种偏好。</p><ul><li>L2范数作为正则化项的好处是:The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself,即在wx+b得分一样的情况下，L2范数的模型偏向于选择smaller and diffuse weights，使得没有哪个维度影响很大。</li><li>setting delta:一般设置为1.0是安全的，在loss function中delta和lambda其实具有相同effect在tradeoff上，所以真正有意义的是对于lambda的控制</li><li>与二分类的svm比较:</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>L</mi><mi>i</mi></msub><mo>=</mo><mi>C</mi><mi>max</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><msup><mi>w</mi><mi>T</mi></msup><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><mo>+</mo><mi>R</mi><mo>(</mo><mi>W</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">L_i = C \max(0, 1 - y_i w^Tx_i) + R(W)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.8913309999999999em"></span><span class="strut bottom" style="height:1.1413309999999999em;vertical-align:-.25em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">L</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mord mathit" style="margin-right:.07153em">C</span><span class="mop">max</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mbin">−</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:.02691em">w</span><span class="vlist"><span style="top:-.413em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:.00773em">R</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.13889em">W</span><span class="mclose">)</span></span></span></span></span></p><p>这里的<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>∈</mo><mo>{</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo>}</mo></mrow><annotation encoding="application/x-tex">y_i \in \{ -1,1 \}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">∈</span><span class="mopen">{</span><span class="mord">−</span><span class="mord mathrm">1</span><span class="mpunct">,</span><span class="mord mathrm">1</span><span class="mclose">}</span></span></span></span></p><h3 id="softmax-classifier"><a class="markdownIt-Anchor" href="#softmax-classifier"></a> Softmax classifier</h3><ul><li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo separator="true">;</mo><mi>W</mi><mo>)</mo><mo>=</mo><mi>W</mi><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">f(x_i; W) = W x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">;</span><span class="mord mathit" style="margin-right:.13889em">W</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:.13889em">W</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span>和svm保持一致，但经过softmax层用于指示概率的大小，损失函数由hinge loss变成cross-entropy loss</li></ul>L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.5in} \text{or equivalently} \hspace{0.5in} L_i = -f_{y_i} + \log\sum_j e^{f_j}<ul><li><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>f</mi><mi>j</mi></msub><mo>(</mo><mi>z</mi><mo>)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>z</mi><mi>j</mi></msub></mrow></msup></mrow><mrow><msub><mo>∑</mo><mi>k</mi></msub><msup><mi>e</mi><mrow><msub><mi>z</mi><mi>k</mi></msub></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.939285em"></span><span class="strut bottom" style="height:1.4942920000000002em;vertical-align:-.555007em"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.04398em">z</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.34500000000000003em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mop"><span class="op-symbol small-op mop" style="top:.074995em">∑</span><span class="vlist"><span style="top:.30001em;margin-right:.07142857142857144em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.03148em">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.3574928571428571em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.04398em">z</span><span class="vlist"><span style="top:.34963999999999995em;margin-right:.1em;margin-left:-.04398em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptscriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.03148em">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.394em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.47143571428571435em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.04398em">z</span><span class="vlist"><span style="top:.31472000000000006em;margin-right:.1em;margin-left:-.04398em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptscriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>称为softmax function,这里的z即为wx+b，整个softmax function estimated class probabilities，即unnormalized log probabilities</li><li>交叉熵:p为true distibution，q为estimated distribution</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>H</mi><mo>(</mo><mi>p</mi><mo separator="true">,</mo><mi>q</mi><mo>)</mo><mo>=</mo><mo>−</mo><msub><mo>∑</mo><mi>x</mi></msub><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo><mi>log</mi><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">H(p,q) = - \sum_x p(x) \log q(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.050005em"></span><span class="strut bottom" style="height:2.3000100000000003em;vertical-align:-1.250005em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.08125em">H</span><span class="mopen">(</span><span class="mord mathit">p</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.03588em">q</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord">−</span><span class="mop op-limits"><span class="vlist"><span style="top:1.150005em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">x</span></span></span><span style="top:-.000005000000000032756em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord mathit">p</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mord mathit" style="margin-right:.03588em">q</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p><ul><li>Practical issues: Numeric stability:</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>f</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub></mrow></msup></mrow><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><mrow><msub><mi>f</mi><mi>j</mi></msub></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>C</mi><msup><mi>e</mi><mrow><msub><mi>f</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub></mrow></msup></mrow><mrow><mi>C</mi><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><mrow><msub><mi>f</mi><mi>j</mi></msub></mrow></msup></mrow></mfrac><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><msub><mi>f</mi><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow></msub><mo>+</mo><mi>log</mi><mi>C</mi></mrow></msup></mrow><mrow><msub><mo>∑</mo><mi>j</mi></msub><msup><mi>e</mi><mrow><msub><mi>f</mi><mi>j</mi></msub><mo>+</mo><mi>log</mi><mi>C</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{e^{f_{y_i}}}{\sum_j e^{f_j}} = \frac{Ce^{f_{y_i}}}{C\sum_j e^{f_j}} = \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.533218em"></span><span class="strut bottom" style="height:2.655414em;vertical-align:-1.122196em"></span><span class="base displaystyle textstyle uncramped"><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686078em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mop"><span class="op-symbol small-op mop" style="top:-.0000050000000000050004em">∑</span><span class="vlist"><span style="top:.30001em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.30997em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.37011em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.31472em;margin-right:.1em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptscriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686078em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:.07153em">C</span><span class="mop"><span class="op-symbol small-op mop" style="top:-.0000050000000000050004em">∑</span><span class="vlist"><span style="top:.30001em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.30997em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:.07153em">C</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.37011em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.31472em;margin-right:.1em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptscriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686078em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mop"><span class="op-symbol small-op mop" style="top:-.0000050000000000050004em">∑</span><span class="vlist"><span style="top:.30001em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.30997em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mord mathit" style="margin-right:.07153em">C</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.37011em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15000000000000002em;margin-right:.07142857142857144em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle cramped"><span class="mord scriptscriptstyle cramped"><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.31472em;margin-right:.1em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptscriptstyle scriptscriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mord mathit" style="margin-right:.07153em">C</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><p>常用的设置方法是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mi>C</mi><mo>=</mo><mo>−</mo><msub><mi>max</mi><mi>j</mi></msub><msub><mi>f</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\log C = -\max_j f_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.980548em;vertical-align:-.286108em"></span><span class="base textstyle uncramped"><span class="mop">lo<span style="margin-right:.01389em">g</span></span><span class="mord mathit" style="margin-right:.07153em">C</span><span class="mrel">=</span><span class="mord">−</span><span class="mop"><span class="mop">max</span><span class="vlist"><span style="top:.15em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord"><span class="mord mathit" style="margin-right:.10764em">f</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.10764em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.05724em">j</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></p><ul><li>The Softmax classifier gets its name from the softmax function, which is used to squash the raw class scores into normalized positive values that sum to one, so that the cross-entropy loss can be applied.</li></ul><h3 id="svm-vs-softmax"><a class="markdownIt-Anchor" href="#svm-vs-softmax"></a> SVM vs. Softmax</h3><ul><li>主要区别在有loss function:</li></ul><p><img src="/images/181003/svmvssoftmax.png" alt=""></p><p>两者的分数向量f都是一样的，不同的在于对f的解释，svm认为f是对应种类的得分，hinge loss鼓励正确的class得分比所有错误的class score都高出一个margin；而softmax认为f是在没有标准化之前表示的是属于某个种类的log概率，并且cross entropy鼓励正确的正确分类的概率大(-log§变小)</p><ul><li>Hence, the probabilities computed by the Softmax classifier are better thought of as confidences where, similar to the SVM</li></ul><h3 id="further-reading"><a class="markdownIt-Anchor" href="#further-reading"></a> Further Reading</h3><p><a href="https://arxiv.org/abs/1306.0239" target="_blank" rel="noopener">Deep Learning using Linear Support Vector Machines</a> from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;linear-classification&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#linear-c
      
    
    </summary>
    
      <category term="cs231n" scheme="http://aier02.com/categories/cs231n/"/>
    
    
      <category term="notebook" scheme="http://aier02.com/tags/notebook/"/>
    
      <category term="linear classification" scheme="http://aier02.com/tags/linear-classification/"/>
    
      <category term="SVM" scheme="http://aier02.com/tags/SVM/"/>
    
  </entry>
  
  <entry>
    <title>pytorch cookbook U2&amp;U3</title>
    <link href="http://aier02.com/2018/10/02/pytorch_cookbook-U2&amp;U3/"/>
    <id>http://aier02.com/2018/10/02/pytorch_cookbook-U2&amp;U3/</id>
    <published>2018-10-02T08:27:32.398Z</published>
    <updated>2018-10-05T15:50:37.362Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --><h1 id="pytorch-cookbook"><a class="markdownIt-Anchor" href="#pytorch-cookbook"></a> pytorch-cookbook</h1><h2 id="第二章"><a class="markdownIt-Anchor" href="#第二章"></a> 第二章</h2><ul><li><p>函数名后带下划线会修改函数本身如y.add_(x)会直接修改y</p></li><li><p>pytorch的tensor和numpy的对象共享内存，两者同时改变;对于tensor不支持的操作，可以先转为numpy进行操作在转为tensor（tensor支持gpu）</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=t.ones(5)</span><br><span class="line">b = a.numpy() # Tensor -&gt; Numpy</span><br><span class="line">a = np.ones(5)</span><br><span class="line">b = t.from_numpy(a) # Numpy-&gt;Tensor</span><br></pre></td></tr></table></figure><ul><li>tensor[idx]得到的为0-dim的tensor，scalar.item()获取tensor的单个元素对象</li><li>t.Tensor(5,3)创建5行3列的tensor，t.tensor([3,4])创建包含3，4两个元素的tensor</li><li>t.tensor()会进行数据拷贝，新的tensor和旧的不共享内存，而torch.from_numpy（）或者tensor.detach()则相反</li><li>使用gpu</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = t.device(&quot;cuda:0&quot; if t.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line">x = x.to(device)</span><br><span class="line">y = y.to(device)</span><br></pre></td></tr></table></figure><ul><li>autograd: 自动微分;要想使得Tensor使用autograd功能，只需要设置<code>tensor.requries_grad=True</code>.如：x = t.ones(2, 2, requires_grad=True)</li><li>注意：<code>grad</code>在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以反向传播之前需把梯度清零。# 以下划线结束的函数是inplace操作，会修改自身的值，就像add__</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad.data.zero_()</span><br></pre></td></tr></table></figure><ul><li>一起求导的过程示例</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x=t.ones(2,2,requires_grad=True)#生成tensor</span><br><span class="line">y=x.sum()#定义表达式</span><br><span class="line">y.grad_fn#查看求导函数</span><br><span class="line">y.backward()#back propagation</span><br><span class="line">x.grad#查看y对x的导数</span><br><span class="line">x.grad.data_zero_()#清空导数缓存空间</span><br></pre></td></tr></table></figure><ul><li>nerual network的定义主要是对torch.nn模块的使用,<br>定义网络时，需要继承<code>nn.Module</code>，并实现它的forward方法，把网络中具有可学习参数的层放在构造函数<code>__init__</code>中。如果某一层(如ReLU)不具有可学习的参数，则既可以放在构造函数中，也可以不放，但建议不放在其中，而在forward中使用<code>nn.functional</code>代替,forwar的输入和输出都是tensor,input = t.randn(1, 1, 32, 32),需要注意的是，torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用 <code>input.unsqueeze(0)</code>将batch_size设为１,size形式为nSamples x nChannels x height x weight</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # nn.Module子类的函数必须在构造函数中执行父类的构造函数</span><br><span class="line">        # 下式等价于nn.Module.__init__(self)</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        </span><br><span class="line">        # 卷积层 &apos;1&apos;表示输入图片为单通道, &apos;6&apos;表示输出通道数，&apos;5&apos;表示卷积核为5*5</span><br><span class="line">        self.conv1 = nn.Conv2d(1, 6, 5) </span><br><span class="line">        # 卷积层</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 5) </span><br><span class="line">        # 仿射层/全连接层，y = Wx + b</span><br><span class="line">        self.fc1   = nn.Linear(16*5*5, 120) </span><br><span class="line">        self.fc2   = nn.Linear(120, 84)</span><br><span class="line">        self.fc3   = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x): </span><br><span class="line">        # 卷积 -&gt; 激活 -&gt; 池化 </span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), 2) </span><br><span class="line">        # reshape，‘-1’表示自适应</span><br><span class="line">        x = x.view(x.size()[0], -1) </span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)        </span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure><ul><li>conv layer主要特征是局部连接和权重共享</li><li></li></ul><p>局部连接：每个神经元仅与输入神经元的一块区域连接，这块局部区域称作感受野（receptive field）。在图像卷积操作中，即神经元在空间维度（spatial dimension，即上图示例H和W所在的平面）是局部连接，但在深度上是全部连接。对于二维图像本身而言，也是局部像素关联较强。这种局部连接保证了学习后的过滤器能够对于局部的输入特征有最强的响应。局部连接的思想，也是受启发于生物学里面的视觉系统结构，视觉皮层的神经元就是局部接受信息的。<br>*<br>权重共享：计算同一个深度切片的神经元时采用的滤波器是共享的。例上图中计算o[:,:,0]的每个每个神经元的滤波器均相同，都为W0，这样可以很大程度上减少参数。共享权重在一定程度上讲是有意义的，例如图片的底层边缘特征与特征在图中的具体位置无关。但是在一些场景中是无意的，比如输入的图片是人脸，眼睛和头发位于不同的位置，希望在不同的位置学到不同的特征 。请注意权重只是对于同一深度切片的神经元是共享的，在卷积层，通常采用多组卷积核提取不同特征，即对应不同深度切片的特征，不同深度切片的神经元权重是不共享。另外，偏重对同一深度切片的所有神经元都是共享的。</p><ul><li>池化是非线性下采样的一种形式，主要作用是通过减少网络的参数来减小计算量，并且能够在一定程度上控制过拟合。</li><li>网络的可学习参数通过<code>net.parameters()</code>返回,<code>net.named_parameters</code>可同时返回可学习的参数及名称。</li><li>nn.MSELoss()实现均方误差，nn.CrossEntropyLoss()实现交叉熵损失</li><li>优化器更新参数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> import torch.optim as optim</span><br><span class="line">#新建一个优化器，指定要调整的参数和学习率</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr = 0.01)</span><br><span class="line"></span><br><span class="line"># 在训练过程中</span><br><span class="line"># 先梯度清零(与net.zero_grad()效果一样)</span><br><span class="line">optimizer.zero_grad() </span><br><span class="line"></span><br><span class="line"># 计算损失</span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line">#反向传播</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">#更新参数</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><ul><li>数据加载和预处理：使用torchvision</li><li>示例：下面我们来尝试实现对CIFAR-10数据集的分类，步骤如下:</li></ul><ol><li>使用torchvision加载并预处理CIFAR-10数据集,得到dataset和dataloader</li><li>定义网络,继承nn.Module,init中写入可学习的参数函数，forward定义好前向传播的过程</li><li>定义损失函数和优化器，criterion和optimizer</li><li>训练网络并更新网络参数，在每个ephco中加载数据，传入net，算loss，loss.backward，optimizer.step</li><li>测试网络</li></ol><ul><li></li></ul><p>定义对数据的预处理:将两种转化合并一起；ToTensor()将shape为(H, W, C)的nump.ndarray或img转为shape为(C, H, W)的tensor，其将每一个数值归一化到[0,1]，其归一化方法比较简单，直接除以255即可，加入normalize则其作用就是先将输入归一化到(0,1)，再使用公式”(x-mean)/std”，将每个元素分布到(-1,1),函数normalize（std,mean）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">        transforms.ToTensor(), # 转为Tensor</span><br><span class="line">        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化</span><br><span class="line">                             ])</span><br></pre></td></tr></table></figure><ul><li>Dataset对象是一个数据集，可以按下标访问，返回形如(data, label)的数据。Dataloader是一个可迭代的对象，它将dataset返回的每一条数据拼接成一个batch，并提供多线程加速优化和数据打乱等操作。当程序对dataset的所有数据遍历完一遍之后，相应的对Dataloader也完成了一次迭代，先定义好dataset，然后定义dataloader对指定的dataset进行操作</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> # 训练集</span><br><span class="line">trainset = tv.datasets.CIFAR10(</span><br><span class="line">                    root=&apos;/home/cy/tmp/data/&apos;, </span><br><span class="line">                    train=True, </span><br><span class="line">                    download=True,</span><br><span class="line">                    transform=transform)</span><br><span class="line"></span><br><span class="line">trainloader = t.utils.data.DataLoader(</span><br><span class="line">                    trainset, </span><br><span class="line">                    batch_size=4,</span><br><span class="line">                    shuffle=True, </span><br><span class="line">                    num_workers=2)</span><br></pre></td></tr></table></figure><ul><li>进行normaliza的必要性：每个样本图像减去数据集图像的均值后除以方差，保证了所有图像的分布相似，使得model训练的时候更快的收敛.</li><li>训练网络的示例</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">t.set_num_threads(8)</span><br><span class="line">for epoch in range(2):  </span><br><span class="line">    </span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    for i, data in enumerate(trainloader, 0):</span><br><span class="line">        </span><br><span class="line">        # 输入数据</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        </span><br><span class="line">        # 梯度清零</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        # forward + backward </span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()   </span><br><span class="line">        </span><br><span class="line">        # 更新参数 </span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        # 打印log信息</span><br><span class="line">        # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0]</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        if i % 2000 == 1999: # 每2000个batch打印一下训练状态</span><br><span class="line">            print(&apos;[%d, %5d] loss: %.3f&apos; \</span><br><span class="line">                  % (epoch+1, i+1, running_loss / 2000))</span><br><span class="line">            running_loss = 0.0</span><br><span class="line">print(&apos;Finished Training&apos;)</span><br></pre></td></tr></table></figure><h2 id="第三章"><a class="markdownIt-Anchor" href="#第三章"></a> 第三章</h2><ul><li>表3-1: 常见新建tensor的方法</li></ul><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">Tensor(*sizes)</td><td style="text-align:center">基础构造函数</td></tr><tr><td style="text-align:center">tensor(data,)</td><td style="text-align:center">类似np.array的构造函数</td></tr><tr><td style="text-align:center">ones(*sizes)</td><td style="text-align:center">全1Tensor</td></tr><tr><td style="text-align:center">zeros(*sizes)</td><td style="text-align:center">全0Tensor</td></tr><tr><td style="text-align:center">eye(*sizes)</td><td style="text-align:center">对角线为1，其他为0</td></tr><tr><td style="text-align:center">arange(s,e,step</td><td style="text-align:center">从s到e，步长为step</td></tr><tr><td style="text-align:center">linspace(s,e,steps)</td><td style="text-align:center">从s到e，均匀切分成steps份</td></tr><tr><td style="text-align:center">rand/randn(*sizes)</td><td style="text-align:center">均匀/标准分布</td></tr><tr><td style="text-align:center">normal(mean,std)/uniform(from,to)</td><td style="text-align:center">正态分布/均匀分布</td></tr><tr><td style="text-align:center">randperm(m)</td><td style="text-align:center">随机排列</td></tr></tbody></table><ul><li>除了<code>tensor.size()</code>，还可以利用<code>tensor.shape</code>直接查看tensor的形状，<code>tensor.shape</code>等价于<code>tensor.size()</code></li><li>tensor = t.Tensor(1,2)创建了一个size为【1，2】的张量</li><li>vector = t.tensor([1, 2])创建了一个值为（1，2）的向量，size为2</li><li>scalar = t.tensor(3.14159) 创建了一个值为3.14159的标量，<br>size为【】,区别于size【0】，empty_tensor = t.tensor([])，size存在即为tensor,否则为scalar</li><li>通过<code>tensor.view</code>方法可以调整tensor的形状，但必须保证调整前后元素总数一致。<code>view</code>不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。<code>b = a.view(-1, 3)</code>当某一维为-1的时候，会自动计算它的大小,</li><li>torch.squeeze() 这个函数主要对数据的维度进行压缩，去掉维数为1的的维度，比如是一行或者一列这种，一个一行三列（1,3）的数去掉第一个维数为一的维度之后就变成（3）行。squeeze(a)就是将a中所有为1的维度删掉。不为1的维度没有影响。a.squeeze(N) 就是去掉a中指定的维数为一的维度。还有一种形式就是b=torch.squeeze(a，N) a中去掉指定的定的维数为一的维度。</li><li>torch.unsqueeze()这个函数主要是对数据维度进行扩充。给指定位置加上维数为一的维度，比如原本有个三行的数据（3），在0的位置加了一维就变成一行三列（1,3）。a.unsqueeze(N) 就是在a中指定位置N加上一个维数为1的维度。还有一种形式就是b=torch.unsqueeze(a，N) a就是在a中指定位置N加上一个维数为1的维度</li><li><code>resize</code>是另一种可用来调整<code>size</code>的方法，但与<code>view</code>不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存，当再次扩展时其值为当时缩小保存的值</li><li>tensor的索引操作和tensor共享内存，即更改其中一个，另一个也会更改。</li><li>a[None]:None类似于np.newaxis, 为a新增了一个轴；等价于a.view(1, a.shape[0], a.shape[1])</li><li>a &gt; 1 # 返回一个ByteTensor,即满足条件的位置值为1，否则为0</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;pytorch-cookbook&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#pytorch-cookb
      
    
    </summary>
    
      <category term="pytorch" scheme="http://aier02.com/categories/pytorch/"/>
    
    
      <category term="basic knowledge" scheme="http://aier02.com/tags/basic-knowledge/"/>
    
      <category term="pytorch cookbook" scheme="http://aier02.com/tags/pytorch-cookbook/"/>
    
  </entry>
  
  <entry>
    <title>first time in Kaggle-summary</title>
    <link href="http://aier02.com/2018/10/02/rsna_summary/"/>
    <id>http://aier02.com/2018/10/02/rsna_summary/</id>
    <published>2018-10-01T16:08:39.573Z</published>
    <updated>2018-10-04T14:10:53.908Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --><h1 id="项目存在的问题和解决方案"><a class="markdownIt-Anchor" href="#项目存在的问题和解决方案"></a> 项目存在的问题和解决方案</h1><h2 id="rsna"><a class="markdownIt-Anchor" href="#rsna"></a> RSNA</h2><h3 id="如何开始比赛"><a class="markdownIt-Anchor" href="#如何开始比赛"></a> 如何开始比赛</h3><ul><li>完全是新手，很早以前就有学长介绍过kaggle，最近看完了cs231n，然后打算试试手，但是一开始并不知道应该做什么，于是就上知乎直接搜了如何打kaggle比赛，找到的很多的都是ml的，但是个人更喜欢cv，添加了关键词后，找到了一篇和我类似经历的blog，知道了kaggle比赛到底是什么，他的每个板块表示的是什么，常用的步骤是什么，有什么需要注意的（这个时候开始意识到我没有gpu，这点必定是后期的瓶颈）</li></ul><h3 id="选择平台"><a class="markdownIt-Anchor" href="#选择平台"></a> 选择平台</h3><ul><li>没有gpu怎么办，没有资源只能去租用云平台跑model，开头抱有侥幸心理，有没有什么免费的平台呢，一开始找到的是google 的colab，听说是免费的，然后我就直接尝试使用，跑了一下mnist的基本模型发现经常自动断开连接，而且他给的免费的硬盘空间只有15g，当时我首先打算打的比赛其实是有关air ship的detection比赛。果然天底下没有免费的午餐，只能另找平台，后来在知乎上看到不少人推荐极客云，他自动帮你搭建好了深度学习的环境，（其实这是个坑，创建的dl环境是不能直接操作整个系统的，任何系统的指令都无法操作，这使得我后来无法进行端口的查询，visdom启动的时候总是提示端口被占用），运行的速度和价钱的确很吸引人</li></ul><h3 id="eda"><a class="markdownIt-Anchor" href="#eda"></a> EDA</h3><ul><li>什么是lung opacities，什么是pneumonia，他们两者有什么关系么？由于缺乏领域知识，得益于知乎，我直接奔向了discussion和kernel，果然找到了一篇名为what r lung opacities的kernel，这篇文章由一个具备radiologist领域知识的kaggler提供，他直接介绍了怎么看chest x-ray，黑色的为air，白色的为bone，grey为tissue或者fluid，这个kernel对于我整个项目影响最大，他补充了我xray的领域知识。做eda主要是看各个文件中的数量、样本中是否存在重复，是否有缺失，class_label.csv文件中有多少中不同的class，各自的数量如何，他与train数据集是否一一对应，open（）函数，创建一个reader，next跳过header之后就循环读取reader实现按行读取数据；也可用pd.read_csv实现，value_counts()统计重复的次数，groupby（keyword）可以根据keyword分组；主要用到了matplotlib.pyplot，numpy，pandas，pydicom，glob进行数据可视化</li></ul><h3 id="detection还是segmentation"><a class="markdownIt-Anchor" href="#detection还是segmentation"></a> detection？还是segmentation？</h3><ul><li>诚然我一开始看到官方的比赛介绍的时候，我主观上是认为要根据dicom图像提供的病人的个人信息以及图像中的信息进行classification，根据我个人对于部分pneumonia的xray的观察，以及提供的数据集中的bboxes的信息和之前提到的那篇kernel的引导，本次比赛更多的是做能指示pneumonia的lung opacities，而且明显是难以完全sgment出来的（opacities有一种是模糊了心脏和lung的边界。于是决定做detection</li></ul><h3 id="数据准备"><a class="markdownIt-Anchor" href="#数据准备"></a> 数据准备</h3><ul><li>前期所做的数据准备，我的想法是直接把pneumonia的bbox的x,y,w,h装入内存，很明显这中做法有一定的风险，遇到内存不足恐怕直接爆了，后期我选择了使用npy文件先预先读取train_lable的信息，然后每次使用的时候再进行读取，这样的做法个人认为可以避免每次训练的时候进行pydicom数据的转换，也能减少内存的使用，后果用了更多的磁盘空间，而且每次读取npy文件都是存在时间成本的。而且有个严重的问题，就是后期如何做数据增强，是对image进行操作，pydicom.dcmread()和cv2.imread读都是hwc，而且是BGR上的0-255为了使后期进行读取方便，我把使用sitk.ReadImage（dcm）将所有的dcm图像转为png格式，sitk.GetArrayFromImage返回的是（hwc），同时存储患有肺炎的图像的numpy数组和bbox的位置信息（ymin,xmin,ymax,xmax），以npy文件的格式保存，实际上在这次操作中图片都是单通道的，而且都是1024*1024的。然后通过对patientid进行岁进shuffle，创建train和valdation，大概9:1</li></ul><h3 id="数据增强"><a class="markdownIt-Anchor" href="#数据增强"></a> 数据增强</h3><ul><li>经过eda后，lung opacities，no lung opacities/not normal,normal,比例是1:1:1，把lung opacities作为positive，则dataset中存在样例不平衡，考虑进行图像的增强，旋转，平移，亮度改变之类的，由于是x-ray图像，考虑进行图像的仿射变换，但是旋转会涉及bbox的改变，因为之前在数字图像课程实验中学过仿射变换（旋转，缩放，平移）然后就直接实现了rotate_img_bbox(img, bboxes, degree=-45, scale=1.)，<br>mat = cv2.getRotationMatrix2D(center,angle=degree,scale=scale) #affine matrix，仿射矩阵为（2，3）的矩阵，以水平进行划分，前（2，2）子矩阵为线性变换矩阵，（2，1）子矩阵为平移矩阵，用标量的形式来看就是ax+b；T=M【x,y,1】,对原来的bbox的四条边的四个中点进行相同的矩阵变化，然后合并为一个矩阵，表示一个仿射后的矩阵，矩形边框（Bounding Rectangle）是说，用一个最小的矩形，把找到的形状包起来。还有一个带旋转的矩形，面积会更小，即使用rx, ry, rw, rh = cv2.boundingRect(concat)得到摆正后的经过仿射变换的新的bbox，整个图像的变换out_image = cv2.warpAffine(img,mat,(width,height))</li><li>做各种变换的时候由于数据集较大，而且操作多，经常出现等待时间较长的情况，缺乏可视化，不能确定是完成了操作还是仍然在等待，后来在循环中使用了tqdm进行进度条的显示，可视化了进程的进度。</li></ul><h3 id="准备dataset"><a class="markdownIt-Anchor" href="#准备dataset"></a> 准备dataset</h3><p>创建类generator(keras.utils.Sequence)，实现的函数分别有</p><ul><li>内置init()，初始化文件夹路径，文件名，bboxing box，batch_size=32,image_size=256(原图为1024)，shuffle，augment，predict为三者为真还是假</li><li>内置len（），返回filenames中数据量的大小</li><li>内置getitem（），以batchz_size为数据单位，根据index确定数据的位置，当predict为真时，返回对应文件名的图像和文件名；否则，返回对应文件名的img图像和msk，注意msks即为bboxes的列表</li><li>内置load（），通过filename确定patientid，然后读取npy文件获取img_array和bboxes列表（每一项为对应图片的bbox的ymin,xmin,ymax,xmax），创建和img等大小的全0msk，根据npy中的‘bboxes’项将对应的标注框区域设置为1；然后resize图片，特别注意由于训练的时候进行的是batch_train,所以对于img和msk都必须添加一个dimension，作为训练的维度</li><li>内置loadpredict（），基本操作和load函数一致，不同在与该函数不用获取bboxes信息</li></ul><h3 id="搭建神经网络"><a class="markdownIt-Anchor" href="#搭建神经网络"></a> 搭建神经网络</h3><ul><li>create_downsample(channels, inputs)下采样函数，channels指示filter的大小，inputs指示images，<a href="http://xn--keras-2b3h3g252orkjqiznnd.layers.BN" target="_blank" rel="noopener">依次使用的是keras.layers.BN</a>(momentum=0.9)-&gt;leakyrelu-&gt;conv2d(padding=same)-&gt;maxpool2d。padding：补0策略，为“valid”, “same” 。“valid”代表只进行有效的卷积，即对边界数据不处理。“same”代表保留边界处的卷积结果，通常会导致输出shape与输入shape相同</li><li>create_resblock(channels, inputs)resblock函数，一个resblock包含了BN-&gt;LEAKYRELU-&gt;CONV2D-&gt;BN-&gt;LEAKYRELU-&gt;CON2D-&gt;add([x, inputs])(channels用于conv层中filters的数目，即输出的维度，kernel_size指定filter的大小)</li><li>create_network(input_size, channels, n_blocks=2, depth=4)搭建整个网络，inputs = keras.Input(shape=(input_size, input_size, 1))对输入进行规范化-&gt;conv2d-&gt;创建depth层结构，每个结构中包括了一个downsample和n_block个resblock-&gt;ouput layer依次为BN-&gt;LEAKYRELU-&gt;CONV2D-&gt;UpSampling2D(2**depth)(x)-&gt;将input和output包装为一个model即model = keras.Model(inputs=inputs, outputs=outputs)</li></ul><h3 id="resnet"><a class="markdownIt-Anchor" href="#resnet"></a> Resnet</h3><ul><li>背景:随着网络的加深，出现了训练集准确率下降的现象，排除overfitting，针对该问题提出了resnet，以允许实现尽可能地加深网络</li><li>resnet中提出了两种mapping，identity mapping 和 residual mapping，输出为y=F(x)+x,显然x为前者，F(x)为后者；理论上，对于“随着网络加深，准确率下降”的问题，Resnet提供了两种选择方式，也就是identity mapping和residual mapping，如果网络已经到达最优，继续加深网络，residual mapping将被push为0，只剩下identity mapping，这样理论上网络一直处于最优状态了，网络的性能也就不会随着深度增加而降低了。即真正学习的是残差，而形式上y保证了不会出现网络加深而经验误差增大的现象</li><li>在resnet中加入1*1的conv layer就是bottleneck layer，目的是为了降维，降低计算量和参数的数目，最后又升维是为了保持和输入x的dimensions一致</li></ul><h3 id="batch-normalization"><a class="markdownIt-Anchor" href="#batch-normalization"></a> Batch Normalization</h3><ul><li>BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。(IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障</li><li>在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。然后提出了BatchNorm的基本思想：能不能让每个隐层节点的激活输入分布固定下来呢？这样就避免了“Internal Covariate Shift”问题了。</li><li>启发式思考：对输入图像进行白化（Whiten）操作的话——所谓白化，就是对输入数据分布变换到0均值，单位方差的正态分布——那么神经网络会较快收敛；BN所做的可以理解为对深层神经网络每个隐层神经元的激活值做简化版本的白化操作</li><li>简而言之，对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛地快。将输入x的分布强制转换到均值为0，方差为1的正态分布。</li><li>经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。</li><li>BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者右移一点并长胖一点或者变瘦一点</li><li>①不仅仅极大提升了训练速度，收敛过程大大加快；②还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等</li></ul><h3 id="定义loss-function"><a class="markdownIt-Anchor" href="#定义loss-function"></a> 定义loss function</h3><ul><li>定义iou损失函数:iou_loss(y_true, y_pred),这里的label(即y)是mask，即bboxes为1的0-1图像，intersection = tf.reduce_sum(y_true * y_pred)求的共同区域的1的个数，score = (intersection + 1.) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)，注意+1.是为了保证score不为0，公式为inserction/union。reduce_sum返回该矩阵所有元素之和</li><li>合并损失函数:0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * iou_loss(y_true, y_pred)，binary_crossentropy为对数损失函数</li><li>损失函数是训练的关键，他表达了模型的目标，通过求原图像经过resnet后的feature map和图像的label，即bboxes的iou，使用adam进行优化，使得网络参数朝iou减少的方向进行更新。</li></ul><h3 id="训练参数的设置"><a class="markdownIt-Anchor" href="#训练参数的设置"></a> 训练参数的设置</h3><ul><li>定义tf.metric算子，即评估指标算子，用于计算accuracy</li><li>model.compile(optimizer=‘adam’,<br>loss=iou_bce_loss,<br>metrics=[‘accuracy’, mean_iou])指定优化器，损失函数和accuracy</li><li>初始化lr=0.01，epoch=25，lr的更新策略:lrx(np.cos(np.pi*x/epochs)+1.)/2,x为动态变化的epoch</li><li>keras.callbacks.LearningRateScheduler(schedule)<br>该回调函数是用于动态设置学习率，其中schedule函数以epoch号为参数（从0算起的整数），返回一个新学习率（浮点数）</li></ul><h3 id="创建train和validation-generator"><a class="markdownIt-Anchor" href="#创建train和validation-generator"></a> 创建train和validation generator</h3><ul><li>指定图片文件夹:folder = ‘./input/stage_1_train_png’</li><li>训练生成器:train_gen = generator(folder, train_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=True, augment=True, predict=False)</li><li>测试生成器:valid_gen = generator(folder, valid_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=False, predict=False)</li></ul><h3 id="训练模型"><a class="markdownIt-Anchor" href="#训练模型"></a> 训练模型</h3><ul><li>history = model.fit_generator(train_gen, validation_data=valid_gen, callbacks=[learning_rate], epochs=25, workers=4, use_multiprocessing=True)用于指定callback内容，即learning rate，训练集，验证集，epoch数量，线程数量</li></ul><h3 id="测试模型"><a class="markdownIt-Anchor" href="#测试模型"></a> 测试模型</h3><ul><li>创建测试数据生成器:test_gen = generator(folder, test_filenames, None, batch_size=25, image_size=256, shuffle=False, predict=True)</li><li>threshold predicted mask,pred中大于0.5才为mask<br>comp = pred[:, :, 0] &gt; 0.5<br>apply connected components<br>comp = measure.label(comp),measure.label作用是给comp标记连通区域，用于确定不同的bbox，</li><li>measure.regionprops(comp)获取comp的不同连通区域，返回的region列表，每一个分别为ymin,xmin,ymax,xmax</li><li>计算置信度proxy for confidence score:conf = np.mean(pred[y:y+height, x:x+width])</li></ul><h3 id="提交结果"><a class="markdownIt-Anchor" href="#提交结果"></a> 提交结果</h3><ul><li>保存结果到csv文件中:以字典的形式保存到csv文件中save dictionary as csv file<br>sub = pd.DataFrame.from_dict(submission_dict,orient=‘index’)</li><li>指定索引名称:sub.index.names = [‘patientId’]</li><li>指定列名:sub.columns = [‘PredictionString’]</li><li>写入指定目录文件:sub.to_csv(’/input/submission.csv’)</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;项目存在的问题和解决方案&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#项目存在的问题和解决方案&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="Kaggle" scheme="http://aier02.com/categories/Kaggle/"/>
    
    
      <category term="segmentation" scheme="http://aier02.com/tags/segmentation/"/>
    
      <category term="keras" scheme="http://aier02.com/tags/keras/"/>
    
      <category term="ResNet" scheme="http://aier02.com/tags/ResNet/"/>
    
      <category term="jikecloud" scheme="http://aier02.com/tags/jikecloud/"/>
    
  </entry>
  
  <entry>
    <title>python rubbish collection</title>
    <link href="http://aier02.com/2018/09/28/python_rubbish_collection/"/>
    <id>http://aier02.com/2018/09/28/python_rubbish_collection/</id>
    <published>2018-09-28T07:32:02.260Z</published>
    <updated>2018-10-03T08:08:16.164Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --><h2 id="python垃圾回收机制"><a class="markdownIt-Anchor" href="#python垃圾回收机制"></a> python垃圾回收机制</h2><h3 id="引用计数"><a class="markdownIt-Anchor" href="#引用计数"></a> 引用计数</h3><p>每个对象维护一个ob_ref字段，每次被别的对象引用的ob_ref加1，若引用失效，则减1，当ob_ref为0则该对象被回收，占用的内存空间被释放。但是该方法不能解决循环引用问题，即两个对象相互引用，当他们的外部引用都失效时，ob_ref仍为1，非零，但是他们实质是要被回收的，而python却不能将其回收</p><h3 id="标记清除"><a class="markdownIt-Anchor" href="#标记清除"></a> 标记清除</h3><p>对活动对象进行标记，将非活动对象进行回收。对象之间通过引用（指针）连在一起，构成一个有向图，对象构成这个有向图的节点，而引用关系构成这个有向图的边。从根对象（root object）出发，沿着有向边遍历对象，可达的（reachable）对象标记为活动对象，不可达的对象就是要被清除的非活动对象。根对象就是全局变量、调用栈、寄存器。</p><h3 id="分代回收"><a class="markdownIt-Anchor" href="#分代回收"></a> 分代回收</h3><p>根据对象存活时间的不同对内存进行了划分，时间由短到长分别划分为年轻代，中年代，老年代，新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发，把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，依此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;python垃圾回收机制&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#python垃圾回收机制&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="python" scheme="http://aier02.com/categories/python/"/>
    
    
      <category term="basic knowledge" scheme="http://aier02.com/tags/basic-knowledge/"/>
    
  </entry>
  
  <entry>
    <title>faster RCNN</title>
    <link href="http://aier02.com/2018/09/18/faster_RCNN/"/>
    <id>http://aier02.com/2018/09/18/faster_RCNN/</id>
    <published>2018-09-18T10:55:07.479Z</published>
    <updated>2018-10-03T08:08:10.513Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --><h2 id="faster-rcnn"><a class="markdownIt-Anchor" href="#faster-rcnn"></a> faster RCNN</h2><ul><li>感受野：在卷积神经网络CNN中，决定某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野receptive field</li><li>fatal error: numpy/arrayobject.h: No such file or directory”​<br>这个错误的出现可以如下解决，将setup.py内容加入一条include_dirs=[numpy.get_include()]​就可以了。<br>示例setup.py文件如下：<br>from distutils.core import setup<br>from distutils.extension import Extension<br>from Cython.Distutils import build_ext<br>import numpy as np<br>ext_modules=[Extension(“test03”,[“test03.pyx”])]<br>setup(<br>name=‘gravity_cy’,<br>cmdclass={‘build_ext’:build_ext},<br>include_dirs = [np.get_include()],<br>ext_modules=ext_modules<br>) 主要问题是要指定numpy的路径</li><li>过程记录：1）下载faster-rcnn的预训练模型并在demo中进行调试<br>2）运行python misc/convert_caffe_pretrain.py把预训练的vgg16下载并保存到checkpoint中，作为cnn提取器<br>3）</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;faster-rcnn&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#faster-rcnn&quot;&gt;&lt;/a&gt; 
      
    
    </summary>
    
      <category term="pytorch" scheme="http://aier02.com/categories/pytorch/"/>
    
    
      <category term="CNN" scheme="http://aier02.com/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>analysis kernel demo</title>
    <link href="http://aier02.com/2018/09/18/analysis_kernel_demo/"/>
    <id>http://aier02.com/2018/09/18/analysis_kernel_demo/</id>
    <published>2018-09-18T10:55:03.519Z</published>
    <updated>2018-10-10T13:01:59.502Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --><h2 id="analysis-kernel-demo"><a class="markdownIt-Anchor" href="#analysis-kernel-demo"></a> analysis kernel demo</h2><ul><li><p>BatchNormalization层：该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1<a href="https://arxiv.org/pdf/1502.03167v3.pdf" target="_blank" rel="noopener">BN</a></p></li><li><p>leakyRelu层,大于0为x，小于0为ax(a常为0.01)</p></li><li><p>pytorch自定义dataset和dataloader</p></li><li><p>torch.utils.data.Dataset是表示数据集的抽象类。您自定义的数据集应该继承Dataset并重写以下方法：len使用len(dataset)将返回数据集的大小。getitem 支持索引，dataset[i]可以获取第i个样本让我们为我们的人脸标记点数据集创建一个数据集类。我们将在init中读取csv，而将在getitem存放读取图片的任务。因为所有的图像不是一次性存储在内存中，而是根据需要进行读取，这样可以高效的使用内存。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomDataset</span><span class="params">(data.Dataset)</span>:</span><span class="comment">#需要继承data.Dataset</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        <span class="comment"># 1. Initialize file path or list of file names.</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        <span class="comment"># 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).</span></span><br><span class="line">        <span class="comment"># 2. Preprocess the data (e.g. torchvision.Transform).</span></span><br><span class="line">        <span class="comment"># 3. Return a data pair (e.g. image and label).</span></span><br><span class="line">        <span class="comment">#这里需要注意的是，第一步：read one data，是一个data</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># You should change 0 to the total size of your dataset.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><p>将numpy ndarry转为tensor：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ToTensor</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Convert ndarrays in sample to Tensors."""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, sample)</span>:</span></span><br><span class="line">        image, landmarks = sample[<span class="string">'image'</span>], sample[<span class="string">'landmarks'</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># swap color axis because</span></span><br><span class="line">        <span class="comment"># numpy image: H x W x C</span></span><br><span class="line">        <span class="comment"># torch image: C X H X W </span></span><br><span class="line">        image = image.transpose((<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'image'</span>: torch.from_numpy(image),</span><br><span class="line">                <span class="string">'landmarks'</span>: torch.from_numpy(landmarks)&#125;</span><br></pre></td></tr></table></figure><p>ending</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;analysis-kernel-demo&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#analysis-
      
    
    </summary>
    
      <category term="pytorch" scheme="http://aier02.com/categories/pytorch/"/>
    
    
      <category term="demo" scheme="http://aier02.com/tags/demo/"/>
    
  </entry>
  
  <entry>
    <title>first time in Kaggle-preparation</title>
    <link href="http://aier02.com/2018/09/06/rsna_pneumonia_detection/"/>
    <id>http://aier02.com/2018/09/06/rsna_pneumonia_detection/</id>
    <published>2018-09-06T09:08:00.932Z</published>
    <updated>2018-10-03T08:15:40.619Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --><h2 id="question-description"><a class="markdownIt-Anchor" href="#question-description"></a> Question description</h2><ul><li>In this competition, you’re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs.(CXR)</li><li>胸部X线片上肺部阴影的定位以跟踪可能的肺炎病况</li><li>They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review.</li><li>But the title “Pneumonia Detection” for the competition is misleading because you actually have to do “Lung Opacities Detection”, and lung opacities are not the same as pneumonia. Lung opacities are vague, fuzzy clouds of white in the darkness of the lungs, which makes detecting them a real challenge.-from a kernal</li><li>如何确认x照片中的阴影？</li></ul><h2 id="basic-information"><a class="markdownIt-Anchor" href="#basic-information"></a> Basic information</h2><ul><li>in CXR:black-air;white-bones;grey-fluid or tissue</li><li>Usually the lungs are full of air. When someone has pneumonia, the air in the lungs is replaced by other material - fluids, bacteria, immune system cells, etc. That’s why areas of opacities are areas that are grey but should be more black.(找出原来是黑但是变灰了的区域</li><li>As you can see, lung opacities are not homogenoues and they do not have a clear center or clear boundaries. I don’t think you can properly segment opacities out of the entire picture because there are no clear boundries.-直接找阴影貌似很难，可以先segment lungs</li><li>in fact, there was only a moderate level of agreement between radiologists about the presence of infiltrates, which are opacities by definition</li><li>预测任务为病人的dcm图像找到lung opacities,以bbox的形式给出，在submission_file中，一张图片中只能有一条bbox信息，每4个信息项为一个bbox，即(x,y,weight,height)</li></ul><h2 id="difficulties"><a class="markdownIt-Anchor" href="#difficulties"></a> Difficulties</h2><ul><li>there is a mass of tissue surrounding the lungs and between the lungs. These areas contain skin, muscles, fat, bones, and also the heart and big blood vessels. That translates into a lot of information on the chest radiograph that is not useful for this competition.</li><li>阴影有多种，怎样的阴影才和肺炎相关The main difference in the types of opacities between these two patients is the borders and the shape of the opacity, Patient 3 has multiple round and clearly defined opacities. Patient 2 has this poorly defined haziness which obscures the margins of the lungs and heart. This haziness is termed consolidation.</li><li>Exclude: obvious mass(es), nodule(s), lobar collapse, linear atelectasis要找的阴影是模糊的，难懂的，不明显的</li><li>In the cases labeled Not Normal/No Lung Opacity, no lung opacity refers to no opacity suspicious for pneumonia. 在数据集中，不正常/没有肺部阴影，是指没有与肺炎相关的阴影，但会有与其他病症相关的阴影，故是不正常。</li><li>两种肺炎阴影：Ground-Glass Opacities；Consolidations前者We can see that the lungs are “whiter” than they should be, but we can see most of the borders of the lungs and hear后者There are fuzzy areas in the lungs and the borders of the lungs and heart cannot be seen.玻璃粉状的阴影不会影响心脏和肺部的边界，而合并类的阴影会模糊两者的边界</li><li>预测pneumoni和opacities应该独立预测？lung opacities并不是是pneumonia的充要条件</li><li>实际上该比赛为Lung Opacities Detectio，而不是Pneumonia Detection</li></ul><h2 id="exprolatory-data-analysis"><a class="markdownIt-Anchor" href="#exprolatory-data-analysis"></a> exprolatory data analysis</h2><ul><li>stage_1_detailed_class_info里面包括了3种class，共有28989条记录，每条信息为patientid：class，标注了病人id对应的正常、不正常/不是肺炎，肺炎3种情况</li><li>stage_1_train_labels记录了bbox的位置，同样有28989条记录，每条记录分别是patientid：x,y,width,height,target;x,y表示bbox的左上角的坐标，width是宽度，即x的范围，height是高度，即y的范围，target=0表示没有肺炎，=1表示有肺炎，对应class_info，normal则bbox信息empty，target=0；No Lung Opacity / Not Normal则bbox信息empty，target=0；因此lung opacities in data is associated with pneumonia；lung opacities 则包含了bbox的信息同时target=1</li><li>注意数据集存在一名病人对应多条记录的情况，并不是有28989名病人，经过eda可知存在25684名病人（patientid）(training data 中有25684张dcm)实际上因为bbox信息表是一条记录只能表示一个bbox，故存在一个病人的cxr中有多个bbox，而class表和label表相一致，所以保持对应关系class存在重复数据</li><li>训练集中dcm文件不只是image，还有meta information，如sex，age等，是否需要添加此类属性，从而考虑相关的内容进行判断？</li><li>patientId - A patientId. Each patientId corresponds to a unique image.</li><li>x_ - the upper-left x coordinate of the bounding box.</li><li>y_ - the upper-left y coordinate of the bounding box.</li><li>width_ - the width of the bounding box.</li><li>height_ - the height of the bounding box.</li><li>Target_ - the binary Target, indicating whether this sample has evidence of pneumonia.</li><li>No Lung Opacity / Not Normal and Normal have together the same percent (69.077%) as the percent of missing values for target window in class details information.显然存在正样本偏少的情况，需要做数据增强，过采样？角度偏转？</li></ul><h2 id="models"><a class="markdownIt-Anchor" href="#models"></a> models</h2><h3 id="chexnet"><a class="markdownIt-Anchor" href="#chexnet"></a> ChexNet</h3><ul><li>The CheXNet algorithm is a 121-layer deep 2D Convolutional Neural Network; a Densenet after Huang &amp; Liu. The Densenet’s multiple residual connections reduce parameters and training time, allowing a deeper, more powerful model. The model accepts a vectorized two-dimensional image of size 224 pixels by 224 pixels.</li><li>CheXNet is a 121-layer Dense Convolutional Network (DenseNet) (Huang et al., 2016) trained on the ChestX-ray 14 dataset. DenseNets improve flow of information and gradients through the network, making the optimization of very deep networks tractable. We replace the final fully connected layer with one that has a single output, after which we apply a sigmoid nonlinearity. The weights of the network are initialized with weights from a model pretrained on ImageNet (Deng et al., 2009). The network is trained end-to-end using Adam with standard parameters (ß1 = 0.9 and ß2 = 0.999) (Kingma &amp; Ba, 2014). We train the model using minibatches of size 16. We use an initial learning rate of 0.001 that is decayed by a factor of 10 each time the validation loss plateaus after an epoch, and pick the model with the lowest validation loss.</li></ul><p><img src="/images/180906/chexnet.jpg" alt=""></p><p><img src="/images/180906/densenet.jpg" alt=""></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --&gt;&lt;h2 id=&quot;question-description&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#question-
      
    
    </summary>
    
      <category term="Kaggle" scheme="http://aier02.com/categories/Kaggle/"/>
    
    
      <category term="segmentation" scheme="http://aier02.com/tags/segmentation/"/>
    
      <category term="chest X-ray" scheme="http://aier02.com/tags/chest-X-ray/"/>
    
      <category term="ChexNet" scheme="http://aier02.com/tags/ChexNet/"/>
    
      <category term="EDA" scheme="http://aier02.com/tags/EDA/"/>
    
  </entry>
  
  <entry>
    <title>experience from a Kaggler</title>
    <link href="http://aier02.com/2018/09/05/kaggle_expericence/"/>
    <id>http://aier02.com/2018/09/05/kaggle_expericence/</id>
    <published>2018-09-05T02:19:10.278Z</published>
    <updated>2018-10-03T08:08:35.464Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --><h1 id="kaggle比赛步骤-经验"><a class="markdownIt-Anchor" href="#kaggle比赛步骤-经验"></a> kaggle比赛步骤-经验</h1><p>1.仔细阅读比赛介绍和数据描述；</p><p>2.查找相似的Kaggle比赛。作为一个接触不久的Kaggler，我已经完成对所有Kaggle比赛基本分析的收集；</p><p>3.研究相似比赛的解决方案；</p><p>4.阅读有关论文，以确保不错过该领域的最新进展；</p><p>5.分析数据，并构建可靠的交叉验证结果；</p><p>6.数据预处理、特征工程和模型训练。</p><p>7.结果分析，包括如预测分布、错误分析和困难样本等；</p><p>8.根据分析来改进模型或设计新模型；</p><p>9.基于数据分析和结果分析来设计模型以增加多样性或解决困难样本；</p><p>10.模型集成；</p><p>11.必要时返回到前面的某个步骤。</p><p>Q：你觉得，赢得比赛的关键是什么？</p><p>可靠的验证方式，借鉴其他比赛并阅读相关论文，以及良好的自制力和心理素质。</p><p>Q：你认为你最具竞争力的比赛技巧或方法是什么？</p><p>我认为应该是在比赛开始时准备解决方案的文档。我会强迫自己写出一份清单，包括面临的挑战、应该阅读的解决方案和论文、可能的风险、可用的验证方式、可能的数据增强方法以及增加模型多样性的方式。而且，我不断更新这个文档。幸运地，这些文档为我后面在很多比赛中取得不错成绩提供了支持</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;kaggle比赛步骤-经验&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=&quot;#kaggle比赛步骤-经验&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="Kaggle" scheme="http://aier02.com/categories/Kaggle/"/>
    
    
      <category term="experience" scheme="http://aier02.com/tags/experience/"/>
    
  </entry>
  
  <entry>
    <title>a failed experience in Kaggle</title>
    <link href="http://aier02.com/2018/09/05/airbus_ship_detection/"/>
    <id>http://aier02.com/2018/09/05/airbus_ship_detection/</id>
    <published>2018-09-05T02:18:30.433Z</published>
    <updated>2018-10-03T08:08:00.121Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --><h1 id="airbus-ship-detection-challenge"><a class="markdownIt-Anchor" href="#airbus-ship-detection-challenge"></a> airbus-ship-detection challenge</h1><h2 id="问题描述"><a class="markdownIt-Anchor" href="#问题描述"></a> 问题描述：</h2><p>从卫星图像中找到ships，并用bbox分割出来</p><h2 id="难点所在"><a class="markdownIt-Anchor" href="#难点所在"></a> 难点所在</h2><ul><li>并不是所有图片都有ships</li><li>在图片中ships的大小不一致</li><li>图像分割不允许存在部分重叠，但数据集中的ships当两者直接相邻时，存在轻微的overlap，重叠部分将会被作为背景而移除</li><li>部分带有人工标注的图片中的bbox可能存在边界像素的丢失</li><li>train-ship-segmentations.csv提供了人工标注的bbox作为训练的数据图片，其中bbox以run-length encoding 表示。</li><li>数据集很大，需要用到gpu，训练模型可能要几天时间</li><li>run-length-encoding</li><li>评价方式为IoU,即人工标注的bbox和预测的bbox的相交部分与两者合并的部分的占比。</li></ul><h2 id="edaexploratory-data-analysis"><a class="markdownIt-Anchor" href="#edaexploratory-data-analysis"></a> EDA(exploratory data analysis)</h2><ul><li>The images (at least many of them) are slices of the same image, not separate frames taken at different times.</li><li>data leak:The images in test are just shifted versions of images in train. The problem also happens in train, it looks like airbus had bigger images and then the 768x768 are random crops of the bigger images; but it looks they didn’t check whether there were any overlaps.<br>How to find:<br>Run nearest neighbors on all images<br>For each image take N closest neighbors and find where it overlaps</li><li>Cutting all the images 256x256 they can be found easier because they match almost exactly (except for compression artifacts).</li></ul><h2 id="up-to-now"><a class="markdownIt-Anchor" href="#up-to-now"></a> up to now</h2><p>due to data leak,large data set,few computation source and time limit,I decided to pause the competition.</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --&gt;&lt;h1 id=&quot;airbus-ship-detection-challenge&quot;&gt;&lt;a class=&quot;markdownIt-Anchor&quot; href=
      
    
    </summary>
    
      <category term="Kaggle" scheme="http://aier02.com/categories/Kaggle/"/>
    
    
      <category term="segmentation" scheme="http://aier02.com/tags/segmentation/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to statistical learning method U1</title>
    <link href="http://aier02.com/2018/08/28/statistic_1/"/>
    <id>http://aier02.com/2018/08/28/statistic_1/</id>
    <published>2018-08-27T16:58:59.803Z</published>
    <updated>2018-10-03T08:31:04.196Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --><p>李航老师的《统计学习方法》基本已经过一遍了，感觉只是略懂皮毛，现为了加强知识点的认识和部分课后题的实现，有必要进行个人总结。</p><h1 id="统计学习"><a class="markdownIt-Anchor" href="#统计学习"></a> 统计学习</h1><h2 id="定义"><a class="markdownIt-Anchor" href="#定义"></a> 定义</h2><p>statistical learning 是关于计算机基于数据构建构建统计模型并运用模型对数据进行预测和分析的一门学科，是概率论、统计学、信息论、计算理论、最优化理论和计算机科学等学科的交叉学科。</p><h2 id="学习对象和目的"><a class="markdownIt-Anchor" href="#学习对象和目的"></a> 学习对象和目的</h2><p>从数据出发，提取数据的特征，抽象出数据的模型（概率统计模型），并对数据进行预测和分析</p><h2 id="实现统计学习方法的一般步骤"><a class="markdownIt-Anchor" href="#实现统计学习方法的一般步骤"></a> 实现统计学习方法的一般步骤</h2><ul><li>得到一个有限的训练数据集合</li><li>确定包含所有可能的模型的假设空间，即学习的模型的集合</li><li>确定模型选择的准则，即学习的策略</li><li>实现求解最优模型的算法，即学习的算法</li><li>通过学习方法选择最优的模型</li><li>利用最优模型对新的数据进行预测或则分析</li></ul><p>简而言之：数据-&gt;假设空间-&gt;策略-&gt;算法-&gt;最优模型-&gt;预测分析</p><h1 id="监督学习"><a class="markdownIt-Anchor" href="#监督学习"></a> 监督学习</h1><p>统计学习包括了监督学习，非监督学习，半监督学习和强化学习。</p><h2 id="基本概念"><a class="markdownIt-Anchor" href="#基本概念"></a> 基本概念</h2><p>输入所有可能取值的集合称为输入空间，同理输出所有可能的取值集合称为输出空间，通常output space 远小于 input space；而特征空间则是所有特征向量所在的空间，特征向量用于表示一个输入实例，特征空间的每个维度对应一个特征，输入空间可以和特征空间相同，也可把输入空间映射到空间，模型实质是定义在特征空间上的（对特征向量进行学习）。X表示输入变量，Y表示输出变量，输入变量X中的第i个表示为</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mo>(</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mn>1</mn><mo>)</mo></mrow></msubsup><mo separator="true">,</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mn>2</mn><mo>)</mo></mrow></msubsup><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msubsup><mi>x</mi><mi>i</mi><mrow><mo>(</mo><mi>n</mi><mo>)</mo></mrow></msubsup><msup><mo>)</mo><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.0448em"></span><span class="strut bottom" style="height:1.321664em;vertical-align:-.276864em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mrel">=</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.276864em;margin-left:0;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span style="top:-.5198em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">1</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.276864em;margin-left:0;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span style="top:-.5198em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mopen">(</span><span class="mord mathrm">2</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mpunct">,</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.276864em;margin-left:0;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span style="top:-.5198em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mopen">(</span><span class="mord mathit">n</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:-.413em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></span></p><p>N个数据的训练数据集(labled)表示为</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>T</mi><mo>=</mo><mo>{</mo><mo>(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>1</mn></msub><mo>)</mo><mo separator="true">,</mo><mo>(</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>y</mi><mn>2</mn></msub><mo>)</mo><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mo>(</mo><msub><mi>x</mi><mi>N</mi></msub><mo separator="true">,</mo><msub><mi>y</mi><mi>N</mi></msub><mo>)</mo><mo>}</mo></mrow><annotation encoding="application/x-tex">T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">T</span><span class="mrel">=</span><span class="mopen">{</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mord mathrm">.</span><span class="mpunct">,</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">}</span></span></span></span></span></p><p>输入到输出的映射关系由模型进行表示，所有可能的由输入空间（特征空间）到输出空间的映射的集合组成了假设空间，学习的范围局限在假设空间中。</p><h1 id="统计学习的三要素"><a class="markdownIt-Anchor" href="#统计学习的三要素"></a> 统计学习的三要素</h1><h2 id="模型"><a class="markdownIt-Anchor" href="#模型"></a> 模型</h2><p>统计学习首先考虑是学习什么样的模型，在监督学习中就是要学习的条件概率分布或者决策函数，由所有可能的模型构成的集合组成了假设空间F,通常是一个由参数决定的函数族.<br>决策函数模型的集合表示为$$F={f|Y=f_\theta(X),\theta\in R^n}$$<br>条件概率模型的集合表示为$$F={f|P=P_\theta(Y|X),\theta\in R^n}$$<br>其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.69444em;vertical-align:0"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.02778em">θ</span></span></span></span>取值于n维的欧氏空间，称为参数空间</p><h2 id="策略"><a class="markdownIt-Anchor" href="#策略"></a> 策略</h2><p>有了假设空间，接着考虑按照什么样的准则学习最优的模型，称为策略。</p><h3 id="损失函数和风险函数"><a class="markdownIt-Anchor" href="#损失函数和风险函数"></a> 损失函数和风险函数</h3><p>损失函数度量模型预测的好坏（预测的结果和标记的差距），风险函数度量平均意义下的模型预测的好坏（在具体某次预测可能出错的期望）</p><h4 id="损失函数"><a class="markdownIt-Anchor" href="#损失函数"></a> 损失函数</h4><p>记作L(f(X),Y)&gt;=0,常见类型:</p><ul><li>0-1损失函数（0-1 loss fuction ）：</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>(</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo separator="true">,</mo><mi>Y</mi><mo>)</mo><mo>=</mo><mrow><mo fence="true">{</mo><mtable><mtr><mtd><mrow><mn>1</mn><mo separator="true">,</mo></mrow></mtd><mtd><mrow><mi>Y</mi><mo>≠</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow></mtd></mtr><mtr><mtd><mrow><mn>0</mn><mo separator="true">,</mo></mrow></mtd><mtd><mrow><mi>Y</mi><mo>=</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">L(f(X),Y) = \begin{cases} 1, &amp; Y \neq f(X)\\ 0, &amp; Y = f(X) \end{cases}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.75em"></span><span class="strut bottom" style="height:3.0000299999999998em;vertical-align:-1.25003em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mclose">)</span><span class="mrel">=</span><span class="minner displaystyle textstyle uncramped"><span class="style-wrap reset-textstyle textstyle uncramped" style="top:0"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist"><span style="top:-.6819999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">1</span><span class="mpunct">,</span></span></span><span style="top:.7579999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathrm">0</span><span class="mpunct">,</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="arraycolsep" style="width:1em"></span><span class="col-align-l"><span class="vlist"><span style="top:-.6819999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mrel">≠</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span></span></span><span style="top:.7579999999999999em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mrel">=</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><ul><li>平方损失函数(quadratic loss function):</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>(</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo separator="true">,</mo><mi>Y</mi><mo>)</mo><mo>=</mo><mo>(</mo><mi>Y</mi><mo>−</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo><msup><mo>)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L(f(X),Y) =(Y-f(X))^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.8641079999999999em"></span><span class="strut bottom" style="height:1.1141079999999999em;vertical-align:-.25em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mclose">)</span><span class="mrel">=</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mclose"><span class="mclose">)</span><span class="vlist"><span style="top:-.413em;margin-right:.05em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span></span></p><ul><li>绝对损失函数(absolute loss fuction):</li></ul><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>(</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo separator="true">,</mo><mi>Y</mi><mo>)</mo><mo>=</mo><mi mathvariant="normal">∣</mi><mo>(</mo><mi>Y</mi><mo>−</mo><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo><mo>)</mo><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">L(f(X),Y) =|(Y-f(X))|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.75em"></span><span class="strut bottom" style="height:1em;vertical-align:-.25em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathrm">∣</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mbin">−</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mclose">)</span><span class="mord mathrm">∣</span></span></span></span></span></p><h3 id="经验风险和结构风险"><a class="markdownIt-Anchor" href="#经验风险和结构风险"></a> 经验风险和结构风险</h3><p>模型f(x)关于训练数据集的平均损失称为经验风险：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mrow><mi>e</mi><mi>m</mi><mi>p</mi></mrow></msub><mo>(</mo><mi>f</mi><mo>)</mo><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mi>L</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">R_{emp}(f) =\frac 1N\sum_{i=1}^NL(y_i,f(x_i))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.8283360000000002em"></span><span class="strut bottom" style="height:3.106005em;vertical-align:-1.277669em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.00773em">R</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.00773em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">e</span><span class="mord mathit">m</span><span class="mord mathit">p</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-.000005000000000143778em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span style="top:-1.2500050000000003em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p><p>结构风险简单而言就是经验风险加入了正则化项，正则化项用于表示模型复杂度，因此结构风险最小化是为了防止过拟合而提出的策略。</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mrow><mi>s</mi><mi>r</mi><mi>m</mi></mrow></msub><mo>(</mo><mi>f</mi><mo>)</mo><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mi>L</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><mo>)</mo><mo>+</mo><mi>λ</mi><mi>J</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">R_{srm}(f) =\frac 1N\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.8283360000000002em"></span><span class="strut bottom" style="height:3.106005em;vertical-align:-1.277669em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.00773em">R</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.00773em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">s</span><span class="mord mathit" style="margin-right:.02778em">r</span><span class="mord mathit">m</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-.000005000000000143778em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span style="top:-1.2500050000000003em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span><span class="mbin">+</span><span class="mord mathit">λ</span><span class="mord mathit" style="margin-right:.09618em">J</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span></span></p><p>其中$\lambda $&gt;=0,用以权衡经验风险和模型复杂度</p><h2 id="算法"><a class="markdownIt-Anchor" href="#算法"></a> 算法</h2><p>算法是指学习模型的具体方法，一般而言就是求解模型f中的参数以达到模型的最优化，故很多时候统计学习的算法到最后基本都是最优化问题。</p><h1 id="模型评估和模型选择"><a class="markdownIt-Anchor" href="#模型评估和模型选择"></a> 模型评估和模型选择</h1><h2 id="训练误差"><a class="markdownIt-Anchor" href="#训练误差"></a> 训练误差</h2><p>训练误差是模型关于训练数据集的平均损失，即前面所说的经验风险；</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>R</mi><mrow><mi>e</mi><mi>x</mi><mi>p</mi></mrow></msub><mo>(</mo><mi>f</mi><mo>)</mo><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mi>L</mi><mo>(</mo><msub><mi>y</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>f</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">R_{exp}(f) =\frac 1N\sum_{i=1}^NL(y_i,f(x_i))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.8283360000000002em"></span><span class="strut bottom" style="height:3.106005em;vertical-align:-1.277669em"></span><span class="base displaystyle textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:.00773em">R</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.00773em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">e</span><span class="mord mathit">x</span><span class="mord mathit">p</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mop op-limits"><span class="vlist"><span style="top:1.1776689999999999em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">i</span><span class="mrel">=</span><span class="mord mathrm">1</span></span></span></span><span style="top:-.000005000000000143778em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span><span class="op-symbol large-op mop">∑</span></span></span><span style="top:-1.2500050000000003em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit" style="margin-right:.10903em">N</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mord mathit">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathit" style="margin-right:.03588em">y</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:-.03588em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathit">x</span><span class="vlist"><span style="top:.15em;margin-right:.05em;margin-left:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></span></p><h2 id="测试误差"><a class="markdownIt-Anchor" href="#测试误差"></a> 测试误差</h2><p>测试误差跟训练误差相似，只是前者是在测试数据集上的平均损失</p><h2 id="过拟合和模型选择"><a class="markdownIt-Anchor" href="#过拟合和模型选择"></a> 过拟合和模型选择</h2><p>一味最求模型对于训练数据的预测能力，所求得的模型往往会比真实模型更加复杂，这种现象叫做过拟合（模型参数过多，训练误差很小，但测试误差较大或者泛化能力很差），而模型的选择就是为了避免过拟合并提高模型对于未知数据的预测能力。</p><h2 id="正则化"><a class="markdownIt-Anchor" href="#正则化"></a> 正则化</h2><p>模型选择的典型方法是正则化，通过结构风险最小化实现，即在经验风险后加上一个正则化项，一般是模型复杂度的单调递增函数（比如L_2范数）<br>正则化的作用是选择经验风险和模型复杂度同时较小的模型；问题是为什么需要简单的模型？一方面是过拟合的存在使得模型有可能过度关注训练数据集的“个性”，导致模型的泛化能力下降；另一个解释是根据奥卡姆剃刀原理，“如无必要，勿增实体”，即在所有可能选择的模型中，能够很好地解释数据并且十分简单的模型才是最优的（尽管这个要求有点自相矛盾，此时<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.69444em;vertical-align:0"></span><span class="base textstyle uncramped"><span class="mord mathit">λ</span></span></span></span>往往起到折中的作用）；从贝叶斯估计的角度而言，正则化项对应于模型的先验概率，复杂的模型先验概率小，简单的模型先验概率大。</p><h2 id="交叉验证"><a class="markdownIt-Anchor" href="#交叉验证"></a> 交叉验证</h2><p>基本思想是重复地使用数据，对给定的数据进行划分，然后组合成训练集和测试集，反复进行训练、测试和模型的选择。</p><h3 id="简单交叉验证"><a class="markdownIt-Anchor" href="#简单交叉验证"></a> 简单交叉验证</h3><p>随机把数据集划分为训练集和测试集两部分（训练集更大），改变不同的条件使得在相同的训练集上也能得到不同的模型，然后在测试集进行模型的测试和选择。</p><h3 id="s折交叉验证"><a class="markdownIt-Anchor" href="#s折交叉验证"></a> S折交叉验证</h3><p>随机地把数据集划分为S个互不相交的子数据集，利用S-1个子数据集作为训练数据集，剩下的一个做为测试集；将这一过程对可能的S中选择重复进行，从得到的S个模型中选择测试误差最小的模型。</p><h3 id="留一交叉验证"><a class="markdownIt-Anchor" href="#留一交叉验证"></a> 留一交叉验证</h3><p>S=N时，用于数据缺乏的情况下，即每次只拿一个数据样本作为测试集，进行N次相同操作，从N个模型中选择最优者（平均测试误差最小）。</p><h1 id="泛化能力"><a class="markdownIt-Anchor" href="#泛化能力"></a> 泛化能力</h1><p>generalization alibity指由学习方法得到的模型对于未知数据的预测能力，常用测试误差评价学习方法的泛化能力，仅记测试数据集是十分宝贵的，在学习模型的过程中不能使用，要在训练完成后才能用于测试，保证对于学得的模型而言，测试数据是“未知的”。对于泛化能力的分析往往通过研究泛化误差的概率上界进行，常具有以下性质：样本容量增加，泛化上界趋于0；假设空间容量越大，模型就越难学，泛化误差上界就越大。<br>存在定理，对于二类分类问题，当假设空间是有限个函数的集合时F={f1,f2,f3…,fd},对任意一个函数f<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>∈</mo></mrow><annotation encoding="application/x-tex">\in</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.5391em"></span><span class="strut bottom" style="height:.5782em;vertical-align:-.0391em"></span><span class="base textstyle uncramped"><span class="mrel">∈</span></span></span></span>F,至少以概率1-<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.69444em"></span><span class="strut bottom" style="height:.69444em;vertical-align:0"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:.03785em">δ</span></span></span></span>,以下不等式成立：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>R</mi><mo>(</mo><mi>f</mi><mo>)</mo><mo>&lt;</mo><mo>=</mo><mover accent="true"><mi>R</mi><mo>^</mo></mover><mo>(</mo><mi>f</mi><mo>)</mo><mo>+</mo><mi>ϵ</mi><mo>(</mo><mi>d</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>δ</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">R(f)&lt;=\hat R(f)+\epsilon (d,N,\delta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.9467699999999999em"></span><span class="strut bottom" style="height:1.19677em;vertical-align:-.25em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.00773em">R</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mclose">)</span><span class="mrel">&lt;</span><span class="mrel">=</span><span class="mord accent"><span class="vlist"><span style="top:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="mord mathit" style="margin-right:.00773em">R</span></span><span style="top:-.25233em;margin-left:.16668em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="accent-body"><span>^</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.10764em">f</span><span class="mclose">)</span><span class="mbin">+</span><span class="mord mathit">ϵ</span><span class="mopen">(</span><span class="mord mathit">d</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.10903em">N</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.03785em">δ</span><span class="mclose">)</span></span></span></span></span></p><p>即泛化误差&lt;=训练误差+N的单调递减函数(右端即为泛化误差的上界)</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>ϵ</mi><mo>(</mo><mi>d</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>δ</mi><mo>)</mo><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><mrow><mn>2</mn><mi>N</mi></mrow></mfrac><mo>(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mi>d</mi><mo>+</mo><mi>l</mi><mi>o</mi><mi>g</mi><mfrac><mn>1</mn><mi>δ</mi></mfrac><mo>)</mo></mrow></msqrt></mrow><annotation encoding="application/x-tex">\epsilon (d,N,\delta)=\sqrt {\frac 1{2N}(logd+log\frac 1\delta)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.65161em"></span><span class="strut bottom" style="height:2.44003em;vertical-align:-.78842em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">ϵ</span><span class="mopen">(</span><span class="mord mathit">d</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.10903em">N</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.03785em">δ</span><span class="mclose">)</span><span class="mrel">=</span><span class="sqrt mord"><span class="sqrt-sign" style="top:-.16161000000000003em"><span class="style-wrap reset-textstyle textstyle uncramped"><span class="delimsizing size3">√</span></span></span><span class="vlist"><span style="top:0"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em">​</span></span><span class="mord displaystyle textstyle cramped"><span class="mord reset-textstyle displaystyle textstyle cramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathrm">2</span><span class="mord mathit" style="margin-right:.10903em">N</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord mathit">d</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:.01968em">l</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:.03588em">g</span><span class="mord reset-textstyle displaystyle textstyle cramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord mathit" style="margin-right:.03785em">δ</span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mclose">)</span></span></span><span style="top:-1.57161em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em">​</span></span>​</span></span></span></span></span></span></span></p><p>可见泛化误差上界和训练误差、样本容量均正相关，和模型复杂度d负相关。</p><h1 id="生成模型和判别模型"><a class="markdownIt-Anchor" href="#生成模型和判别模型"></a> 生成模型和判别模型</h1><h2 id="生成模型"><a class="markdownIt-Anchor" href="#生成模型"></a> 生成模型</h2><p>由生成方法学到的模型称为生成模型，生成方法由数据学习联合概率分布P(X,Y),然后求出条件概率分布P(Y|X)作为预测的模型，即：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(Y|X)=\frac {P(X,Y)}{P(X)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.427em"></span><span class="strut bottom" style="height:2.363em;vertical-align:-.936em"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mord mathrm">∣</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.686em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="mord mathit" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mclose">)</span></span></span></span><span style="top:-.2300000000000001em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.677em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord mathit" style="margin-right:.13889em">P</span><span class="mopen">(</span><span class="mord mathit" style="margin-right:.07847em">X</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:.22222em">Y</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span></span></p><p>生成模型表示了给定输入X产生输出Y的生成关系，典型的生成模型由朴素贝叶斯法和隐马尔可夫模型。</p><h2 id="判别模型"><a class="markdownIt-Anchor" href="#判别模型"></a> 判别模型</h2><p>由判别方法学到的模型称为判别模型，判别方法由数据直接学习决策函数f(X)或者条件概率P(Y|X)作为预测的模型。判别模型表示给定输入X应该预测什么样的输出Y。典型的判别模型有k紧邻法，感知机，决策树，逻辑斯谛回归模型，最大熵模型，支持向量机，提升方法，条件随机场等。</p><p>存在隐变量时不能使用判别方法，但可以用生成方法；判别方法直接面对预测，准确率更高。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;李航老师的《统计学习方法》基本已经过一遍了，感觉只是略懂皮毛，现为了加强知识点的认识和部分课后题的实现，有必要进行个人总结。&lt;/p&gt;&lt;h1 id
      
    
    </summary>
    
      <category term="statistical learning method" scheme="http://aier02.com/categories/statistical-learning-method/"/>
    
    
      <category term="basic knowledge" scheme="http://aier02.com/tags/basic-knowledge/"/>
    
      <category term="introduction" scheme="http://aier02.com/tags/introduction/"/>
    
  </entry>
  
  <entry>
    <title>Build personal blog</title>
    <link href="http://aier02.com/2018/08/07/blog_tutorial/"/>
    <id>http://aier02.com/2018/08/07/blog_tutorial/</id>
    <published>2018-08-07T11:24:35.583Z</published>
    <updated>2018-10-03T08:30:19.069Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --><p>最近在加强ml和cv基础知识的学习，为了加深理解，同时记录自己的学习过程，尝试着写blog；向好友请教后得知建立个人blog的方式，我的选择是Hexo(一种静态博客网页框架)+Github page（免费托管博客项目代码），当然还有租云服务器和自己写后台和前端的方式，前者更加方便和易于上手。</p><h2 id="前期准备"><a class="markdownIt-Anchor" href="#前期准备"></a> 前期准备</h2><h3 id="安装git"><a class="markdownIt-Anchor" href="#安装git"></a> 安装Git</h3><p>毕竟所有项目代码都是托管到Github上面，必须保证系统中已经安装了git，terminal中输入git不出现commant not found即可</p><h3 id="安装nodejs"><a class="markdownIt-Anchor" href="#安装nodejs"></a> 安装Node.js</h3><p>因为Hexo的使用基于Node.js，所以要先安装Node.js和他的包安装器npm,建议到<a href="https://nodejs.org/" target="_blank" rel="noopener">官网</a>下载pkg并选择稳定版本(LTS)</p><h3 id="github创建项目"><a class="markdownIt-Anchor" href="#github创建项目"></a> Github创建项目</h3><p>在Github上面创建一个名为：<a href="http://yourname.github.io" target="_blank" rel="noopener">yourname.github.io</a> 的空项目，yourname是指你创建的github账户名（github上面每个用户的用户名是唯一的标识），该Repository就是之后你的blog所有代码和文件存放的地方，blog的地址在不添加设置的情况下：<a href="https://yourname.github.io" target="_blank" rel="noopener">https://yourname.github.io</a></p><h3 id="安装hexo"><a class="markdownIt-Anchor" href="#安装hexo"></a> 安装Hexo</h3><p>在终端输入npm命令下载静态网页生成器Hexo</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo</span><br></pre></td></tr></table></figure><p>若出现permiss denied等情况，则加入sudo</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo npm install -g hexo</span><br></pre></td></tr></table></figure><h3 id="创建hexo文件夹"><a class="markdownIt-Anchor" href="#创建hexo文件夹"></a> 创建Hexo文件夹</h3><p>在本地文件系统中创建Hexo文件夹，cd进入该文件夹，然后进行初始化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Hexo init</span><br></pre></td></tr></table></figure><p>没有错误，则以后所有操作都必须在该文件目录下进行，特别注意文件夹<code>Hexo/node_models</code>,所有通过npm install XXX --save操作下载的依赖都会存放在该文件夹中，易于包的管理(–save参数的意义就是存放在node_models)</p><p>顺利完成上述步骤后可以登陆https://yourname.github.io 查看效果</p><h2 id="绑定个人域名"><a class="markdownIt-Anchor" href="#绑定个人域名"></a> 绑定个人域名</h2><p>Ps:不想花钱的同学可以跳过</p><h3 id="购买国外个人域名"><a class="markdownIt-Anchor" href="#购买国外个人域名"></a> 购买国外个人域名</h3><p>对于没有个人域名的同学来说，国内备案实在是太漫长了(我前段时间就在腾讯云备案过一次，周期太长==)，直接购买国外的域名方便而且预算基本一样，推荐<a href="https://www.namesilo.com" target="_blank" rel="noopener">namesilo</a>，60一年，后缀多选择而且有免费的private protection；注册的时候我写的假的信息(除了邮箱）；一开始先检索你想注册的域名，然后在根据英文指导取购买就ok了(默认设置),网站支持alipay，非常方便了。</p><h3 id="更改dns"><a class="markdownIt-Anchor" href="#更改dns"></a> 更改DNS</h3><p>统一使用一个dns的话对于以后多域名管会更加方便，推荐使用国内的厂商<a href="https://www.dnspod.cn" target="_blank" rel="noopener">dnspod</a>，登陆后添加你更注册的域名进行管理，主要添加连个主机记录，分别是@和www,譬如我的设置为</p><p><img src="/images/180807/dnspodset.png" alt=""></p><p>图中A记录的记录值为github page中项目的ip，可以通过在终端中输入ping <a href="https://yourname.github.io" target="_blank" rel="noopener">https://yourname.github.io</a> 找到，也可在Github help中找到。</p><p>登陆<a href="https://www.namesilo.com" target="_blank" rel="noopener">namesilo</a>，在右侧找到domain manager,进入后选择更改nameserver,处理过程还是挺长的，所以不用急着登陆你的域名</p><p><img src="/images/180807/domain.png" alt=""></p><h2 id="更改blog主题"><a class="markdownIt-Anchor" href="#更改blog主题"></a> 更改blog主题</h2><p>在 Hexo 中有两份主要的配置文件，其名称都是 _config.yml。 其中，一份位于站点根目录下，主要包含 Hexo 本身的配置；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项。为了描述方便，在以下说明中，将前者称为站点配置文件， 后者称为主题配置文件</p><h3 id="选择主题"><a class="markdownIt-Anchor" href="#选择主题"></a> 选择主题</h3><p>进入官网选择适合的主题[hexo]<br>(<a href="https://hexo.io/themes/" target="_blank" rel="noopener">https://hexo.io/themes/</a>)，进入相应的Github地址，git clone命令把整个文件夹都下载下来，存放在<code>Hexo/themes</code>文件夹中。</p><h3 id="更改配置文件"><a class="markdownIt-Anchor" href="#更改配置文件"></a> 更改配置文件</h3><p>一般而言，下载下来的主题文件中都有新手引导，注意readme就ok了。</p><h2 id="部署项目代码"><a class="markdownIt-Anchor" href="#部署项目代码"></a> 部署项目代码</h2><p>在<code>Hexo/_config.yml</code>和themes里面的config文件中添加以下说明</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repo: https://github.com/aier02/aier02.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure><p>注意修改repo的地址为你的github上面的地址(只要修改你的用户名)</p><p>在Hexo目录下输入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</span><br></pre></td></tr></table></figure><p>即可将本地下的<code>Hexo/public</code>文件夹所有内容上传到github项目中(可能需要生成公私钥，请自行百度）</p><p>hexo clean实质是删除<code>Hexo/public</code>，而hexo g则是根据配置和<code>Hexo/source</code>生成<code>Hexo/public</code>,新写的blog存放在<code>Hexo/source/_posts</code>即可，hexo d命令则是将<code>Hexo/public</code>上传到git服务器中,使用个人域名的同学还得在Github上面的yourname.github.io仓库根目录下新建一个CNAME文件，写入你的个人域名,不需要http和www,比如我的就是在第一行为：<a href="http://aier02.com">aier02.com</a></p><h2 id="个人寄语"><a class="markdownIt-Anchor" href="#个人寄语"></a> 个人寄语</h2><p>blog关键在于内容，所以我基本很多操作都是为了方便，倘若遇到了什么问题可在下方评论提出(主要是看我是否也遇到了可以提供解决方案，没有踩过的坑我也不会=。=）,还有可以选择的theme很多，奉劝各位以实用为主，再好看的blog没有内容也是没价值的。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- build time:Wed Oct 10 2018 21:02:42 GMT+0800 (GMT+08:00) --&gt;&lt;p&gt;最近在加强ml和cv基础知识的学习，为了加深理解，同时记录自己的学习过程，尝试着写blog；向好友请教后得知建立个人blog的方式，我的选择是H
      
    
    </summary>
    
      <category term="blog" scheme="http://aier02.com/categories/blog/"/>
    
    
      <category term="Hexo" scheme="http://aier02.com/tags/Hexo/"/>
    
      <category term="Github page" scheme="http://aier02.com/tags/Github-page/"/>
    
      <category term="namesilo" scheme="http://aier02.com/tags/namesilo/"/>
    
  </entry>
  
</feed>
