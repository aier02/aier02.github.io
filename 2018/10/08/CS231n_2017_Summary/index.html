<!-- build time:Sun Jan 09 2022 21:31:56 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta name="referrer" content="never"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>CS231n-2017-Summary | Amadeus</title><meta name="description" content="Something new to me when I read such a good notebook about CS231n-2017-Summary CNNs常用same策略同时保存图像边缘信息Padding strategy:in order to maintain our full size of the input. If we didn’t do padding zero the"><meta name="keywords" content="notebook"><meta property="og:type" content="article"><meta property="og:title" content="CS231n-2017-Summary"><meta property="og:url" content="http://aier02.com/2018/10/08/CS231n_2017_Summary/index.html"><meta property="og:site_name" content="Aier02"><meta property="og:description" content="Something new to me when I read such a good notebook about CS231n-2017-Summary CNNs常用same策略同时保存图像边缘信息Padding strategy:in order to maintain our full size of the input. If we didn’t do padding zero the"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://aier02.com/images/181008/activation.png"><meta property="og:image" content="http://aier02.com/images/181008/43.png"><meta property="og:image" content="http://aier02.com/images/181008/02.jpg"><meta property="og:image" content="http://aier02.com/images/181008/03.png"><meta property="og:image" content="http://aier02.com/images/181008/05.png"><meta property="og:image" content="http://aier02.com/images/181008/43.png"><meta property="og:image" content="http://aier02.com/images/181008/45.png"><meta property="og:image" content="http://aier02.com/images/181008/07.jpg"><meta property="og:image" content="http://aier02.com/images/181008/08.png"><meta property="og:image" content="http://aier02.com/images/181008/46.jpg"><meta property="og:image" content="http://aier02.com/images/181008/47.jpg"><meta property="og:image" content="http://aier02.com/images/181008/10.jpg"><meta property="og:updated_time" content="2018-12-17T02:48:08.053Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="CS231n-2017-Summary"><meta name="twitter:description" content="Something new to me when I read such a good notebook about CS231n-2017-Summary CNNs常用same策略同时保存图像边缘信息Padding strategy:in order to maintain our full size of the input. If we didn’t do padding zero the"><meta name="twitter:image" content="http://aier02.com/images/181008/activation.png"><link rel="canonical" href="http://aier02.com/2018/10/08/CS231n_2017_Summary/index.html"><link rel="alternate" href="/atom.xml" title="Aier02" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css" rel="stylesheet"></head><body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://github.com/aier02" target="_blank"><img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">Mileeet</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">software engineer</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Guangdong, China</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="搜索"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav"><li class="menu-item menu-item-home"><a href="/."><i class="icon icon-home-fill"></i> <span class="menu-title">首页</span></a></li><li class="menu-item menu-item-archives"><a href="/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">归档</span></a></li><li class="menu-item menu-item-categories"><a href="/categories"><i class="icon icon-folder"></i> <span class="menu-title">分类</span></a></li><li class="menu-item menu-item-tags"><a href="/tags"><i class="icon icon-tags"></i> <span class="menu-title">标签</span></a></li><li class="menu-item menu-item-repository"><a href="/repository"><i class="icon icon-project"></i> <span class="menu-title">项目</span></a></li><li class="menu-item menu-item-books"><a href="/books"><i class="icon icon-book-fill"></i> <span class="menu-title">书单</span></a></li><li class="menu-item menu-item-links"><a href="/links"><i class="icon icon-friendship"></i> <span class="menu-title">友链</span></a></li><li class="menu-item menu-item-about"><a href="/about"><i class="icon icon-cup-fill"></i> <span class="menu-title">关于</span></a></li></ul><ul class="social-links"><li><a href="https://github.com/aier02" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">公告</h3><div class="widget-body"><div id="board"><div class="content"><p>欢迎交流与分享经验!</p></div></div></div></div><div class="widget"><h3 class="widget-title">分类</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/algorithms/">algorithms</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/blog/">blog</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs229n/">cs229n</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs231n/">cs231n</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cv/">cv</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/dl/">dl</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/">leetcode</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/personal-summary/">personal summary</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/">pytorch</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/statistical-learning-method/">statistical learning method</a><span class="category-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签</h3><div class="widget-body"><ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ChexNet/">ChexNet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EDA/">EDA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Github-page/">Github page</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ResNet/">ResNet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/array/">array</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/backpropagation/">backpropagation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/backtracking/">backtracking</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basic-knowledge/">basic knowledge</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/blog/">blog</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/chain-rule/">chain rule</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/chest-X-ray/">chest X-ray</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cv/">cv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/demo/">demo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/experience/">experience</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/gradient/">gradient</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hash-table/">hash table</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/image-classification/">image classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/introduction/">introduction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jikecloud/">jikecloud</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/keras/">keras</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/">leetcode</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linear-classification/">linear classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linked-list/">linked list</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/namesilo/">namesilo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/notebook/">notebook</a><span class="tag-list-count">18</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/paper-reading/">paper reading</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/plan/">plan</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch/">pytorch</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch-cookbook/">pytorch cookbook</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/regularization/">regularization</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/segmentation/">segmentation</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sensetime/">sensetime</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/stack/">stack</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/string/">string</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/two-pointers/">two pointers</a><span class="tag-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签云</h3><div class="widget-body tagcloud"><a href="/tags/CNN/" style="font-size:13px">CNN</a> <a href="/tags/ChexNet/" style="font-size:13px">ChexNet</a> <a href="/tags/EDA/" style="font-size:13px">EDA</a> <a href="/tags/Github-page/" style="font-size:13px">Github page</a> <a href="/tags/Hexo/" style="font-size:13px">Hexo</a> <a href="/tags/ResNet/" style="font-size:13px">ResNet</a> <a href="/tags/SVM/" style="font-size:13px">SVM</a> <a href="/tags/array/" style="font-size:13px">array</a> <a href="/tags/backpropagation/" style="font-size:13px">backpropagation</a> <a href="/tags/backtracking/" style="font-size:13.25px">backtracking</a> <a href="/tags/basic-knowledge/" style="font-size:13.75px">basic knowledge</a> <a href="/tags/blog/" style="font-size:13.5px">blog</a> <a href="/tags/chain-rule/" style="font-size:13px">chain rule</a> <a href="/tags/chest-X-ray/" style="font-size:13px">chest X-ray</a> <a href="/tags/cv/" style="font-size:13.25px">cv</a> <a href="/tags/demo/" style="font-size:13px">demo</a> <a href="/tags/experience/" style="font-size:13px">experience</a> <a href="/tags/gradient/" style="font-size:13px">gradient</a> <a href="/tags/hash-table/" style="font-size:13px">hash table</a> <a href="/tags/image-classification/" style="font-size:13px">image classification</a> <a href="/tags/introduction/" style="font-size:13px">introduction</a> <a href="/tags/jikecloud/" style="font-size:13px">jikecloud</a> <a href="/tags/keras/" style="font-size:13px">keras</a> <a href="/tags/leetcode/" style="font-size:13.25px">leetcode</a> <a href="/tags/linear-classification/" style="font-size:13px">linear classification</a> <a href="/tags/linked-list/" style="font-size:13.25px">linked list</a> <a href="/tags/namesilo/" style="font-size:13px">namesilo</a> <a href="/tags/notebook/" style="font-size:14px">notebook</a> <a href="/tags/paper-reading/" style="font-size:13.25px">paper reading</a> <a href="/tags/plan/" style="font-size:13px">plan</a> <a href="/tags/pytorch/" style="font-size:13px">pytorch</a> <a href="/tags/pytorch-cookbook/" style="font-size:13px">pytorch cookbook</a> <a href="/tags/regularization/" style="font-size:13.25px">regularization</a> <a href="/tags/segmentation/" style="font-size:13.5px">segmentation</a> <a href="/tags/sensetime/" style="font-size:13.25px">sensetime</a> <a href="/tags/stack/" style="font-size:13px">stack</a> <a href="/tags/string/" style="font-size:13px">string</a> <a href="/tags/two-pointers/" style="font-size:13px">two pointers</a></div></div><div class="widget"><h3 class="widget-title">归档</h3><div class="widget-body"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">九月 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">八月 2018</a><span class="archive-list-count">2</span></li></ul></div></div><div class="widget"><h3 class="widget-title">最新文章</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/cv/">cv</a></p><p class="item-title"><a href="/2019/01/15/inception/" class="title">inception</a></p><p class="item-date"><time datetime="2019-01-15T08:23:32.427Z" itemprop="datePublished">2019-01-15</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/cv/">cv</a></p><p class="item-title"><a href="/2019/01/12/BoW_in_cv/" class="title">BoW in cv</a></p><p class="item-date"><time datetime="2019-01-12T02:59:03.404Z" itemprop="datePublished">2019-01-12</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/dl/">dl</a></p><p class="item-title"><a href="/2019/01/11/gradient-vanishing-&-exploding-problem/" class="title">gradient vanishing &amp; exploding problem</a></p><p class="item-date"><time datetime="2019-01-11T07:33:27.121Z" itemprop="datePublished">2019-01-11</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/cv/">cv</a></p><p class="item-title"><a href="/2019/01/11/LSTMs/" class="title">LSTMs</a></p><p class="item-date"><time datetime="2019-01-11T07:33:13.044Z" itemprop="datePublished">2019-01-11</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/cv/">cv</a></p><p class="item-title"><a href="/2019/01/10/epoch & iteration/" class="title">epoch &amp; iteration</a></p><p class="item-date"><time datetime="2019-01-10T11:15:52.935Z" itemprop="datePublished">2019-01-10</time></p></div></li></ul></div></div></div></aside><aside class="sidebar sidebar-toc collapse" id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><nav id="toc" class="article-toc"><h3 class="toc-title">文章目录</h3><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#cnns"><span class="toc-number">1.</span> <span class="toc-text">CNNs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#training-neural-networks-i"><span class="toc-number">2.</span> <span class="toc-text">Training neural networks I</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#training-neural-networks-ii"><span class="toc-number">3.</span> <span class="toc-text">Training neural networks II</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#deep-learning-software"><span class="toc-number">4.</span> <span class="toc-text">Deep learning software</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cnn-architectures"><span class="toc-number">5.</span> <span class="toc-text">CNN architectures</span></a></li></ol><li class="toc-item toc-level-2"><a class="toc-link" href="#recurrent-neural-networks"><span class="toc-number"></span> <span class="toc-text">Recurrent Neural networks</span></a></li></nav></div></aside><main class="main" role="main"><div class="content"><article id="post-CS231n_2017_Summary" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">CS231n-2017-Summary</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/2018/10/08/CS231n_2017_Summary/" class="article-date"><time datetime="2018-10-08T14:46:19.006Z" itemprop="datePublished">2018-10-08</time></a></span> <span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="/categories/cs231n/">cs231n</a></span> <span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link" href="/tags/notebook/">notebook</a></span> <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2018/10/08/CS231n_2017_Summary/#comments" class="article-comment-link">评论</a></span></div></div><div class="article-entry markdown-body" itemprop="articleBody"><p>Something new to me when I read such a good notebook about <a href="https://github.com/mbadry1/CS231n-2017-Summary" target="_blank" rel="noopener">CS231n-2017-Summary</a></p><h3 id="cnns"><a class="markdownIt-Anchor" href="#cnns"></a> CNNs</h3><p>常用same策略同时保存图像边缘信息</p><ul><li>Padding strategy:in order to maintain our full size of the input. If we didn’t do padding zero the input will be shrinking too fast and we will lose a lot of data.Give a stride of <code>1</code> its common to pad to this equation: <code>(F-1)/2</code> where F is the filter size, zero padding from both sides.If we pad this way we call this same convolution.<ul><li>If we have input of shape (32,32,3) and ten filters with shape is (5,5) with stride 1 and pad 2;Output size will be (32,32,10) # We maintain the size.</li><li>Size of parameters per filter = 5x5x3 + 1 = 76(+1 for bias)</li><li>All parameters 76x10=760</li></ul></li><li>So here are the parameters for the Conv layer:<ul><li>Number of filters K.<ul><li>Usually a power of 2.</li></ul></li><li>Spatial content size F.<ul><li>3,5,7 …</li></ul></li><li>The stride S.<ul><li>Usually 1 or 2 (If the stride is big there will be a downsampling but different of pooling)</li></ul></li><li>Amount of Padding<ul><li>If we want the input shape to be as the output shape, based on the F if 3 its 1, if F is 5 the 2 and so on</li></ul></li></ul></li></ul><p>一般而言pooling层是不可(用)学习的</p><ul><li>Pooling makes the representation smaller and more manageable.</li><li>Pooling Operates over each activation map independently.</li><li>Example of pooling is the maxpooling.<ul><li>Parameters of max pooling is the size of the filter and the stride&quot;<ul><li>Example <code>2x2</code> with stride <code>2</code> <code># Usually the two parameters are the same 2 , 2</code></li></ul></li></ul></li><li>Also example of pooling is average pooling.<ul><li>In this case it might be learnable.</li></ul></li></ul><h3 id="training-neural-networks-i"><a class="markdownIt-Anchor" href="#training-neural-networks-i"></a> Training neural networks I</h3><ul><li><p>As a revision here are the Mini batch stochastic gradient descent algorithm steps,小批量的随机梯度下降算法的步骤:</p><ul><li>Loop:<ol><li>Sample a batch of data.</li><li>Forward prop it through the graph (network) and get loss.(define loss)</li><li>Backprop to calculate the gradients.(chain rule)</li><li>Update the parameters using the gradients(learning rate)</li></ol></li></ul></li><li><p>Activation functions,用于引入非线性因素，单纯的线性模型表达能力不足。</p><p><img src="/images/181008/activation.png" alt=""></p><ul><li><p>Sigmoid:</p><ul><li>Squashes the numbers between [0,1]</li><li>Used as a firing rate like human brains.</li><li><code>Sigmoid(x) = 1 / (1 + e^-x)</code></li><li>Problems with sigmoid:<ul><li>big values neurons kill the gradients.</li><li>Gradients are in most cases near 0 (Big values/small values), that kills the updates if the graph/network are large.</li><li>Not Zero-centered.<ul><li>Didn’t produce zero-mean data.</li></ul></li><li>exp() is a bit compute expensive.<ul><li>just to mention. We have a more complex operations in deep learning like convolution.</li></ul></li></ul></li></ul></li><li><p>Tanh:</p><ul><li>Squashes the numbers between [-1,1]</li><li>Zero centered.</li><li>Still big values neurons “kill” the gradients.</li><li><code>Tanh(x)</code> is the equation.<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mi>s</mi><mi>i</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">tanh(x)=\frac {sinh(x)}{cosh(x)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.01em"></span><span class="strut bottom" style="height:1.53em;vertical-align:-.52em"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit">a</span><span class="mord mathit">n</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.34500000000000003em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">s</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.485em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">s</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>,<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>i</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><mi>x</mi></mrow></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><mn>2</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">sinh(x)=\frac {e^{x}-e^{-x}}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.939765em"></span><span class="strut bottom" style="height:1.284765em;vertical-align:-.345em"></span><span class="base textstyle uncramped"><span class="mord mathit">s</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.345em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.394em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord">−</span><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>,<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><mi>x</mi></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><mn>2</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">cosh(x)=\frac {e^{x}+e^{-x}}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.939765em"></span><span class="strut bottom" style="height:1.284765em;vertical-align:-.345em"></span><span class="base textstyle uncramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">s</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.345em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.394em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord">−</span><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>,</li><li>Proposed by Yann Lecun in 1991</li></ul></li><li><p>RELU (Rectified linear unit):</p><ul><li><code>RELU(x) = max(0,x)</code></li><li>Doesn’t kill the gradients.<ul><li>Only small values that are killed. Killed the gradient in the half</li></ul></li><li>Computationally efficient.</li><li>Converges much faster than Sigmoid and Tanh <code>(6x)</code></li><li>More biologically plausible than sigmoid.</li><li>Proposed by Alex Krizhevsky in 2012 Toronto university. (AlexNet)</li><li>Problems:<ul><li>Not zero centered.</li></ul></li><li>If weights aren’t initialized good, maybe 75% of the neurons will be dead and thats a waste computation. But its still works. This is an active area of research to optimize this.</li><li>To solve the issue mentioned above, people might initialize all the biases by 0.01</li></ul></li><li><p>Leaky RELU:</p><ul><li><code>leaky_RELU(x) = max(0.01x,x)</code></li><li>Doesn’t kill the gradients from both sides.</li><li>Computationally efficient.</li><li>Converges much faster than Sigmoid and Tanh (6x)</li><li>Will not die.</li><li>PRELU is placing the 0.01 by a variable alpha which is learned as a parameter.</li></ul></li><li><p>Exponential linear units (ELU):</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ELU(x) = &#123; x                                           if x &gt; 0</span><br><span class="line">		   alpah *(exp(x) -1)		                   if x &lt;= 0</span><br><span class="line">           # alpah are a learning parameter</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>It has all the benefits of RELU</p></li><li><p>Closer to zero mean outputs and adds some robustness to noise.</p></li><li><p>problems</p><ul><li><code>exp()</code> is a bit compute expensive.</li></ul></li></ul></li><li><p>Maxout activations:</p><ul><li><code>maxout(x) = max(w1.T*x + b1, w2.T*x + b2)</code></li><li>Generalizes RELU and Leaky RELU</li><li>Doesn’t die!</li><li>Problems:<ul><li>oubles the number of parameters per neuron</li></ul></li></ul></li><li><p>In practice:</p><ul><li>Use RELU. Be careful for your learning rates.</li><li>Try out Leaky RELU/Maxout/ELU</li><li>Try out tanh but don’t expect much.</li><li>Don’t use sigmoid!</li></ul></li></ul><p><strong>Data preprocessing</strong>:</p><ul><li><p>Normalize the data:减去均值后除以标准差</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Zero centered data. (Calculate the mean for every input).</span></span><br><span class="line"><span class="comment"># On of the reasons we do this is because we need data to be between positive and negative and not all the be negative or positive. </span></span><br><span class="line">X -= np.mean(X, axis = <span class="number">1</span>)</span><br><span class="line"><span class="comment">#np.mean()中的参数axis指定了哪个维度被压缩成1，例如axis=0,则输出的结果为一行，即求得输入x的每一列的平均，压缩成一行，同理axis=1，则输出为一列，该结果中的每一行为按照行进行平均的值。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Then apply the standard deviation. Hint: in images we don't do this.</span></span><br><span class="line">X /= np.std(X, axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p>To normalize images:对图像进行标准化</p><ul><li>Subtract the mean image (E.g. Alexnet)<ul><li>Mean image shape is the same as the input images.</li></ul></li><li>Or Subtract per-channel mean<ul><li>Means calculate the mean for each channel of all images. Shape is 3 (3 channels)</li></ul></li></ul></li><li><p>First idea is to initialize the w’s with small random numbers:</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.rand(D, H)</span><br><span class="line"><span class="comment"># Works OK for small networks but it makes problems with deeper networks!</span></span><br></pre></td></tr></table></figure></li><li><p>The standard deviations is going to zero in deeper networks. and the gradient will vanish sooner in deep networks.使用任意小的数字进行对w初始化，随着网络的加深可能导致梯度消失问题(每一层的输入很小)。</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">1</span> * np.random.rand(D, H) </span><br><span class="line"><span class="comment"># Works OK for small networks but it makes problems with deeper networks!</span></span><br></pre></td></tr></table></figure></li><li><p>The network will explode with big numbers!，使用大于一的初值可能会导致深层网络中梯度爆炸问题。</p></li></ul></li><li><p>Xavier initialization:</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.rand(in, out) / np.sqrt(in)</span><br></pre></td></tr></table></figure></li><li><p>It works because we want the variance of the input to be as the variance of the output.</p></li><li><p>But it has an issue, It breaks when you are using RELU.</p></li></ul></li><li><p>He initialization(Solution for the RELU issue):</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.rand(in, out) / np.sqrt(in/2)</span><br></pre></td></tr></table></figure></li><li><p>Solves the issue with RELU. Its recommended when you are using RELU</p></li></ul></li></ul><p><strong>Batch normalization</strong>:</p><ul><li><p>is a technique to provide any layer in a Neural Network with inputs that are zero mean/unit variance.</p></li><li><p>It speeds up the training. You want to do this a lot.</p><ul><li>Made by Sergey Ioffe and Christian Szegedy at 2015.</li></ul></li><li><p>We make a Gaussian activations in each layer. by calculating the mean and the variance.</p></li><li><p>Usually inserted after (fully connected or Convolutional layers) and (before nonlinearity).</p></li><li><p>Steps (For each output of a layer)</p><ol><li><p>First we compute the mean and variance^2 of the batch for each feature.</p></li><li><p>We normalize by subtracting the mean and dividing by square root of (variance^2 + epsilon)</p><ul><li>epsilon to not divide by zero</li></ul></li><li><p>Then we make a scale and shift variables:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Result = gamma * normalizedX + beta</span><br></pre></td></tr></table></figure><ul><li>gamma and beta are learnable parameters.</li><li>it basically possible to say “Hey!! I don’t want zero mean/unit variance input, give me back the raw input - it’s better for me.”</li><li>Hey shift and scale by what you want not just the mean and variance!</li></ul></li></ol></li></ul><p><strong>Baby sitting the learning process</strong></p><ol><li>Preprocessing of data.</li><li>Choose the architecture.</li><li>Make a forward pass and check the loss (Disable regularization). Check if the loss is reasonable.</li><li>Add regularization, the loss should go up!</li><li>Disable the regularization again and take a small number of data and try to train the loss and reach zero loss.<ul><li>You should overfit perfectly for small datasets.</li></ul></li><li>Take your full training data, and small regularization then try some value of learning rate.<ul><li>If loss is barely changing, then the learning rate is small.</li><li>If you got <code>NAN</code> then your NN exploded and your learning rate is high.</li><li>Get your learning rate range by trying the min value (That can change) and the max value that doesn’t explode the network.</li></ul></li><li>Do Hyperparameters optimization to get the best hyperparameters values.</li></ol><p>Hyperparameter Optimization</p><ul><li>Try Cross validation strategy.<ul><li>Run with a few ephocs, and try to optimize the ranges.</li></ul></li><li>Its best to optimize in log space.</li><li>Adjust your ranges and try again.</li><li>Its better to try random search instead of grid searches (In log space)</li></ul><h3 id="training-neural-networks-ii"><a class="markdownIt-Anchor" href="#training-neural-networks-ii"></a> Training neural networks II</h3><p><strong>Optimization algorithms</strong>:</p><ul><li><p>Problems with stochastic gradient descent:随机梯度下降算法的问题</p><ul><li><p>if loss quickly in one direction and slowly in another (For only two variables), you will get very slow progress along shallow dimension, jitter along steep direction. Our NN will have a lot of parameters then the problem will be more.</p></li><li><p>Local minimum or saddle points；局部最小或者鞍点问题</p><ul><li>何为鞍点？鞍点（Saddle point）在微分方程中，沿着某一方向是稳定的，另一条方向是不稳定的奇点，叫做鞍点。在泛函中，既不是极大值点也不是极小值点的临界点，叫做鞍点。在矩阵中，一个数在所在行中是最大值，在所在列中是最小值，则被称为鞍点。在物理上要广泛一些，指在一个方向是极大值，另一个方向是极小值的点</li></ul><ul><li>If SGD went into local minimum we will stuck at this point because the gradient is zero.遇到极小值点会stuck</li><li>Also in saddle points the gradient will be zero so we will stuck.</li><li>Saddle points says that at some point:鞍点在gradient上的表现<ul><li>Some gradients will get the loss up.</li><li>Some gradients will get the loss down.</li><li>And that happens more in high dimensional (100 million dimension for example)</li></ul></li><li>The problem of deep NN is more about saddle points than about local minimum because deep NN has high dimensions (Parameters)</li><li>Mini batches are noisy because the gradient is not taken for the whole batch.</li></ul></li></ul></li><li><p><strong>SGD + momentum</strong>:引入momentum，解决在鞍点或者极小值点处出现gradient为0而无法及继续更新参数的情况</p><ul><li><p>Build up velocity as a running mean of gradients:</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Computing weighted average. rho best is in range [0.9 - 0.99]</span></span><br><span class="line">V[t+<span class="number">1</span>] = rho * v[t] + dx</span><br><span class="line">x[t+<span class="number">1</span>] = x[t] - learningRate * V[t+<span class="number">1</span>]</span><br></pre></td></tr></table></figure></li><li><p><code>V[0]</code> is zero.</p></li><li><p>Solves the saddle point and local minimum problems.</p></li><li><p>It overshoots the problem and returns to it back.</p></li></ul></li><li><p><strong>Nestrov momentum</strong>:</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dx = compute_gradient(x)</span><br><span class="line">old_v = v</span><br><span class="line">v = rho * v - learning_rate * dx</span><br><span class="line">x+= -rho * old_v + (<span class="number">1</span>+rho) * v</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>AdaGrad</strong></p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(<span class="keyword">True</span>):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># here is a problem, the grad_squared isn't decayed (gets so large)</span></span><br><span class="line">  grad_squared += dx * dx			</span><br><span class="line">  </span><br><span class="line">  x -= (learning_rate*dx) / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>RMSProp</strong></p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(<span class="keyword">True</span>):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#Solved ADAgra</span></span><br><span class="line">  grad_squared = decay_rate * grad_squared + (<span class="number">1</span>-grad_squared) * dx * dx  </span><br><span class="line">  </span><br><span class="line">  x -= (learning_rate*dx) / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure></li><li><p>People uses this instead of AdaGrad</p></li></ul></li><li><p><strong>Adam</strong>,最常用的优化器，结合了momentum和RMSProp</p><ul><li>Calculates the momentum and RMSProp as the gradients.</li><li>It need a Fixing bias to fix starts of gradients.</li><li>Is the best technique so far runs best on a lot of problems.</li><li>With <code>beta1 = 0.9</code> and <code>beta2 = 0.999</code> and <code>learning_rate = 1e-3</code> or <code>5e-4</code> is a great starting point for many models!</li></ul></li><li><p><strong>Learning decay</strong>学习率退火，避免因为网络的不断加深而导致学习率相对参数而言过大</p><ul><li>Ex. decay learning rate by half every few epochs.</li><li>To help the learning rate not to bounce out.</li><li>Learning decay is common with SGD+momentum but not common with Adam.</li><li>Dont use learning decay from the start at choosing your hyperparameters. Try first and check if you need decay or not.</li></ul></li></ul><p><strong>Regularization</strong>:损失函数中引入正则化项；集成学习；drop out修改网络结构；数据增强</p><ul><li>So far we have talked about reducing the training error, but we care about most is how our model will handle unseen data!上述优化更多的是在做如何更新参数使得error减少，但我们更加关心的是模型的泛化能力</li><li>What if the gab of the error between training data and validation data are too large?</li><li>This error is called high variance.</li><li>Model Ensemble:<ul><li>Algorithm:<ul><li>Train multiple independent models of the same architecture with different initializations.</li><li>At test time average their results.</li></ul></li><li>It can get you extra 2% performance.</li><li>It reduces the generalization error.</li><li>You can use some snapshots of your NN at the training ensembles them and take the results.</li></ul></li><li>Regularization solves the high variance problem. We have talked about L1, L2 Regularization.</li><li>L0-norm用于统计向量中非零元素的个数</li><li>Some Regularization techniques are designed for only NN and can do better.</li><li>Drop out:使得activation 函数失效，即让其输出在任何输入下都为0<ul><li>In each forward pass, randomly set some of the neurons to zero. Probability of dropping is a hyperparameter that are 0.5 for almost cases.训练过程中随机将部分神经元设置为失活</li><li>So you will chooses some activation and makes them zero.</li><li>It works because:<ul><li>It forces the network to have redundant representation; prevent co-adaption of features!</li><li>If you think about this, It ensemble some of the models in the same model!相当于集成学习，在一个模型中将他的多个不同的子模型进行集成</li></ul></li><li>At test time we might multiply each dropout layer by the probability of the dropout.</li><li>Sometimes at test time we don’t multiply anything and leave it as it is.</li><li>With drop out it takes more time to train.</li><li>Dropout是一种在深度学习环境中应用的正规化手段。它是这样运作的：在一次循环中我们先随机选择神经层中的一些单元并将其临时隐藏，然后再进行该次循环中神经网络的训练和优化过程。在下一次循环中，我们又将隐藏另外一些神经元，如此直至训练结束。<br>在训练时，每个神经单元以概率p被保留(dropout丢弃率为1-p)；在测试阶段，每个神经单元都是存在的，权重参数w要乘以p，成为：pw。测试时需要乘上p的原因：考虑第一隐藏层的一个神经元在dropout之前的输出是x，那么dropout之后的期望值是E=px+(1−p)0 ，在测试时该神经元总是激活，为了保持同样的输出期望值并使下一层也得到同样的结果，需要调整x→pxx→px. 其中p是Bernoulli分布（0-1分布）中值为1的概率</li></ul></li><li>Data augmentation:<ul><li>Another technique that makes Regularization.</li><li>Change the data!</li><li>For example flip the image, or rotate it.</li><li>Example in ResNet:<ul><li>Training: Sample random crops and scales:<ol><li>Pick random L in range [256,480]</li><li>Resize training image, short side = L</li><li>Sample random 224x224 patch.</li></ol></li><li>Testing: average a fixed set of crops<ol><li>Resize image at 5 scales: {224, 256, 384, 480, 640}</li><li>For each size, use 10 224x224 crops: 4 corners + center + flips</li></ol></li><li>Apply Color jitter or PCA</li><li>Translation, rotation, stretching.</li></ul></li></ul></li><li>Drop connect<ul><li>Like drop out idea it makes a regularization.</li><li>Instead of dropping the activation, we randomly zeroing the weights.</li></ul></li></ul><p><strong>Transfer learning</strong>:</p><ul><li>Some times your data is overfitted by your model because the data is small not because of regularization.自己的数据集太小</li><li>You need a lot of data if you want to train/use CNNs.</li><li>Steps of transfer learning<ol><li>Train on a big dataset that has common features with your dataset. Called pretraining.找到一个和你的小的数据集特征类似的大的数据集，并用你的模型在该数据集中进行训练</li><li>Freeze the layers except the last layer and feed your small dataset to learn only the last layer.将模型中除了最后一层外的所有层结构进行冻结，然后在小的数据集中进行训练，以学习最后一层</li><li>Not only the last layer maybe trained again, you can fine tune any number of layers you want based on the number of data you have</li></ol></li></ul></li></ul><h3 id="deep-learning-software"><a class="markdownIt-Anchor" href="#deep-learning-software"></a> Deep learning software</h3><ul><li><p>CPU vs GPU</p><ul><li><p>GPU The graphics card was developed to render graphics to play games or make 3D media,. etc.</p><ul><li>NVIDIA vs AMD<ul><li>Deep learning choose NVIDIA over AMD GPU because NVIDIA is pushing research forward deep learning also makes it architecture more suitable for deep learning.</li></ul></li></ul></li><li><p>CPU has fewer cores but each core is much faster and much more capable; great at sequential tasks. While GPUs has more cores but each core is much slower “dumber”; great for parallel tasks.CPU的核心更少，但是更快，胜任串行任务；GPU的核心更多，但是更慢，胜任并行任务</p></li><li><p>GPU cores needs to work together. and has its own memory.GPU各个核心需要并行工作，而且GPU有自己的内存，称为显存</p></li><li><p>Matrix multiplication is from the operations that are suited for GPUs. It has MxN independent operations that can be done on parallel.矩阵乘法适用于GPU中</p></li><li><p>Convolution operation also can be paralyzed because it has independent operations.卷积操作也能并行化</p></li><li><p>Programming GPUs frameworks:</p><ul><li><p>CUDA</p><p>(NVIDIA only)</p><ul><li>Write c-like code that runs directly on the GPU.</li><li>Its hard to build a good optimized code that runs on GPU. Thats why they provided high level APIs.</li><li>Higher level APIs: cuBLAS, cuDNN, etc</li><li><strong>CuDNN</strong> has implemented back prop. , convolution, recurrent and a lot more for you!</li><li>In practice you won’t write a parallel code. You will use the code implemented and optimized by others!</li></ul></li></ul></li><li><p>If you aren’t careful, training can bottleneck on reading data and transferring to GPU. So the solutions are:训练过程中在读取数据和迁移到gpu的过程中可能出现瓶颈</p><ul><li>Read all the data into RAM. # If possible将所有数据读如到内存</li><li>Use SSD instead of HDD使用固态硬盘</li><li>Use multiple CPU threads to prefetch data!使用多条CPU线程去预读取数据<ul><li>While the GPU are computing, a CPU thread will fetch the data for you.</li><li>A lot of frameworks implemented that for you because its a little bit painful!</li></ul></li></ul></li></ul></li><li><p><strong>Deep learning Frameworks</strong></p><ul><li>Its super fast moving!</li><li>Currently available frameworks:<ul><li>Tensorflow (Google)</li><li>Caffe (UC Berkeley)</li><li>Caffe2 (Facebook)</li><li>Torch (NYU / Facebook)</li><li>PyTorch (Facebook)</li><li>Theano (U monteral)</li><li>Paddle (Baidu)</li><li>CNTK (Microsoft)</li><li>MXNet (Amazon)</li></ul></li><li>The instructor thinks that you should focus on Tensorflow and PyTorch.</li><li>The point of deep learning frameworks:<ul><li>Easily build big computational graphs.方便地构建计算图</li><li>Easily compute gradients in computational graphs.方便地通过计算图计算梯度</li><li>Run it efficiently on GPU (cuDNN - cuBLAS)支持GPU</li></ul></li><li>Numpy doesn’t run on GPU.Numpy不能使用GPU</li><li>Most of the frameworks tries to be like NUMPY in the forward pass and then they compute the gradients for you.很多框架尽力去靠拢NUMPY，同时又能支持GPU</li></ul></li><li><p>**Tensorflow (Google)**静态架构</p><ul><li><p>Code are two parts:</p><ol><li>Define computational graph.定义好计算图</li><li>Run the graph and reuse it many times.运行计算图并多次使用</li></ol></li><li><p>Tensorflow uses a static graph architecture.静态的图结构</p></li><li><p>Tensorflow variables live in the graph. while the placeholders are feed each run.variable在计算图中生存，placeholders用于在计算图中占位，运行时填入数据</p></li><li><p>Global initializer function initializes the variables that lives in the graph.</p></li><li><p>Use predefined optimizers and losses.使用预先定义的优化器和损失函数</p></li><li><p>You can make a full layers with layers.dense function.</p></li><li><p>Keras</p><p>(High level wrapper):</p><ul><li>Keras is a layer on top pf Tensorflow, makes common things easy to do.</li><li>So popular!</li><li>Trains a full deep NN in a few lines of codes.</li></ul></li><li><p>There are a lot high level wrappers:</p><ul><li>Keras</li><li>TFLearn</li><li>TensorLayer</li><li>tf.layers <code>#Ships with tensorflow</code></li><li>tf-Slim <code>#Ships with tensorflow</code></li><li>tf.contrib.learn <code>#Ships with tensorflow</code></li><li>Sonnet <code># New from deep mind</code></li></ul></li><li><p>Tensorflow has pretrained models that you can use while you are using transfer learning.迁移学习的时候可以使用多个预训练模型</p></li><li><p>Tensorboard adds logging to record loss, stats. Run server and get pretty graphs! Tensorboard添加了日志用于跟踪损失</p></li><li><p>It has distributed code if you want to split your graph on some nodes.</p></li><li><p>Tensorflow is actually inspired from Theano. It has the same inspirations and structure.</p></li></ul></li><li><p><strong>PyTorch (Facebook)</strong></p><ul><li>Has three layers of abstraction:<ul><li>Tensor: ndarraybut runs on GPU,Like numpy arrays in tensorflow<ul><li>Variable: Node in a computational graphs; stores data and gradient <code>#Like Tensor, Variable, Placeholders</code></li></ul></li><li>Module: A NN layer; may store state or learnable weights<code>#Like tf.layers in tensorflow</code></li></ul></li><li>In PyTorch the graphs runs in the same loop you are executing which makes it easier for debugging. This is called a dynamic graph.动态图架构，容易进行debugging</li><li>In PyTorch you can define your own autograd functions by writing forward and backward for tensors. Most of the times it will implemented for you.一般只用重写forward函数</li><li>Torch.nn is a high level api like keras in tensorflow. You can create the models and go on and on.<ul><li>You can define your own nn module!</li></ul></li><li>Also Pytorch contains optimizers like tensorflow.</li><li>It contains a data loader that wraps a Dataset and provides minbatches, shuffling and multithreading.自己编写数据加载器很重要</li><li>PyTorch contains the best and super easy to use pretrained models</li><li>PyTorch contains Visdom that are like tensorboard. but Tensorboard seems to be more powerful.提供Visdom用于记录日志</li><li>PyTorch is new and still evolving compared to Torch. Its still in beta state.</li><li>PyTorch is best for research.对于research更加常用</li></ul></li><li><p>Tensorflow builds the graph once, then run them many times (Called static graph)定义一次网络即可多次使用，有专门的保存方式,在工业中更常用</p></li><li><p>In each PyTorch iteration we build a new graph (Called dynamic graph)每次使用都要重新搭建网络,在研究中更常用</p></li><li><p>Tensorflow fold make dynamic graphs easier in Tensorflow through dynamic batching.</p></li><li><p>Dynamic graph applications include: recurrent networks and recursive networks.</p></li><li><p>Caffe2 uses static graphs and can train model in python also works on IOS and Android</p></li><li><p>Tensorflow/Caffe2 are used a lot in production especially on mobile.</p></li></ul><h3 id="cnn-architectures"><a class="markdownIt-Anchor" href="#cnn-architectures"></a> CNN architectures</h3><ul><li>Focuses on CNN architectures that won ImageNet competition since 2012.</li></ul><p><img src="/images/181008/43.png" alt=""></p><ul><li><p>These architectures includes: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlexNet</a>, <a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">VGG</a>, <a href="https://research.google.com/pubs/pub43022.html" target="_blank" rel="noopener">GoogLeNet</a>, and <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">ResNet</a>.</p></li><li><p>Also we will discuss some interesting architectures as we go.</p></li><li><p>The first ConvNet that was made was <a href="http://ieeexplore.ieee.org/document/726791/" target="_blank" rel="noopener">LeNet-5</a> architectures are:by Yann Lecun at 1998.</p><ul><li>Architecture are: <code>CONV-POOL-CONV-POOL-FC-FC-FC</code></li></ul><p><img src="/images/181008/02.jpg" alt=""></p></li><li><p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener"><strong>AlexNet</strong></a> (2012):</p><ul><li>ConvNet that started the evolution and wins the ImageNet at 2012.</li><li>Architecture are: <code>CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MAXPOOL3-FC6-FC7-FC8</code></li><li>Contains exactly <strong>8</strong> layers the first 5 are Convolutional and the last 3 are fully connected layers.</li><li>Some other details:<ul><li>First use of RELU.</li><li>Norm layers but not used any more.</li><li>heavy data augmentation</li><li>Dropout <code>0.5</code></li><li>batch size <code>128</code></li><li>SGD momentum <code>0.9</code></li><li>Learning rate <code>1e-2</code> reduce by 10 at some iterations</li><li>7 CNN ensembles!</li></ul></li><li>AlexNet was trained on GTX 580 GPU with only 3 GB which wasn’t enough to train in one machine so they have spread the feature maps in half. The first AlexNet was distributed!</li><li>Its still used in transfer learning in a lot of tasks.</li><li>Total number of parameters are <code>60 million</code></li></ul></li><li><p><a href="https://arxiv.org/pdf/1409.1556" target="_blank" rel="noopener"><strong>VGGNet</strong></a> (2014) (Oxford)</p><ul><li>Deeper network with more layers.</li><li>Contains 19 layers.</li><li>Won on 2014 with GoogleNet with error 7.3%</li><li>Smaller filters with deeper layers.</li><li>The great advantage of VGG was the insight that multiple 3 × 3 convolution in sequence can emulate the effect of larger receptive fields, for examples 5 × 5 and 7 × 7.</li><li>Used the simple 3 x 3 Conv all through the network.</li></ul><p><img src="/images/181008/03.png" alt=""></p><ul><li><p>Has a similar details in training like AlexNet. Like using momentum and dropout.</p></li><li><p>在卷积神经网络中，感受野的定义是 卷积神经网络每一层输出的特征图（feature map）上的像素点在<strong>原始图像</strong>上映射的区域大小，即每一层feature的感受野都是对于原始输入图像而言的，而不是上一层的输入,但是在计算过程中，需要逐层计算该层在往上的每一层的感受野;公式 (N-1)_RF = f(N_RF, stride, ksize) = (N_RF - 1) * stride(convN) +ksize(convN)，其中，RF是感受野。N_RF和RF有点像，<strong>N代表 neighbour</strong>，指的是第n层的 a feature在n-1层的RF,显然第N层feature map在第N层的RF=1,在N-1层的RF=ksize__convN，计算RF时不需要考虑padding的影响。</p></li><li><p><strong>Feature Map的尺寸=(input_size + 2 * padding_size − ksize)/stride+1</strong></p><p><strong>根据定义 感受野是决定某一层输出结果中一个元素所对应的输入层的区域大小</strong></p><p><strong>这里指的是要求解的那层的一个元素也就是最初输入的out=1:</strong></p><p><strong>rfsize = f(out, stride, ksize) = (out - 1) * stride + ksize</strong></p><p><strong>感受野近似于用feature map为1时反推input_size ，只是不考虑padding</strong></p></li></ul></li><li><p><a href="https://research.google.com/pubs/pub43022.html" target="_blank" rel="noopener"><strong>GoogleNet</strong></a> (2014)</p><ul><li><p>Deeper network with more layers.</p></li><li><p>Contains 22 layers.</p></li><li><p>It has Efficient <strong>Inception</strong> module.</p></li><li><p>Only 5 million parameters! 12x less than AlexNet</p></li><li><p>Won on 2014 with VGGNet with error 6.7%</p></li><li><p>Inception module:内含并行操作的多种conv,将他们各自的输出在最后按depth堆叠成一个输出.</p><ul><li>Design a good local network topology (network within a network (NiN)) and then stack these modules on top of each other.</li><li>It consists of:<ul><li>Apply parallel filter operations on the input from previous layer<ul><li>Multiple convs of sizes (1 x 1, 3 x 3, 5 x 5)<ul><li>Adds padding to maintain the sizes.</li></ul></li><li>Pooling operation. (Max Pooling)<ul><li>Adds padding to maintain the sizes.</li></ul></li></ul></li><li>Concatenate all filter outputs together depth-wise.</li></ul></li><li>For example:<ul><li>Input for inception module is 28 x 28 x 256</li><li>Then the parallel filters applied:<ul><li>(1 x 1), 128 filter <code># output shape (28,28,128)</code></li><li>(3 x 3), 192 filter <code># output shape (28,28,192)</code></li><li>(5 x 5), 96 filter <code># output shape (28,28,96)</code></li><li>(3 x 3) Max pooling <code># output shape (28,28,256)</code></li></ul></li><li>After concatenation this will be <code>(28,28,672)</code></li></ul></li><li>By this design -We call Naive- it has a big computation complexity.<ul><li>The last example will make:<ul><li>[1 x 1 conv, 128] ==&gt; 28 * 28 * 128 * 1 * 1 * 256 = 25 Million approx</li><li>[3 x 3 conv, 192] ==&gt; 28 * 28 * 192 *3 *3 * 256 = 346 Million approx</li><li>[5 x 5 conv, 96] ==&gt; 28 * 28 * 96 * 5 * 5 * 256 = 482 Million approx</li><li>In total around 854 Million operation!</li></ul></li></ul></li><li>Solution:bottleneck layers that use 1x1 convolutions to reduce feature depth.<ul><li>Inspired from NiN (<a href="https://arxiv.org/abs/1312.4400" target="_blank" rel="noopener">Network in network</a>)</li></ul></li></ul><p><img src="/images/181008/05.png" alt=""></p></li><li><p>So GoogleNet stacks this Inception module multiple times to get a full architecture of a network that can solve a problem without the Fully connected layers.</p></li><li><p>Just to mention, it uses an average pooling layer at the end before the classification step.</p></li><li><p>Full architecture:</p></li></ul><p><img src="/images/181008/43.png" alt=""></p><ul><li>In February 2015 Batch-normalized Inception was introduced as Inception V2. Batch-normalization computes the mean and standard-deviation of all feature maps at the output of a layer, and normalizes their responses with these values.</li></ul></li><li><p><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener"><strong>ResNet</strong></a> (2015) (Microsoft Research)</p><ul><li>152-layer model for ImageNet. Winner by 3.57% which is more than human level error.</li><li>This is also the very first time that a network of &gt; hundred, even 1000 layers was trained.</li><li>Swept all classification and detection competitions in ILSVRC’15 and COCO’15!</li><li>What happens when we continue stacking deeper layers on a “plain” Convolutional neural network?<ul><li>The deeper model performs worse, but it’s not caused by overfitting!网络变深,准确率反而下降</li><li>The learning stops performs well somehow because deeper NN are harder to optimize!网络越深，越难优化，容易出现vanish gradient</li></ul></li><li>The deeper model should be able to perform at least as well as the shallower model.</li><li>A solution by construction is copying the learned layers from the shallower model and setting additional layers to identity mapping.如何保证加深的网络能够学到新的知识，并保证旧的知识也保存了下来？identity mapping的存在就是这个价值。</li><li>Residual block:<ul><li>Microsoft came with the Residual block which has this architecture:</li></ul></li></ul></li></ul><p><img src="/images/181008/45.png" alt=""></p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Instead of us trying To learn a new representation, We learn only Residual</span></span><br><span class="line">Y = (W2* RELU(W1x+b1) + b2) + X</span><br></pre></td></tr></table></figure><ul><li>Say you have a network till a depth of N layers. You only want to add a new layer if you get something extra out of adding that layer.</li><li>One way to ensure this new (N+1)th layer learns something new about your network is to also provide the input(x) without any transformation to the output of the (N+1)th layer. This essentially drives the new layer to learn something different from what the input has already encoded.这样可以驱使新的网络层学习与前N层已经学到的东西不同的内容</li><li>The other advantage is such connections help in handling the Vanishing gradient problem in very deep networks.这样的结构有助于解决在很深的网络中导致的梯度消失问题</li><li>With the Residual block we can now have a deep NN of any depth without the fearing that we can’t optimize the network.</li><li>ResNet with a large number of layers started to use a bottleneck layer similar to the Inception bottleneck to reduce the dimensions.Resnet中包含了大量的类似inception bottleneck设计的bottlenect layer用于减少维度</li><li><img src="/images/181008/07.jpg" alt=""></li><li><strong>Full ResNet architecture</strong>:<ul><li>Stack residual blocks.</li><li><img src="/images/181008/08.png" alt=""></li><li>Every residual block has two 3 x 3 conv layers.</li><li>Additional conv layer at the beginning.</li><li>No FC layers at the end (only FC 1000 to output classes)</li><li>Periodically, double number of filters and downsample spatially using stride 2 (/2 in each dimension)</li><li>Training ResNet in practice:<ul><li>Batch Normalization after every CONV layer.</li><li>Xavier/2 initialization from He et al.</li><li>SGD + Momentum (<code>0.9</code>)</li><li>Learning rate: 0.1, divided by 10 when validation error plateaus</li><li>Mini-batch size <code>256</code></li><li>Weight decay of <code>1e-5</code></li><li>No dropout used.</li></ul></li></ul></li></ul></li><li><p><strong>ResNets Improvements</strong>:</p><ul><li>(2016) Identity Mappings in Deep Residual Networks<ul><li>From the creators of ResNet.</li><li>Gives better performance.</li></ul></li><li>(2016) Wide Residual Networks<ul><li>Argues that residuals are the important factor, not depth</li><li>50-layer wide ResNet outperforms 152-layer original ResNet</li><li>Increasing width instead of depth more computationally efficient (parallelizable)</li></ul></li><li>(2016) Deep Networks with Stochastic Depth<ul><li>Motivation: reduce vanishing gradients and training time through short networks during training.</li><li>Randomly drop a subset of layers during each training pass</li><li>Use full deep network at test time.</li></ul></li></ul></li><li><p><strong>Beyond ResNets</strong>:</p><ul><li>(2017) FractalNet: Ultra-Deep Neural Networks without Residuals<ul><li>Argues that key is transitioning effectively from shallow to deep and residual representations are not necessary.</li><li>Trained with dropping out sub-paths</li><li>Full network at test time.</li></ul></li><li>(<a href="https://arxiv.org/abs/1608.06993" target="_blank" rel="noopener">2017</a>) Densely Connected Convolutional Networks</li><li>(2017) SqueezeNet: AlexNet-level Accuracy With 50x Fewer Parameters and &lt;0.5Mb Model Size<ul><li>Good for production.</li><li>It is a re-hash of many concepts from ResNet and Inception, and show that after all, a better design of architecture will deliver small network sizes and parameters without needing complex compression algorithms.</li></ul></li></ul></li><li><p>Conclusion:</p><ul><li>ResNet current best default.</li><li>Trend towards extremely deep networks</li><li>In the last couple of years, some models all using the shortcuts like “ResNet” to eaisly flow the gradients.</li></ul></li></ul><h2 id="recurrent-neural-networks"><a class="markdownIt-Anchor" href="#recurrent-neural-networks"></a> Recurrent Neural networks</h2><ul><li><p>Recurrent Neural Networks RNN Models:</p></li><li><p><img src="/images/181008/46.jpg" alt=""></p><ul><li>One to many<ul><li>Example: Image Captioning<ul><li>image ==&gt; sequence of words</li></ul></li></ul></li><li>Many to One<ul><li>Example: Sentiment Classification<ul><li>sequence of words ==&gt; sentiment</li></ul></li></ul></li><li>Many to many<ul><li>Example: Machine Translation<ul><li>seq of words in one language ==&gt; seq of words in another language</li></ul></li><li>Example: Video classification on frame level</li></ul></li></ul></li><li><p>So what is a recurrent neural network?</p><ul><li><p>Recurrent core cell that take an input x and that cell has an internal state that are updated each time it reads an input.</p></li><li><p><img src="/images/181008/47.jpg" alt=""></p></li><li><p>The RNN block should return a vector.</p></li><li><p>We can process a sequence of vectors x by applying a recurrence formula at every time step:</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">h[t] = fw (h[t-1], x[t])			# Where fw is some function with parameters W</span><br></pre></td></tr></table></figure></li><li><p>The same function and the same set of parameters are used at every time step.</p></li></ul></li><li><p>(Vanilla) Recurrent Neural Network:</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">h[t] = tanh (W[h,h]*h[t-1] + W[x,h]*x[t])    # Then we save h[t]</span><br><span class="line">y[t] = W[h,y]*h[t]</span><br></pre></td></tr></table></figure></li><li><p>This is the simplest example of a RNN.</p></li></ul></li><li><p>RNN works on a sequence of related data.</p></li></ul></li><li><p>Recurrent NN Computational graph:</p></li><li><p><img src="/images/181008/10.jpg" alt=""></p><ul><li><code>h0</code> are initialized to zero.</li><li>Gradient of <code>W</code> is the sum of all the <code>W</code> gradients that has been calculated!</li><li>A many to many graph:</li></ul></li></ul><p>​</p></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="http://aier02.com/2018/10/08/CS231n_2017_Summary/" title="CS231n-2017-Summary" target="_blank" rel="external">http://aier02.com/2018/10/08/CS231n_2017_Summary/</a></li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://github.com/aier02" target="_blank" class="img-burn thumb-sm visible-lg"><img src="/images/avatar.jpg" class="img-rounded w-full" alt=""></a></div><div class="media-body"><h3 class="media-heading"><a href="https://github.com/aier02" target="_blank"><span class="text-dark">Mileeet</span><small class="ml-1x">software engineer</small></a></h3><div>个人简介。</div></div></figure></div></div></div></article><section id="comments"><div id="vcomments"></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="prev"><a href="/2018/10/13/common_knowledge/" title="common knowledge"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a></li><li class="next"><a href="/2018/10/05/optimization/" title="optimization"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li><li class="toggle-toc"><a class="toggle-btn collapsed" data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button"><span>[&nbsp;</span><span>文章目录</span> <i class="text-collapsed icon icon-anchor"></i> <i class="text-in icon icon-close"></i> <span>]</span></a></li></ul><div class="bar-right"><div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div></div></div></nav></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="https://github.com/aier02" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul><div class="copyright"><div class="publishby">Theme by <a href="https://github.com/cofess" target="_blank">cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.</div></div></footer><script src="https://cdn.bootcss.com/jquery/1.12.4/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/js/plugin.min.js"></script><script src="/js/application.js"></script><script>!function(T){var N={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(未命名)"},ROOT_URL:"/",CONTENT_URL:"/content.json"};T.INSIGHT_CONFIG=N}(window)</script><script src="/js/insight.js"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/npm/valine"></script><script type="text/javascript">var GUEST=["nick","mail","link"],meta="nick,mail,link";meta=meta.split(",").filter(function(e){return GUEST.indexOf(e)>-1}),new Valine({el:"#vcomments",verify:!1,notify:!1,appId:"EkgjNDuvPLC2P4JUvz9XLze6-gzGzoHsz",appKey:"mKLzHHOFc0SEcUIl3DvykWdG",placeholder:"Just go go",avatar:"mm",meta:meta,pageSize:"10",visitor:!1})</script></body></html><!-- rebuild by neat -->