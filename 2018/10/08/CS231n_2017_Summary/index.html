<!-- build time:Wed Oct 10 2018 23:27:46 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta name="referrer" content="never"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>CS231n-2017-Summary | Amadeus</title><meta name="description" content="Something new to me when I read such a good notebook about CS231n-2017-Summary CNNs常用same策略同时保存图像边缘信息Padding strategy:in order to maintain our full size of the input. If we didn’t do padding zero the"><meta name="keywords" content="notebook"><meta property="og:type" content="article"><meta property="og:title" content="CS231n-2017-Summary"><meta property="og:url" content="http://aier02.com/2018/10/08/CS231n_2017_Summary/index.html"><meta property="og:site_name" content="Aier02"><meta property="og:description" content="Something new to me when I read such a good notebook about CS231n-2017-Summary CNNs常用same策略同时保存图像边缘信息Padding strategy:in order to maintain our full size of the input. If we didn’t do padding zero the"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="http://aier02.com/images/181008/activation.png"><meta property="og:image" content="http://aier02.com/images/181008/43.png"><meta property="og:image" content="http://aier02.com/images/181008/02.jpg"><meta property="og:image" content="http://aier02.com/images/181008/03.png"><meta property="og:updated_time" content="2018-10-10T02:23:16.013Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="CS231n-2017-Summary"><meta name="twitter:description" content="Something new to me when I read such a good notebook about CS231n-2017-Summary CNNs常用same策略同时保存图像边缘信息Padding strategy:in order to maintain our full size of the input. If we didn’t do padding zero the"><meta name="twitter:image" content="http://aier02.com/images/181008/activation.png"><link rel="canonical" href="http://aier02.com/2018/10/08/CS231n_2017_Summary/index.html"><link rel="alternate" href="/atom.xml" title="Aier02" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"><link href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css" rel="stylesheet"></head><body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://github.com/aier02" target="_blank"><img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">Amadeus</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">cv爱好者</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Guangdong, China</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="搜索"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav"><li class="menu-item menu-item-home"><a href="/."><i class="icon icon-home-fill"></i> <span class="menu-title">首页</span></a></li><li class="menu-item menu-item-archives"><a href="/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">归档</span></a></li><li class="menu-item menu-item-categories"><a href="/categories"><i class="icon icon-folder"></i> <span class="menu-title">分类</span></a></li><li class="menu-item menu-item-tags"><a href="/tags"><i class="icon icon-tags"></i> <span class="menu-title">标签</span></a></li><li class="menu-item menu-item-repository"><a href="/repository"><i class="icon icon-project"></i> <span class="menu-title">项目</span></a></li><li class="menu-item menu-item-books"><a href="/books"><i class="icon icon-book-fill"></i> <span class="menu-title">书单</span></a></li><li class="menu-item menu-item-links"><a href="/links"><i class="icon icon-friendship"></i> <span class="menu-title">友链</span></a></li><li class="menu-item menu-item-about"><a href="/about"><i class="icon icon-cup-fill"></i> <span class="menu-title">关于</span></a></li></ul><ul class="social-links"><li><a href="https://github.com/aier02" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">公告</h3><div class="widget-body"><div id="board"><div class="content"><p>欢迎交流与分享经验!</p></div></div></div></div><div class="widget"><h3 class="widget-title">分类</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/blog/">blog</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs231n/">cs231n</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/">pytorch</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/statistical-learning-method/">statistical learning method</a><span class="category-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签</h3><div class="widget-body"><ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ChexNet/">ChexNet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EDA/">EDA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Github-page/">Github page</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ResNet/">ResNet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basic-knowledge/">basic knowledge</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/chest-X-ray/">chest X-ray</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/demo/">demo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/experience/">experience</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/image-classification/">image classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/introduction/">introduction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jikecloud/">jikecloud</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/keras/">keras</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linear-classification/">linear classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/namesilo/">namesilo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/notebook/">notebook</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch-cookbook/">pytorch cookbook</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/segmentation/">segmentation</a><span class="tag-list-count">3</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签云</h3><div class="widget-body tagcloud"><a href="/tags/CNN/" style="font-size:13px">CNN</a> <a href="/tags/ChexNet/" style="font-size:13px">ChexNet</a> <a href="/tags/EDA/" style="font-size:13px">EDA</a> <a href="/tags/Github-page/" style="font-size:13px">Github page</a> <a href="/tags/Hexo/" style="font-size:13px">Hexo</a> <a href="/tags/ResNet/" style="font-size:13px">ResNet</a> <a href="/tags/SVM/" style="font-size:13px">SVM</a> <a href="/tags/basic-knowledge/" style="font-size:14px">basic knowledge</a> <a href="/tags/chest-X-ray/" style="font-size:13px">chest X-ray</a> <a href="/tags/demo/" style="font-size:13px">demo</a> <a href="/tags/experience/" style="font-size:13px">experience</a> <a href="/tags/image-classification/" style="font-size:13px">image classification</a> <a href="/tags/introduction/" style="font-size:13px">introduction</a> <a href="/tags/jikecloud/" style="font-size:13px">jikecloud</a> <a href="/tags/keras/" style="font-size:13px">keras</a> <a href="/tags/linear-classification/" style="font-size:13px">linear classification</a> <a href="/tags/namesilo/" style="font-size:13px">namesilo</a> <a href="/tags/notebook/" style="font-size:14px">notebook</a> <a href="/tags/pytorch-cookbook/" style="font-size:13px">pytorch cookbook</a> <a href="/tags/segmentation/" style="font-size:14px">segmentation</a></div></div><div class="widget"><h3 class="widget-title">归档</h3><div class="widget-body"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">九月 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">八月 2018</a><span class="archive-list-count">2</span></li></ul></div></div><div class="widget"><h3 class="widget-title">最新文章</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/cs231n/">cs231n</a></p><p class="item-title"><a href="/2018/10/08/CS231n_2017_Summary/" class="title">CS231n-2017-Summary</a></p><p class="item-date"><time datetime="2018-10-08T14:46:19.006Z" itemprop="datePublished">2018-10-08</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/cs231n/">cs231n</a></p><p class="item-title"><a href="/2018/10/05/optimization/" class="title">optimization</a></p><p class="item-date"><time datetime="2018-10-05T12:44:14.537Z" itemprop="datePublished">2018-10-05</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/cs231n/">cs231n</a></p><p class="item-title"><a href="/2018/10/03/linear_classification/" class="title">Linear classification</a></p><p class="item-date"><time datetime="2018-10-03T05:31:44.756Z" itemprop="datePublished">2018-10-03</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/pytorch/">pytorch</a></p><p class="item-title"><a href="/2018/10/02/pytorch_cookbook-U2&U3/" class="title">pytorch cookbook U2&amp;U3</a></p><p class="item-date"><time datetime="2018-10-02T08:27:32.398Z" itemprop="datePublished">2018-10-02</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/Kaggle/">Kaggle</a></p><p class="item-title"><a href="/2018/10/02/rsna_summary/" class="title">first time in Kaggle-summary</a></p><p class="item-date"><time datetime="2018-10-01T16:08:39.573Z" itemprop="datePublished">2018-10-02</time></p></div></li></ul></div></div></div></aside><aside class="sidebar sidebar-toc collapse" id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><nav id="toc" class="article-toc"><h3 class="toc-title">文章目录</h3><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#cnns"><span class="toc-number">1.</span> <span class="toc-text">CNNs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#training-neural-networks-i"><span class="toc-number">2.</span> <span class="toc-text">Training neural networks I</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#training-neural-networks-ii"><span class="toc-number">3.</span> <span class="toc-text">Training neural networks II</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#deep-learning-software"><span class="toc-number">4.</span> <span class="toc-text">Deep learning software</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cnn-architectures"><span class="toc-number">5.</span> <span class="toc-text">CNN architectures</span></a></li></ol></nav></div></aside><main class="main" role="main"><div class="content"><article id="post-CS231n_2017_Summary" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">CS231n-2017-Summary</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/2018/10/08/CS231n_2017_Summary/" class="article-date"><time datetime="2018-10-08T14:46:19.006Z" itemprop="datePublished">2018-10-08</time></a></span> <span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="/categories/cs231n/">cs231n</a></span> <span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link" href="/tags/notebook/">notebook</a></span> <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2018/10/08/CS231n_2017_Summary/#comments" class="article-comment-link">评论</a></span></div></div><div class="article-entry markdown-body" itemprop="articleBody"><p>Something new to me when I read such a good notebook about <a href="https://github.com/mbadry1/CS231n-2017-Summary" target="_blank" rel="noopener">CS231n-2017-Summary</a></p><h3 id="cnns"><a class="markdownIt-Anchor" href="#cnns"></a> CNNs</h3><p>常用same策略同时保存图像边缘信息</p><ul><li>Padding strategy:in order to maintain our full size of the input. If we didn’t do padding zero the input will be shrinking too fast and we will lose a lot of data.Give a stride of <code>1</code> its common to pad to this equation: <code>(F-1)/2</code> where F is the filter size, zero padding from both sides.If we pad this way we call this same convolution.<ul><li>If we have input of shape (32,32,3) and ten filters with shape is (5,5) with stride 1 and pad 2;Output size will be (32,32,10) # We maintain the size.</li><li>Size of parameters per filter = 5x5x3 + 1 = 76(+1 for bias)</li><li>All parameters 76x10=760</li></ul></li><li>So here are the parameters for the Conv layer:<ul><li>Number of filters K.<ul><li>Usually a power of 2.</li></ul></li><li>Spatial content size F.<ul><li>3,5,7 …</li></ul></li><li>The stride S.<ul><li>Usually 1 or 2 (If the stride is big there will be a downsampling but different of pooling)</li></ul></li><li>Amount of Padding<ul><li>If we want the input shape to be as the output shape, based on the F if 3 its 1, if F is 5 the 2 and so on</li></ul></li></ul></li></ul><p>一般而言pooling层是不可(用)学习的</p><ul><li>Pooling makes the representation smaller and more manageable.</li><li>Pooling Operates over each activation map independently.</li><li>Example of pooling is the maxpooling.<ul><li>Parameters of max pooling is the size of the filter and the stride&quot;<ul><li>Example <code>2x2</code> with stride <code>2</code> <code># Usually the two parameters are the same 2 , 2</code></li></ul></li></ul></li><li>Also example of pooling is average pooling.<ul><li>In this case it might be learnable.</li></ul></li></ul><h3 id="training-neural-networks-i"><a class="markdownIt-Anchor" href="#training-neural-networks-i"></a> Training neural networks I</h3><ul><li><p>As a revision here are the Mini batch stochastic gradient descent algorithm steps,小批量的随机梯度下降算法的步骤:</p><ul><li>Loop:<ol><li>Sample a batch of data.</li><li>Forward prop it through the graph (network) and get loss.(define loss)</li><li>Backprop to calculate the gradients.(chain rule)</li><li>Update the parameters using the gradients(learning rate)</li></ol></li></ul></li><li><p>Activation functions,用于引入非线性因素，单纯的线性模型表达能录不足。</p><p><img src="/images/181008/activation.png" alt=""></p><ul><li><p>Sigmoid:</p><ul><li>Squashes the numbers between [0,1]</li><li>Used as a firing rate like human brains.</li><li><code>Sigmoid(x) = 1 / (1 + e^-x)</code></li><li>Problems with sigmoid:<ul><li>big values neurons kill the gradients.</li><li>Gradients are in most cases near 0 (Big values/small values), that kills the updates if the graph/network are large.</li><li>Not Zero-centered.<ul><li>Didn’t produce zero-mean data.</li></ul></li><li>exp() is a bit compute expensive.<ul><li>just to mention. We have a more complex operations in deep learning like convolution.</li></ul></li></ul></li></ul></li><li><p>Tanh:</p><ul><li>Squashes the numbers between [-1,1]</li><li>Zero centered.</li><li>Still big values neurons “kill” the gradients.</li><li><code>Tanh(x)</code> is the equation.<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mi>s</mi><mi>i</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">tanh(x)=\frac {sinh(x)}{cosh(x)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:1.01em"></span><span class="strut bottom" style="height:1.53em;vertical-align:-.52em"></span><span class="base textstyle uncramped"><span class="mord mathit">t</span><span class="mord mathit">a</span><span class="mord mathit">n</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.34500000000000003em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">s</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.485em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">s</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>,<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>i</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><mi>x</mi></mrow></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><mn>2</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">sinh(x)=\frac {e^{x}-e^{-x}}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.939765em"></span><span class="strut bottom" style="height:1.284765em;vertical-align:-.345em"></span><span class="base textstyle uncramped"><span class="mord mathit">s</span><span class="mord mathit">i</span><span class="mord mathit">n</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.345em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.394em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">−</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord">−</span><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>,<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi>h</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mrow><mi>x</mi></mrow></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><mn>2</mn></mrow></mfrac></mrow><annotation encoding="application/x-tex">cosh(x)=\frac {e^{x}+e^{-x}}{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="strut" style="height:.939765em"></span><span class="strut bottom" style="height:1.284765em;vertical-align:-.345em"></span><span class="base textstyle uncramped"><span class="mord mathit">c</span><span class="mord mathit">o</span><span class="mord mathit">s</span><span class="mord mathit">h</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:.345em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathrm">2</span></span></span></span><span style="top:-.22999999999999998em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-.394em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord"><span class="mord mathit">e</span><span class="vlist"><span style="top:-.363em;margin-right:.07142857142857144em"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span><span class="reset-scriptstyle scriptscriptstyle uncramped"><span class="mord scriptscriptstyle uncramped"><span class="mord">−</span><span class="mord mathit">x</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span></span></span></span>,</li><li>Proposed by Yann Lecun in 1991</li></ul></li><li><p>RELU (Rectified linear unit):</p><ul><li><code>RELU(x) = max(0,x)</code></li><li>Doesn’t kill the gradients.<ul><li>Only small values that are killed. Killed the gradient in the half</li></ul></li><li>Computationally efficient.</li><li>Converges much faster than Sigmoid and Tanh <code>(6x)</code></li><li>More biologically plausible than sigmoid.</li><li>Proposed by Alex Krizhevsky in 2012 Toronto university. (AlexNet)</li><li>Problems:<ul><li>Not zero centered.</li></ul></li><li>If weights aren’t initialized good, maybe 75% of the neurons will be dead and thats a waste computation. But its still works. This is an active area of research to optimize this.</li><li>To solve the issue mentioned above, people might initialize all the biases by 0.01</li></ul></li><li><p>Leaky RELU:</p><ul><li><code>leaky_RELU(x) = max(0.01x,x)</code></li><li>Doesn’t kill the gradients from both sides.</li><li>Computationally efficient.</li><li>Converges much faster than Sigmoid and Tanh (6x)</li><li>Will not die.</li><li>PRELU is placing the 0.01 by a variable alpha which is learned as a parameter.</li></ul></li><li><p>Exponential linear units (ELU):</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ELU(x) = &#123; x                                           if x &gt; 0</span><br><span class="line">		   alpah *(exp(x) -1)		                   if x &lt;= 0</span><br><span class="line">           # alpah are a learning parameter</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>It has all the benefits of RELU</p></li><li><p>Closer to zero mean outputs and adds some robustness to noise.</p></li><li><p>problems</p><ul><li><code>exp()</code> is a bit compute expensive.</li></ul></li></ul></li><li><p>Maxout activations:</p><ul><li><code>maxout(x) = max(w1.T*x + b1, w2.T*x + b2)</code></li><li>Generalizes RELU and Leaky RELU</li><li>Doesn’t die!</li><li>Problems:<ul><li>oubles the number of parameters per neuron</li></ul></li></ul></li><li><p>In practice:</p><ul><li>Use RELU. Be careful for your learning rates.</li><li>Try out Leaky RELU/Maxout/ELU</li><li>Try out tanh but don’t expect much.</li><li>Don’t use sigmoid!</li></ul></li></ul><p><strong>Data preprocessing</strong>:</p><ul><li><p>Normalize the data:减去均值后除以标准差</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Zero centered data. (Calculate the mean for every input).</span></span><br><span class="line"><span class="comment"># On of the reasons we do this is because we need data to be between positive and negative and not all the be negative or positive. </span></span><br><span class="line">X -= np.mean(X, axis = <span class="number">1</span>)</span><br><span class="line"><span class="comment">#np.mean()中的参数axis指定了哪个维度被压缩成1，例如axis=0,则输出的结果为一行，即求得输入x的每一列的平均，压缩成一行，同理axis=1，则输出为一列，该结果中的每一行为按照行进行平均的值。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Then apply the standard deviation. Hint: in images we don't do this.</span></span><br><span class="line">X /= np.std(X, axis = <span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p>To normalize images:对图像进行标准化</p><ul><li>Subtract the mean image (E.g. Alexnet)<ul><li>Mean image shape is the same as the input images.</li></ul></li><li>Or Subtract per-channel mean<ul><li>Means calculate the mean for each channel of all images. Shape is 3 (3 channels)</li></ul></li></ul></li><li><p>First idea is to initialize the w’s with small random numbers:</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">0.01</span> * np.random.rand(D, H)</span><br><span class="line"><span class="comment"># Works OK for small networks but it makes problems with deeper networks!</span></span><br></pre></td></tr></table></figure></li><li><p>The standard deviations is going to zero in deeper networks. and the gradient will vanish sooner in deep networks.使用任意小的数字进行对w初始化，随着网络的加深可能导致梯度消失问题(每一层的蔬输入很小)。</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">W = <span class="number">1</span> * np.random.rand(D, H) </span><br><span class="line"><span class="comment"># Works OK for small networks but it makes problems with deeper networks!</span></span><br></pre></td></tr></table></figure></li><li><p>The network will explode with big numbers!，使用大于一的初值可能会导致深层网络中梯度爆炸问题。</p></li></ul></li><li><p>Xavier initialization:</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.rand(in, out) / np.sqrt(in)</span><br></pre></td></tr></table></figure></li><li><p>It works because we want the variance of the input to be as the variance of the output.</p></li><li><p>But it has an issue, It breaks when you are using RELU.</p></li></ul></li><li><p>He initialization(Solution for the RELU issue):</p><ul><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">W = np.random.rand(in, out) / np.sqrt(in/2)</span><br></pre></td></tr></table></figure></li><li><p>Solves the issue with RELU. Its recommended when you are using RELU</p></li></ul></li></ul><p><strong>Batch normalization</strong>:</p><ul><li><p>is a technique to provide any layer in a Neural Network with inputs that are zero mean/unit variance.</p></li><li><p>It speeds up the training. You want to do this a lot.</p><ul><li>Made by Sergey Ioffe and Christian Szegedy at 2015.</li></ul></li><li><p>We make a Gaussian activations in each layer. by calculating the mean and the variance.</p></li><li><p>Usually inserted after (fully connected or Convolutional layers) and (before nonlinearity).</p></li><li><p>Steps (For each output of a layer)</p><ol><li><p>First we compute the mean and variance^2 of the batch for each feature.</p></li><li><p>We normalize by subtracting the mean and dividing by square root of (variance^2 + epsilon)</p><ul><li>epsilon to not divide by zero</li></ul></li><li><p>Then we make a scale and shift variables:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Result = gamma * normalizedX + beta</span><br></pre></td></tr></table></figure><ul><li>gamma and beta are learnable parameters.</li><li>it basically possible to say “Hey!! I don’t want zero mean/unit variance input, give me back the raw input - it’s better for me.”</li><li>Hey shift and scale by what you want not just the mean and variance!</li></ul></li></ol></li></ul><p><strong>Baby sitting the learning process</strong></p><ol><li>Preprocessing of data.</li><li>Choose the architecture.</li><li>Make a forward pass and check the loss (Disable regularization). Check if the loss is reasonable.</li><li>Add regularization, the loss should go up!</li><li>Disable the regularization again and take a small number of data and try to train the loss and reach zero loss.<ul><li>You should overfit perfectly for small datasets.</li></ul></li><li>Take your full training data, and small regularization then try some value of learning rate.<ul><li>If loss is barely changing, then the learning rate is small.</li><li>If you got <code>NAN</code> then your NN exploded and your learning rate is high.</li><li>Get your learning rate range by trying the min value (That can change) and the max value that doesn’t explode the network.</li></ul></li><li>Do Hyperparameters optimization to get the best hyperparameters values.</li></ol><p>Hyperparameter Optimization</p><ul><li>Try Cross validation strategy.<ul><li>Run with a few ephocs, and try to optimize the ranges.</li></ul></li><li>Its best to optimize in log space.</li><li>Adjust your ranges and try again.</li><li>Its better to try random search instead of grid searches (In log space)</li></ul><h3 id="training-neural-networks-ii"><a class="markdownIt-Anchor" href="#training-neural-networks-ii"></a> Training neural networks II</h3><p><strong>Optimization algorithms</strong>:</p><ul><li><p>Problems with stochastic gradient descent:随机梯度下降算法的问题</p><ul><li><p>if loss quickly in one direction and slowly in another (For only two variables), you will get very slow progress along shallow dimension, jitter along steep direction. Our NN will have a lot of parameters then the problem will be more.</p></li><li><p>Local minimum or saddle points；极小值或者鞍点问题</p><ul><li>何为鞍点？鞍点（Saddle point）在微分方程中，沿着某一方向是稳定的，另一条方向是不稳定的奇点，叫做鞍点。在泛函中，既不是极大值点也不是极小值点的临界点，叫做鞍点。在矩阵中，一个数在所在行中是最大值，在所在列中是最小值，则被称为鞍点。在物理上要广泛一些，指在一个方向是极大值，另一个方向是极小值的点</li></ul><ul><li>If SGD went into local minimum we will stuck at this point because the gradient is zero.遇到极小值点会stuck</li><li>Also in saddle points the gradient will be zero so we will stuck.</li><li>Saddle points says that at some point:鞍点在gradient上的表现<ul><li>Some gradients will get the loss up.</li><li>Some gradients will get the loss down.</li><li>And that happens more in high dimensional (100 million dimension for example)</li></ul></li><li>The problem of deep NN is more about saddle points than about local minimum because deep NN has high dimensions (Parameters)</li><li>Mini batches are noisy because the gradient is not taken for the whole batch.</li></ul></li></ul></li><li><p><strong>SGD + momentum</strong>:引入momentum，解决在鞍点或者极小值点处出现gradient为0而无法及继续更新参数的情况</p><ul><li><p>Build up velocity as a running mean of gradients:</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Computing weighted average. rho best is in range [0.9 - 0.99]</span></span><br><span class="line">V[t+<span class="number">1</span>] = rho * v[t] + dx</span><br><span class="line">x[t+<span class="number">1</span>] = x[t] - learningRate * V[t+<span class="number">1</span>]</span><br></pre></td></tr></table></figure></li><li><p><code>V[0]</code> is zero.</p></li><li><p>Solves the saddle point and local minimum problems.</p></li><li><p>It overshoots the problem and returns to it back.</p></li></ul></li><li><p><strong>Nestrov momentum</strong>:</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dx = compute_gradient(x)</span><br><span class="line">old_v = v</span><br><span class="line">v = rho * v - learning_rate * dx</span><br><span class="line">x+= -rho * old_v + (<span class="number">1</span>+rho) * v</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>AdaGrad</strong></p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(<span class="keyword">True</span>):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># here is a problem, the grad_squared isn't decayed (gets so large)</span></span><br><span class="line">  grad_squared += dx * dx			</span><br><span class="line">  </span><br><span class="line">  x -= (learning_rate*dx) / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>RMSProp</strong></p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">grad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span>(<span class="keyword">True</span>):</span><br><span class="line">  dx = compute_gradient(x)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#Solved ADAgra</span></span><br><span class="line">  grad_squared = decay_rate * grad_squared + (<span class="number">1</span>-grad_squared) * dx * dx  </span><br><span class="line">  </span><br><span class="line">  x -= (learning_rate*dx) / (np.sqrt(grad_squared) + <span class="number">1e-7</span>)</span><br></pre></td></tr></table></figure></li><li><p>People uses this instead of AdaGrad</p></li></ul></li><li><p><strong>Adam</strong>,最常用的优化器，结合了momentum和RMSProp</p><ul><li>Calculates the momentum and RMSProp as the gradients.</li><li>It need a Fixing bias to fix starts of gradients.</li><li>Is the best technique so far runs best on a lot of problems.</li><li>With <code>beta1 = 0.9</code> and <code>beta2 = 0.999</code> and <code>learning_rate = 1e-3</code> or <code>5e-4</code> is a great starting point for many models!</li></ul></li><li><p><strong>Learning decay</strong>学习率退火，避免因为网络的不断加深而导致学习率相对参数而言过大</p><ul><li>Ex. decay learning rate by half every few epochs.</li><li>To help the learning rate not to bounce out.</li><li>Learning decay is common with SGD+momentum but not common with Adam.</li><li>Dont use learning decay from the start at choosing your hyperparameters. Try first and check if you need decay or not.</li></ul></li></ul><p><strong>Regularization</strong>:损失函数中引入正则化项；集成学习；drop out修改网络结构；数据增强</p><ul><li>So far we have talked about reducing the training error, but we care about most is how our model will handle unseen data!上述优化更多的是在做如何更新参数使得error减少，但我们更加关心的是模型的泛化能力</li><li>What if the gab of the error between training data and validation data are too large?</li><li>This error is called high variance.</li><li>Model Ensemble:<ul><li>Algorithm:<ul><li>Train multiple independent models of the same architecture with different initializations.</li><li>At test time average their results.</li></ul></li><li>It can get you extra 2% performance.</li><li>It reduces the generalization error.</li><li>You can use some snapshots of your NN at the training ensembles them and take the results.</li></ul></li><li>Regularization solves the high variance problem. We have talked about L1, L2 Regularization.</li><li>Some Regularization techniques are designed for only NN and can do better.</li><li>Drop out:使得activation 函数失效，即让其输出在任何输入下都为0<ul><li>In each forward pass, randomly set some of the neurons to zero. Probability of dropping is a hyperparameter that are 0.5 for almost cases.训练过程中随机将部分神经元设置为失活</li><li>So you will chooses some activation and makes them zero.</li><li>It works because:<ul><li>It forces the network to have redundant representation; prevent co-adaption of features!</li><li>If you think about this, It ensemble some of the models in the same model!相当于集成学习，在一个模型中将他的多个不同的子模型进行集成</li></ul></li><li>At test time we might multiply each dropout layer by the probability of the dropout.</li><li>Sometimes at test time we don’t multiply anything and leave it as it is.</li><li>With drop out it takes more time to train.</li><li>Dropout是一种在深度学习环境中应用的正规化手段。它是这样运作的：在一次循环中我们先随机选择神经层中的一些单元并将其临时隐藏，然后再进行该次循环中神经网络的训练和优化过程。在下一次循环中，我们又将隐藏另外一些神经元，如此直至训练结束。<br>在训练时，每个神经单元以概率p被保留(dropout丢弃率为1-p)；在测试阶段，每个神经单元都是存在的，权重参数w要乘以p，成为：pw。测试时需要乘上p的原因：考虑第一隐藏层的一个神经元在dropout之前的输出是x，那么dropout之后的期望值是E=px+(1−p)0 ，在测试时该神经元总是激活，为了保持同样的输出期望值并使下一层也得到同样的结果，需要调整x→pxx→px. 其中p是Bernoulli分布（0-1分布）中值为1的概率</li></ul></li><li>Data augmentation:<ul><li>Another technique that makes Regularization.</li><li>Change the data!</li><li>For example flip the image, or rotate it.</li><li>Example in ResNet:<ul><li>Training: Sample random crops and scales:<ol><li>Pick random L in range [256,480]</li><li>Resize training image, short side = L</li><li>Sample random 224x244 patch.</li></ol></li><li>Testing: average a fixed set of crops<ol><li>Resize image at 5 scales: {224, 256, 384, 480, 640}</li><li>For each size, use 10 224x224 crops: 4 corners + center + flips</li></ol></li><li>Apply Color jitter or PCA</li><li>Translation, rotation, stretching.</li></ul></li></ul></li><li>Drop connect<ul><li>Like drop out idea it makes a regularization.</li><li>Instead of dropping the activation, we randomly zeroing the weights.</li></ul></li></ul><p><strong>Transfer learning</strong>:</p><ul><li>Some times your data is overfitted by your model because the data is small not because of regularization.自己的数据集太小</li><li>You need a lot of data if you want to train/use CNNs.</li><li>Steps of transfer learning<ol><li>Train on a big dataset that has common features with your dataset. Called pretraining.找到一个和你的小的数据集特征类似的打的数据集，并用你的模型在该数据集中进行训练</li><li>Freeze the layers except the last layer and feed your small dataset to learn only the last layer.将模型中除了最后一层外的所有层结构进行冻结，然后在小的数据集中进行训练，以学习最后一层</li><li>Not only the last layer maybe trained again, you can fine tune any number of layers you want based on the number of data you have</li></ol></li></ul></li></ul><h3 id="deep-learning-software"><a class="markdownIt-Anchor" href="#deep-learning-software"></a> Deep learning software</h3><ul><li><p>CPU vs GPU</p><ul><li><p>GPU The graphics card was developed to render graphics to play games or make 3D media,. etc.</p><ul><li>NVIDIA vs AMD<ul><li>Deep learning choose NVIDIA over AMD GPU because NVIDIA is pushing research forward deep learning also makes it architecture more suitable for deep learning.</li></ul></li></ul></li><li><p>CPU has fewer cores but each core is much faster and much more capable; great at sequential tasks. While GPUs has more cores but each core is much slower “dumber”; great for parallel tasks.CPU的核心更少，但是更快，胜任串行任务；GPU的核心更多，但是更慢，胜任并行任务</p></li><li><p>GPU cores needs to work together. and has its own memory.GPU各个核心需要并行工作，而且GPU有自己的内存，称为显存</p></li><li><p>Matrix multiplication is from the operations that are suited for GPUs. It has MxN independent operations that can be done on parallel.矩阵乘法适用于GPU中</p></li><li><p>Convolution operation also can be paralyzed because it has independent operations.卷积操作也能并行化</p></li><li><p>Programming GPUs frameworks:</p><ul><li><p>CUDA</p><p>(NVIDIA only)</p><ul><li>Write c-like code that runs directly on the GPU.</li><li>Its hard to build a good optimized code that runs on GPU. Thats why they provided high level APIs.</li><li>Higher level APIs: cuBLAS, cuDNN, etc</li><li><strong>CuDNN</strong> has implemented back prop. , convolution, recurrent and a lot more for you!</li><li>In practice you won’t write a parallel code. You will use the code implemented and optimized by others!</li></ul></li></ul></li><li><p>If you aren’t careful, training can bottleneck on reading data and transferring to GPU. So the solutions are:训练过程中在读取数据和迁移到gpu的过程中可能出现瓶颈</p><ul><li>Read all the data into RAM. # If possible将所有数据读如到内存</li><li>Use SSD instead of HDD使用固态硬盘</li><li>Use multiple CPU threads to prefetch data!使用多条CPU线程去预读取数据<ul><li>While the GPU are computing, a CPU thread will fetch the data for you.</li><li>A lot of frameworks implemented that for you because its a little bit painful!</li></ul></li></ul></li></ul></li><li><p><strong>Deep learning Frameworks</strong></p><ul><li>Its super fast moving!</li><li>Currently available frameworks:<ul><li>Tensorflow (Google)</li><li>Caffe (UC Berkeley)</li><li>Caffe2 (Facebook)</li><li>Torch (NYU / Facebook)</li><li>PyTorch (Facebook)</li><li>Theano (U monteral)</li><li>Paddle (Baidu)</li><li>CNTK (Microsoft)</li><li>MXNet (Amazon)</li></ul></li><li>The instructor thinks that you should focus on Tensorflow and PyTorch.</li><li>The point of deep learning frameworks:<ul><li>Easily build big computational graphs.方便地构建计算图</li><li>Easily compute gradients in computational graphs.方便地通过计算图计算梯度</li><li>Run it efficiently on GPU (cuDNN - cuBLAS)支持GPU</li></ul></li><li>Numpy doesn’t run on GPU.Numpy不能使用GPU</li><li>Most of the frameworks tries to be like NUMPY in the forward pass and then they compute the gradients for you.很多框架尽力去靠拢NUMPY，同时又能支持GPU</li></ul></li><li><p>**Tensorflow (Google)**静态架构</p><ul><li><p>Code are two parts:</p><ol><li>Define computational graph.定义好计算图</li><li>Run the graph and reuse it many times.运行计算图并多次使用</li></ol></li><li><p>Tensorflow uses a static graph architecture.静态的图结构</p></li><li><p>Tensorflow variables live in the graph. while the placeholders are feed each run.variable在计算图中生存，placeholders用于在计算图中占位，运行时填入数据</p></li><li><p>Global initializer function initializes the variables that lives in the graph.</p></li><li><p>Use predefined optimizers and losses.使用预先定义的优化器和损失函数</p></li><li><p>You can make a full layers with layers.dense function.</p></li><li><p>Keras</p><p>(High level wrapper):</p><ul><li>Keras is a layer on top pf Tensorflow, makes common things easy to do.</li><li>So popular!</li><li>Trains a full deep NN in a few lines of codes.</li></ul></li><li><p>There are a lot high level wrappers:</p><ul><li>Keras</li><li>TFLearn</li><li>TensorLayer</li><li>tf.layers <code>#Ships with tensorflow</code></li><li>tf-Slim <code>#Ships with tensorflow</code></li><li>tf.contrib.learn <code>#Ships with tensorflow</code></li><li>Sonnet <code># New from deep mind</code></li></ul></li><li><p>Tensorflow has pretrained models that you can use while you are using transfer learning.迁移学习的时候可以使用多个预训练模型</p></li><li><p>Tensorboard adds logging to record loss, stats. Run server and get pretty graphs! Tensorboard添加了日志用于跟踪损失</p></li><li><p>It has distributed code if you want to split your graph on some nodes.</p></li><li><p>Tensorflow is actually inspired from Theano. It has the same inspirations and structure.</p></li></ul></li><li><p><strong>PyTorch (Facebook)</strong></p><ul><li>Has three layers of abstraction:<ul><li>Tensor: ndarraybut runs on GPU,Like numpy arrays in tensorflow<ul><li>Variable: Node in a computational graphs; stores data and gradient <code>#Like Tensor, Variable, Placeholders</code></li></ul></li><li>Module: A NN layer; may store state or learnable weights<code>#Like tf.layers in tensorflow</code></li></ul></li><li>In PyTorch the graphs runs in the same loop you are executing which makes it easier for debugging. This is called a dynamic graph.动态图架构，容易进行debugging</li><li>In PyTorch you can define your own autograd functions by writing forward and backward for tensors. Most of the times it will implemented for you.一般只用重写forward函数</li><li>Torch.nn is a high level api like keras in tensorflow. You can create the models and go on and on.<ul><li>You can define your own nn module!</li></ul></li><li>Also Pytorch contains optimizers like tensorflow.</li><li>It contains a data loader that wraps a Dataset and provides minbatches, shuffling and multithreading.自己编写数据加载器很重要</li><li>PyTorch contains the best and super easy to use pretrained models</li><li>PyTorch contains Visdom that are like tensorboard. but Tensorboard seems to be more powerful.提供Visdom用于记录日志</li><li>PyTorch is new and still evolving compared to Torch. Its still in beta state.</li><li>PyTorch is best for research.对于research更加常用</li></ul></li><li><p>Tensorflow builds the graph once, then run them many times (Called static graph)定义一次网络即可多次使用，有专门的保存方式,在工业中更常用</p></li><li><p>In each PyTorch iteration we build a new graph (Called dynamic graph)每次使用都要重新搭建网络,在研究中更常用</p></li><li><p>Tensorflow fold make dynamic graphs easier in Tensorflow through dynamic batching.</p></li><li><p>Dynamic graph applications include: recurrent networks and recursive networks.</p></li><li><p>Caffe2 uses static graphs and can train model in python also works on IOS and Android</p></li><li><p>Tensorflow/Caffe2 are used a lot in production especially on mobile.</p></li></ul><h3 id="cnn-architectures"><a class="markdownIt-Anchor" href="#cnn-architectures"></a> CNN architectures</h3><ul><li>Focuses on CNN architectures that won ImageNet competition since 2012.</li></ul><p><img src="/images/181008/43.png" alt=""></p><ul><li><p>These architectures includes: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">AlexNet</a>, <a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noopener">VGG</a>, <a href="https://research.google.com/pubs/pub43022.html" target="_blank" rel="noopener">GoogLeNet</a>, and <a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="noopener">ResNet</a>.</p></li><li><p>Also we will discuss some interesting architectures as we go.</p></li><li><p>The first ConvNet that was made was <a href="http://ieeexplore.ieee.org/document/726791/" target="_blank" rel="noopener">LeNet-5</a> architectures are:by Yann Lecun at 1998.</p><ul><li>Architecture are: <code>CONV-POOL-CONV-POOL-FC-FC-FC</code></li></ul><p><img src="/images/181008/02.jpg" alt=""></p></li><li><p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener"><strong>AlexNet</strong></a> (2012):</p><ul><li>ConvNet that started the evolution and wins the ImageNet at 2012.</li><li>Architecture are: <code>CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MAXPOOL3-FC6-FC7-FC8</code></li><li>Contains exactly <strong>8</strong> layers the first 5 are Convolutional and the last 3 are fully connected layers.</li><li>Some other details:<ul><li>First use of RELU.</li><li>Norm layers but not used any more.</li><li>heavy data augmentation</li><li>Dropout <code>0.5</code></li><li>batch size <code>128</code></li><li>SGD momentum <code>0.9</code></li><li>Learning rate <code>1e-2</code> reduce by 10 at some iterations</li><li>7 CNN ensembles!</li></ul></li><li>AlexNet was trained on GTX 580 GPU with only 3 GB which wasn’t enough to train in one machine so they have spread the feature maps in half. The first AlexNet was distributed!</li><li>Its still used in transfer learning in a lot of tasks.</li><li>Total number of parameters are <code>60 million</code></li></ul></li><li><p><a href="https://arxiv.org/pdf/1409.1556" target="_blank" rel="noopener"><strong>VGGNet</strong></a> (2014) (Oxford)</p><ul><li>Deeper network with more layers.</li><li>Contains 19 layers.</li><li>Won on 2014 with GoogleNet with error 7.3%</li><li>Smaller filters with deeper layers.</li><li>The great advantage of VGG was the insight that multiple 3 × 3 convolution in sequence can emulate the effect of larger receptive fields, for examples 5 × 5 and 7 × 7.</li><li>Used the simple 3 x 3 Conv all through the network.</li></ul><p><img src="/images/181008/03.png" alt=""></p><ul><li>Has a similar details in training like AlexNet. Like using momentum and dropout.</li><li>在卷积神经网络中，感受野的定义是 卷积神经网络每一层输出的特征图（feature map）上的像素点在<strong>原始图像</strong>上映射的区域大小，即每一层feature的感受野都是对于原始输入图像而言的，而不是上一层的输入,但是在计算过程中，需要逐层计算该层在往上的每一层的感受野;公式 (N-1)_RF = f(N_RF, stride, ksize) = (N_RF - 1) * stride(convN) +ksize(convN)，其中，RF是感受野。N_RF和RF有点像，<strong>N代表 neighbour</strong>，指的是第n层的 a feature在n-1层的RF,显然第N层feature map在第N层的RF=1,在N-1层的RF=ksize__convN</li></ul></li></ul></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="http://aier02.com/2018/10/08/CS231n_2017_Summary/" title="CS231n-2017-Summary" target="_blank" rel="external">http://aier02.com/2018/10/08/CS231n_2017_Summary/</a></li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://github.com/aier02" target="_blank" class="img-burn thumb-sm visible-lg"><img src="/images/avatar.jpg" class="img-rounded w-full" alt=""></a></div><div class="media-body"><h3 class="media-heading"><a href="https://github.com/aier02" target="_blank"><span class="text-dark">Amadeus</span><small class="ml-1x">cv爱好者</small></a></h3><div>个人简介。</div></div></figure></div></div></div></article><section id="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="next"><a href="/2018/10/05/optimization/" title="optimization"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li><li class="toggle-toc"><a class="toggle-btn collapsed" data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button"><span>[&nbsp;</span><span>文章目录</span> <i class="text-collapsed icon icon-anchor"></i> <i class="text-in icon icon-close"></i> <span>]</span></a></li></ul><div class="bar-right"><div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div></div></div></nav></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="https://github.com/aier02" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul><div class="copyright"><div class="publishby">Theme by <a href="https://github.com/cofess" target="_blank">cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.</div></div></footer><script src="https://cdn.bootcss.com/jquery/1.12.4/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/js/plugin.min.js"></script><script src="/js/application.js"></script><script>!function(T){var N={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(未命名)"},ROOT_URL:"/",CONTENT_URL:"/content.json"};T.INSIGHT_CONFIG=N}(window)</script><script src="/js/insight.js"></script><script defer>var disqus_config=function(){this.page.url="http://aier02.com/2018/10/08/CS231n_2017_Summary/",this.page.identifier="CS231n_2017_Summary"};!function(){var e=document,t=e.createElement("script");t.src="//aier02-1.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)}()</script></body></html><!-- rebuild by neat -->