<!-- build time:Sun Dec 16 2018 19:55:29 GMT+0800 (GMT+08:00) --><!DOCTYPE html><html lang="zh"><head><meta charset="utf-8"><meta name="referrer" content="never"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,minimum-scale=1,user-scalable=no,minimal-ui"><meta name="renderer" content="webkit"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><meta name="format-detection" content="telephone=no,email=no,adress=no"><meta name="theme-color" content="#000000"><meta http-equiv="window-target" content="_top"><title>pytorch cookbook U2&amp;U3 | Amadeus</title><meta name="description" content="pytorch-cookbook 第二章函数名后带下划线会修改函数本身如y.add_(x)会直接修改ypytorch的tensor和numpy的对象共享内存，两者同时改变;对于tensor不支持的操作，可以先转为numpy进行操作在转为tensor（tensor支持gpu）1234a=t.ones(5)b = a.numpy() # Tensor -&amp;gt; Numpya = np.ones(5"><meta name="keywords" content="basic knowledge,pytorch cookbook"><meta property="og:type" content="article"><meta property="og:title" content="pytorch cookbook U2&amp;U3"><meta property="og:url" content="http://aier02.com/2018/10/02/pytorch_cookbook-U2&U3/index.html"><meta property="og:site_name" content="Aier02"><meta property="og:description" content="pytorch-cookbook 第二章函数名后带下划线会修改函数本身如y.add_(x)会直接修改ypytorch的tensor和numpy的对象共享内存，两者同时改变;对于tensor不支持的操作，可以先转为numpy进行操作在转为tensor（tensor支持gpu）1234a=t.ones(5)b = a.numpy() # Tensor -&amp;gt; Numpya = np.ones(5"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2018-11-07T07:21:58.471Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="pytorch cookbook U2&amp;U3"><meta name="twitter:description" content="pytorch-cookbook 第二章函数名后带下划线会修改函数本身如y.add_(x)会直接修改ypytorch的tensor和numpy的对象共享内存，两者同时改变;对于tensor不支持的操作，可以先转为numpy进行操作在转为tensor（tensor支持gpu）1234a=t.ones(5)b = a.numpy() # Tensor -&amp;gt; Numpya = np.ones(5"><link rel="canonical" href="http://aier02.com/2018/10/02/pytorch_cookbook-U2&U3/index.html"><link rel="alternate" href="/atom.xml" title="Aier02" type="application/atom+xml"><link rel="icon" href="/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/style.css"></head><body class="main-center theme-black" itemscope itemtype="http://schema.org/WebPage"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="slimContent"><div class="navbar-header"><div class="profile-block text-center"><a id="avatar" href="https://github.com/aier02" target="_blank"><img class="img-circle img-rotate" src="/images/avatar.jpg" width="200" height="200"></a><h2 id="name" class="hidden-xs hidden-sm">Amadeus</h2><h3 id="title" class="hidden-xs hidden-sm hidden-md">cv &amp; dl</h3><small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Guangdong, China</small></div><div class="search" id="search-form-wrap"><form class="search-form sidebar-form"><div class="input-group"><input type="text" class="search-form-input form-control" placeholder="搜索"> <span class="input-group-btn"><button type="submit" class="search-form-submit btn btn-flat" onclick="return!1"><i class="icon icon-search"></i></button></span></div></form><div class="ins-search"><div class="ins-search-mask"></div><div class="ins-search-container"><div class="ins-input-wrapper"><input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech> <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button></div><div class="ins-section-wrapper"><div class="ins-section-container"></div></div></div></div></div><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false"><span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span></button></div><nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation"><ul class="nav navbar-nav main-nav"><li class="menu-item menu-item-home"><a href="/."><i class="icon icon-home-fill"></i> <span class="menu-title">首页</span></a></li><li class="menu-item menu-item-archives"><a href="/archives"><i class="icon icon-archives-fill"></i> <span class="menu-title">归档</span></a></li><li class="menu-item menu-item-categories"><a href="/categories"><i class="icon icon-folder"></i> <span class="menu-title">分类</span></a></li><li class="menu-item menu-item-tags"><a href="/tags"><i class="icon icon-tags"></i> <span class="menu-title">标签</span></a></li><li class="menu-item menu-item-repository"><a href="/repository"><i class="icon icon-project"></i> <span class="menu-title">项目</span></a></li><li class="menu-item menu-item-books"><a href="/books"><i class="icon icon-book-fill"></i> <span class="menu-title">书单</span></a></li><li class="menu-item menu-item-links"><a href="/links"><i class="icon icon-friendship"></i> <span class="menu-title">友链</span></a></li><li class="menu-item menu-item-about"><a href="/about"><i class="icon icon-cup-fill"></i> <span class="menu-title">关于</span></a></li></ul><ul class="social-links"><li><a href="https://github.com/aier02" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul></nav></div></header><aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><div class="widget"><h3 class="widget-title">公告</h3><div class="widget-body"><div id="board"><div class="content"><p>欢迎交流与分享经验!</p></div></div></div></div><div class="widget"><h3 class="widget-title">分类</h3><div class="widget-body"><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Kaggle/">Kaggle</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/algorithms/">algorithms</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/blog/">blog</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs229n/">cs229n</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cs231n/">cs231n</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/leetcode/">leetcode</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/">pytorch</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/statistical-learning-method/">statistical learning method</a><span class="category-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签</h3><div class="widget-body"><ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/CNN/">CNN</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ChexNet/">ChexNet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/EDA/">EDA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Github-page/">Github page</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/">Hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ResNet/">ResNet</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/array/">array</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/backpropagation/">backpropagation</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/backtracking/">backtracking</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/basic-knowledge/">basic knowledge</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/chain-rule/">chain rule</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/chest-X-ray/">chest X-ray</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/cv/">cv</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/demo/">demo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/experience/">experience</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hash-table/">hash table</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/image-classification/">image classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/introduction/">introduction</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jikecloud/">jikecloud</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/keras/">keras</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/leetcode/">leetcode</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linear-classification/">linear classification</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linked-list/">linked list</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/namesilo/">namesilo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/notebook/">notebook</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/paper-reading/">paper reading</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/pytorch-cookbook/">pytorch cookbook</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/segmentation/">segmentation</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/stack/">stack</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/string/">string</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/two-pointers/">two pointers</a><span class="tag-list-count">1</span></li></ul></div></div><div class="widget"><h3 class="widget-title">标签云</h3><div class="widget-body tagcloud"><a href="/tags/CNN/" style="font-size:13px">CNN</a> <a href="/tags/ChexNet/" style="font-size:13px">ChexNet</a> <a href="/tags/EDA/" style="font-size:13px">EDA</a> <a href="/tags/Github-page/" style="font-size:13px">Github page</a> <a href="/tags/Hexo/" style="font-size:13px">Hexo</a> <a href="/tags/ResNet/" style="font-size:13px">ResNet</a> <a href="/tags/SVM/" style="font-size:13px">SVM</a> <a href="/tags/array/" style="font-size:13px">array</a> <a href="/tags/backpropagation/" style="font-size:13px">backpropagation</a> <a href="/tags/backtracking/" style="font-size:13.33px">backtracking</a> <a href="/tags/basic-knowledge/" style="font-size:13.67px">basic knowledge</a> <a href="/tags/chain-rule/" style="font-size:13px">chain rule</a> <a href="/tags/chest-X-ray/" style="font-size:13px">chest X-ray</a> <a href="/tags/cv/" style="font-size:13.33px">cv</a> <a href="/tags/demo/" style="font-size:13px">demo</a> <a href="/tags/experience/" style="font-size:13px">experience</a> <a href="/tags/hash-table/" style="font-size:13px">hash table</a> <a href="/tags/image-classification/" style="font-size:13px">image classification</a> <a href="/tags/introduction/" style="font-size:13px">introduction</a> <a href="/tags/jikecloud/" style="font-size:13px">jikecloud</a> <a href="/tags/keras/" style="font-size:13px">keras</a> <a href="/tags/leetcode/" style="font-size:13.33px">leetcode</a> <a href="/tags/linear-classification/" style="font-size:13px">linear classification</a> <a href="/tags/linked-list/" style="font-size:13.33px">linked list</a> <a href="/tags/namesilo/" style="font-size:13px">namesilo</a> <a href="/tags/notebook/" style="font-size:14px">notebook</a> <a href="/tags/paper-reading/" style="font-size:13.33px">paper reading</a> <a href="/tags/pytorch-cookbook/" style="font-size:13px">pytorch cookbook</a> <a href="/tags/segmentation/" style="font-size:13.67px">segmentation</a> <a href="/tags/stack/" style="font-size:13px">stack</a> <a href="/tags/string/" style="font-size:13px">string</a> <a href="/tags/two-pointers/" style="font-size:13px">two pointers</a></div></div><div class="widget"><h3 class="widget-title">归档</h3><div class="widget-body"><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">十二月 2018</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">十月 2018</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">九月 2018</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/08/">八月 2018</a><span class="archive-list-count">2</span></li></ul></div></div><div class="widget"><h3 class="widget-title">最新文章</h3><div class="widget-body"><ul class="recent-post-list list-unstyled no-thumbnail"><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/cs229n/">cs229n</a></p><p class="item-title"><a href="/2018/12/16/logistic_regression/" class="title">logistic regression</a></p><p class="item-date"><time datetime="2018-12-16T11:54:52.916Z" itemprop="datePublished">2018-12-16</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/algorithms/">algorithms</a></p><p class="item-title"><a href="/2018/11/23/leetcode_linked_list/" class="title">leetcode linked list</a></p><p class="item-date"><time datetime="2018-11-23T02:00:17.093Z" itemprop="datePublished">2018-11-23</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/algorithms/">algorithms</a></p><p class="item-title"><a href="/2018/11/23/leetcode_hash_table/" class="title">leetcode hash table</a></p><p class="item-date"><time datetime="2018-11-23T01:36:36.136Z" itemprop="datePublished">2018-11-23</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/paper/">paper</a></p><p class="item-title"><a href="/2018/11/12/VGG/" class="title">VGG</a></p><p class="item-date"><time datetime="2018-11-12T03:28:24.262Z" itemprop="datePublished">2018-11-12</time></p></div></li><li><div class="item-inner"><p class="item-category"><a class="category-link" href="/categories/paper/">paper</a></p><p class="item-title"><a href="/2018/10/30/AlexNet/" class="title">AlexNet</a></p><p class="item-date"><time datetime="2018-10-30T02:56:58.917Z" itemprop="datePublished">2018-10-30</time></p></div></li></ul></div></div></div></aside><aside class="sidebar sidebar-toc collapse" id="collapseToc" itemscope itemtype="http://schema.org/WPSideBar"><div class="slimContent"><nav id="toc" class="article-toc"><h3 class="toc-title">文章目录</h3><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch-cookbook"><span class="toc-number">1.</span> <span class="toc-text">pytorch-cookbook</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#第二章"><span class="toc-number">1.1.</span> <span class="toc-text">第二章</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#第三章"><span class="toc-number">1.2.</span> <span class="toc-text">第三章</span></a></li></ol></li></ol></nav></div></aside><main class="main" role="main"><div class="content"><article id="post-pytorch_cookbook-U2&amp;U3" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting"><div class="article-header"><h1 class="article-title" itemprop="name">pytorch cookbook U2&amp;U3</h1><div class="article-meta"><span class="article-date"><i class="icon icon-calendar-check"></i> <a href="/2018/10/02/pytorch_cookbook-U2&U3/" class="article-date"><time datetime="2018-10-02T08:27:32.398Z" itemprop="datePublished">2018-10-02</time></a></span> <span class="article-category"><i class="icon icon-folder"></i> <a class="article-category-link" href="/categories/pytorch/">pytorch</a></span> <span class="article-tag"><i class="icon icon-tags"></i> <a class="article-tag-link" href="/tags/basic-knowledge/">basic knowledge</a>, <a class="article-tag-link" href="/tags/pytorch-cookbook/">pytorch cookbook</a></span> <span class="post-comment"><i class="icon icon-comment"></i> <a href="/2018/10/02/pytorch_cookbook-U2&U3/#comments" class="article-comment-link">评论</a></span></div></div><div class="article-entry markdown-body" itemprop="articleBody"><h1 id="pytorch-cookbook"><a class="markdownIt-Anchor" href="#pytorch-cookbook"></a> pytorch-cookbook</h1><h2 id="第二章"><a class="markdownIt-Anchor" href="#第二章"></a> 第二章</h2><ul><li><p>函数名后带下划线会修改函数本身如y.add_(x)会直接修改y</p></li><li><p>pytorch的tensor和numpy的对象共享内存，两者同时改变;对于tensor不支持的操作，可以先转为numpy进行操作在转为tensor（tensor支持gpu）</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a=t.ones(5)</span><br><span class="line">b = a.numpy() # Tensor -&gt; Numpy</span><br><span class="line">a = np.ones(5)</span><br><span class="line">b = t.from_numpy(a) # Numpy-&gt;Tensor</span><br></pre></td></tr></table></figure><ul><li>tensor[idx]得到的为0-dim的tensor，scalar.item()获取tensor的单个元素对象</li><li>t.Tensor(5,3)创建5行3列的tensor，t.tensor([3,4])创建包含3，4两个元素的tensor</li><li>t.tensor()会进行数据拷贝，新的tensor和旧的不共享内存，而torch.from_numpy（）或者tensor.detach()则相反</li><li>使用gpu</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = t.device(&quot;cuda:0&quot; if t.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line">x = x.to(device)</span><br><span class="line">y = y.to(device)</span><br></pre></td></tr></table></figure><ul><li>autograd: 自动微分;要想使得Tensor使用autograd功能，只需要设置<code>tensor.requries_grad=True</code>.如：x = t.ones(2, 2, requires_grad=True)</li><li>注意：<code>grad</code>在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以反向传播之前需把梯度清零。# 以下划线结束的函数是inplace操作，会修改自身的值，就像add__</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.grad.data.zero_()</span><br></pre></td></tr></table></figure><ul><li>一起求导的过程示例</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x=t.ones(2,2,requires_grad=True)#生成tensor</span><br><span class="line">y=x.sum()#定义表达式</span><br><span class="line">y.grad_fn#查看求导函数</span><br><span class="line">y.backward()#back propagation</span><br><span class="line">x.grad#查看y对x的导数</span><br><span class="line">x.grad.data_zero_()#清空导数缓存空间</span><br></pre></td></tr></table></figure><ul><li>nerual network的定义主要是对torch.nn模块的使用,<br>定义网络时，需要继承<code>nn.Module</code>，并实现它的forward方法，把网络中具有可学习参数的层放在构造函数<code>__init__</code>中。如果某一层(如ReLU)不具有可学习的参数，则既可以放在构造函数中，也可以不放，但建议不放在其中，而在forward中使用<code>nn.functional</code>代替,forwar的输入和输出都是tensor,input = t.randn(1, 1, 32, 32),需要注意的是，torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用 <code>input.unsqueeze(0)</code>将batch_size设为１,size形式为nSamples x nChannels x height x weight</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">class Net(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        # nn.Module子类的函数必须在构造函数中执行父类的构造函数</span><br><span class="line">        # 下式等价于nn.Module.__init__(self)</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        </span><br><span class="line">        # 卷积层 &apos;1&apos;表示输入图片为单通道, &apos;6&apos;表示输出通道数，&apos;5&apos;表示卷积核为5*5</span><br><span class="line">        self.conv1 = nn.Conv2d(1, 6, 5) </span><br><span class="line">        # 卷积层</span><br><span class="line">        self.conv2 = nn.Conv2d(6, 16, 5) </span><br><span class="line">        # 仿射层/全连接层，y = Wx + b</span><br><span class="line">        self.fc1   = nn.Linear(16*5*5, 120) </span><br><span class="line">        self.fc2   = nn.Linear(120, 84)</span><br><span class="line">        self.fc3   = nn.Linear(84, 10)</span><br><span class="line"></span><br><span class="line">    def forward(self, x): </span><br><span class="line">        # 卷积 -&gt; 激活 -&gt; 池化 </span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))</span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), 2) </span><br><span class="line">        # reshape，‘-1’表示自适应</span><br><span class="line">        x = x.view(x.size()[0], -1) </span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)        </span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">print(net)</span><br></pre></td></tr></table></figure><ul><li>conv layer主要特征是局部连接和权重共享</li><li></li></ul><p>局部连接：每个神经元仅与输入神经元的一块区域连接，这块局部区域称作感受野（receptive field）。在图像卷积操作中，即神经元在空间维度（spatial dimension，即上图示例H和W所在的平面）是局部连接，但在深度上是全部连接。对于二维图像本身而言，也是局部像素关联较强。这种局部连接保证了学习后的过滤器能够对于局部的输入特征有最强的响应。局部连接的思想，也是受启发于生物学里面的视觉系统结构，视觉皮层的神经元就是局部接受信息的。<br>*<br>权重共享：计算同一个深度切片的神经元时采用的滤波器是共享的。例上图中计算o[:,:,0]的每个每个神经元的滤波器均相同，都为W0，这样可以很大程度上减少参数。共享权重在一定程度上讲是有意义的，例如图片的底层边缘特征与特征在图中的具体位置无关。但是在一些场景中是无意的，比如输入的图片是人脸，眼睛和头发位于不同的位置，希望在不同的位置学到不同的特征 。请注意权重只是对于同一深度切片的神经元是共享的，在卷积层，通常采用多组卷积核提取不同特征，即对应不同深度切片的特征，不同深度切片的神经元权重是不共享。另外，偏重对同一深度切片的所有神经元都是共享的。</p><ul><li>池化是非线性下采样的一种形式，主要作用是通过减少网络的参数来减小计算量，并且能够在一定程度上控制过拟合。</li><li>网络的可学习参数通过<code>net.parameters()</code>返回,<code>net.named_parameters</code>可同时返回可学习的参数及名称。</li><li>nn.MSELoss()实现均方误差，nn.CrossEntropyLoss()实现交叉熵损失</li><li>优化器更新参数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"> import torch.optim as optim</span><br><span class="line">	#新建一个优化器，指定要调整的参数和学习率</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr = 0.01)</span><br><span class="line"></span><br><span class="line">	# 在训练过程中</span><br><span class="line">	# 先梯度清零(与net.zero_grad()效果一样)</span><br><span class="line">optimizer.zero_grad() </span><br><span class="line"></span><br><span class="line">	# 计算损失</span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line">	#反向传播</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">	#更新参数</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><ul><li>数据加载和预处理：使用torchvision</li><li>示例：下面我们来尝试实现对CIFAR-10数据集的分类，步骤如下:</li></ul><ol><li>使用torchvision加载并预处理CIFAR-10数据集,得到dataset和dataloader</li><li>定义网络,继承nn.Module,init中写入可学习的参数函数，forward定义好前向传播的过程</li><li>定义损失函数和优化器，criterion和optimizer</li><li>训练网络并更新网络参数，在每个ephco中加载数据，传入net，算loss，loss.backward，optimizer.step</li><li>测试网络</li></ol><ul><li></li></ul><p>定义对数据的预处理:将两种转化合并一起；ToTensor()将shape为(H, W, C)的nump.ndarray或img转为shape为(C, H, W)的tensor，其将每一个数值归一化到[0,1]，其归一化方法比较简单，直接除以255即可，加入normalize则其作用就是先将输入归一化到(0,1)，再使用公式”(x-mean)/std”，将每个元素分布到(-1,1),函数normalize（std,mean）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">        transforms.ToTensor(), # 转为Tensor</span><br><span class="line">        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化</span><br><span class="line">                             ])</span><br></pre></td></tr></table></figure><ul><li>Dataset对象是一个数据集，可以按下标访问，返回形如(data, label)的数据。Dataloader是一个可迭代的对象，它将dataset返回的每一条数据拼接成一个batch，并提供多线程加速优化和数据打乱等操作。当程序对dataset的所有数据遍历完一遍之后，相应的对Dataloader也完成了一次迭代，先定义好dataset，然后定义dataloader对指定的dataset进行操作</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> # 训练集</span><br><span class="line">trainset = tv.datasets.CIFAR10(</span><br><span class="line">                    root=&apos;/home/cy/tmp/data/&apos;, </span><br><span class="line">                    train=True, </span><br><span class="line">                    download=True,</span><br><span class="line">                    transform=transform)</span><br><span class="line"></span><br><span class="line">trainloader = t.utils.data.DataLoader(</span><br><span class="line">                    trainset, </span><br><span class="line">                    batch_size=4,</span><br><span class="line">                    shuffle=True, </span><br><span class="line">                    num_workers=2)</span><br></pre></td></tr></table></figure><ul><li>进行normaliza的必要性：每个样本图像减去数据集图像的均值后除以方差，保证了所有图像的分布相似，使得model训练的时候更快的收敛.</li><li>训练网络的示例</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">t.set_num_threads(8)</span><br><span class="line">for epoch in range(2):  </span><br><span class="line">    </span><br><span class="line">    running_loss = 0.0</span><br><span class="line">    for i, data in enumerate(trainloader, 0):</span><br><span class="line">        </span><br><span class="line">        # 输入数据</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        </span><br><span class="line">        # 梯度清零</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        # forward + backward </span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()   </span><br><span class="line">        </span><br><span class="line">        # 更新参数 </span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        # 打印log信息</span><br><span class="line">        # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0]</span><br><span class="line">        running_loss += loss.item()</span><br><span class="line">        if i % 2000 == 1999: # 每2000个batch打印一下训练状态</span><br><span class="line">            print(&apos;[%d, %5d] loss: %.3f&apos; \</span><br><span class="line">                  % (epoch+1, i+1, running_loss / 2000))</span><br><span class="line">            running_loss = 0.0</span><br><span class="line">print(&apos;Finished Training&apos;)</span><br></pre></td></tr></table></figure><h2 id="第三章"><a class="markdownIt-Anchor" href="#第三章"></a> 第三章</h2><ul><li>表3-1: 常见新建tensor的方法</li></ul><table><thead><tr><th style="text-align:center">函数</th><th style="text-align:center">功能</th></tr></thead><tbody><tr><td style="text-align:center">Tensor(*sizes)</td><td style="text-align:center">基础构造函数</td></tr><tr><td style="text-align:center">tensor(data,)</td><td style="text-align:center">类似np.array的构造函数</td></tr><tr><td style="text-align:center">ones(*sizes)</td><td style="text-align:center">全1Tensor</td></tr><tr><td style="text-align:center">zeros(*sizes)</td><td style="text-align:center">全0Tensor</td></tr><tr><td style="text-align:center">eye(*sizes)</td><td style="text-align:center">对角线为1，其他为0</td></tr><tr><td style="text-align:center">arange(s,e,step</td><td style="text-align:center">从s到e，步长为step</td></tr><tr><td style="text-align:center">linspace(s,e,steps)</td><td style="text-align:center">从s到e，均匀切分成steps份</td></tr><tr><td style="text-align:center">rand/randn(*sizes)</td><td style="text-align:center">均匀/标准分布</td></tr><tr><td style="text-align:center">normal(mean,std)/uniform(from,to)</td><td style="text-align:center">正态分布/均匀分布</td></tr><tr><td style="text-align:center">randperm(m)</td><td style="text-align:center">随机排列</td></tr></tbody></table><ul><li>除了<code>tensor.size()</code>，还可以利用<code>tensor.shape</code>直接查看tensor的形状，<code>tensor.shape</code>等价于<code>tensor.size()</code></li><li>tensor = t.Tensor(1,2)创建了一个size为【1，2】的张量</li><li>vector = t.tensor([1, 2])创建了一个值为（1，2）的向量，size为2</li><li>scalar = t.tensor(3.14159) 创建了一个值为3.14159的标量，<br>size为【】,区别于size【0】，empty_tensor = t.tensor([])，size存在即为tensor,否则为scalar</li><li>通过<code>tensor.view</code>方法可以调整tensor的形状，但必须保证调整前后元素总数一致。<code>view</code>不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。<code>b = a.view(-1, 3)</code>当某一维为-1的时候，会自动计算它的大小,</li><li>torch.squeeze() 这个函数主要对数据的维度进行压缩，去掉维数为1的的维度，比如是一行或者一列这种，一个一行三列（1,3）的数去掉第一个维数为一的维度之后就变成（3）行。squeeze(a)就是将a中所有为1的维度删掉。不为1的维度没有影响。a.squeeze(N) 就是去掉a中指定的维数为一的维度。还有一种形式就是b=torch.squeeze(a，N) a中去掉指定的定的维数为一的维度。</li><li>torch.unsqueeze()这个函数主要是对数据维度进行扩充。给指定位置加上维数为一的维度，比如原本有个三行的数据（3），在0的位置加了一维就变成一行三列（1,3）。a.unsqueeze(N) 就是在a中指定位置N加上一个维数为1的维度。还有一种形式就是b=torch.unsqueeze(a，N) a就是在a中指定位置N加上一个维数为1的维度</li><li><code>resize</code>是另一种可用来调整<code>size</code>的方法，但与<code>view</code>不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存，当再次扩展时其值为当时缩小保存的值</li><li>tensor的索引操作和tensor共享内存，即更改其中一个，另一个也会更改。</li><li>a[None]:None类似于np.newaxis, 为a新增了一个轴；等价于a.view(1, a.shape[0], a.shape[1])</li><li>a &gt; 1 # 返回一个ByteTensor,即满足条件的位置值为1，否则为0</li><li>对tensor的任何索引操作仍是一个tensor，想要获取标准的python对象数值，需要调用<code>tensor.item()</code>, 这个方法只对包含一个元素的tensor适用</li></ul></div><div class="article-footer"><blockquote class="mt-2x"><ul class="post-copyright list-unstyled"><li class="post-copyright-link hidden-xs"><strong>本文链接：</strong> <a href="http://aier02.com/2018/10/02/pytorch_cookbook-U2&U3/" title="pytorch cookbook U2&amp;U3" target="_blank" rel="external">http://aier02.com/2018/10/02/pytorch_cookbook-U2&U3/</a></li></ul></blockquote><div class="panel panel-default panel-badger"><div class="panel-body"><figure class="media"><div class="media-left"><a href="https://github.com/aier02" target="_blank" class="img-burn thumb-sm visible-lg"><img src="/images/avatar.jpg" class="img-rounded w-full" alt=""></a></div><div class="media-body"><h3 class="media-heading"><a href="https://github.com/aier02" target="_blank"><span class="text-dark">Amadeus</span><small class="ml-1x">cv &amp; dl</small></a></h3><div>个人简介。</div></div></figure></div></div></div></article><section id="comments"><div id="vcomments"></div></section></div><nav class="bar bar-footer clearfix" data-stick-bottom><div class="bar-inner"><ul class="pager pull-left"><li class="prev"><a href="/2018/10/03/linear_classification/" title="Linear classification"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a></li><li class="next"><a href="/2018/10/02/rsna_summary/" title="first time in Kaggle-summary"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a></li><li class="toggle-toc"><a class="toggle-btn collapsed" data-toggle="collapse" href="#collapseToc" aria-expanded="false" title="文章目录" role="button"><span>[&nbsp;</span><span>文章目录</span> <i class="text-collapsed icon icon-anchor"></i> <i class="text-in icon icon-close"></i> <span>]</span></a></li></ul><div class="bar-right"><div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div></div></div></nav></main><footer class="footer" itemscope itemtype="http://schema.org/WPFooter"><ul class="social-links"><li><a href="https://github.com/aier02" target="_blank" title="Github" data-toggle="tooltip" data-placement="top"><i class="icon icon-github"></i></a></li><li><a href="/atom.xml" target="_blank" title="Rss" data-toggle="tooltip" data-placement="top"><i class="icon icon-rss"></i></a></li></ul><div class="copyright"><div class="publishby">Theme by <a href="https://github.com/cofess" target="_blank">cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.</div></div></footer><script src="https://cdn.bootcss.com/jquery/1.12.4/jquery.min.js"></script><script>window.jQuery||document.write('<script src="js/jquery.min.js"><\/script>')</script><script src="/js/plugin.min.js"></script><script src="/js/application.js"></script><script>!function(T){var N={TRANSLATION:{POSTS:"文章",PAGES:"页面",CATEGORIES:"分类",TAGS:"标签",UNTITLED:"(未命名)"},ROOT_URL:"/",CONTENT_URL:"/content.json"};T.INSIGHT_CONFIG=N}(window)</script><script src="/js/insight.js"></script><script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="//cdn.jsdelivr.net/npm/valine"></script><script type="text/javascript">var GUEST=["nick","mail","link"],meta="nick,mail,link";meta=meta.split(",").filter(function(e){return GUEST.indexOf(e)>-1}),new Valine({el:"#vcomments",verify:!1,notify:!1,appId:"EkgjNDuvPLC2P4JUvz9XLze6-gzGzoHsz",appKey:"mKLzHHOFc0SEcUIl3DvykWdG",placeholder:"Just go go",avatar:"mm",meta:meta,pageSize:"10",visitor:!1})</script></body></html><!-- rebuild by neat -->