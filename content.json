{"meta":{"title":"Aier02","subtitle":null,"description":null,"author":"易安明","url":"http://aier02.com"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2018-08-06T10:41:28.220Z","updated":"2018-08-04T16:46:32.021Z","comments":false,"path":"/404.html","permalink":"http://aier02.com//404.html","excerpt":"","text":""},{"title":"关于","date":"2022-01-03T10:06:33.396Z","updated":"2018-08-04T16:46:32.022Z","comments":false,"path":"about/index.html","permalink":"http://aier02.com/about/index.html","excerpt":"","text":"个人详细介绍"},{"title":"书单","date":"2018-08-05T02:38:10.217Z","updated":"2018-08-04T16:46:32.022Z","comments":false,"path":"books/index.html","permalink":"http://aier02.com/books/index.html","excerpt":"","text":""},{"title":"分类","date":"2018-08-05T02:38:10.227Z","updated":"2018-08-04T16:46:32.022Z","comments":false,"path":"categories/index.html","permalink":"http://aier02.com/categories/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2018-08-05T02:19:59.559Z","updated":"2018-08-04T16:46:32.023Z","comments":true,"path":"links/index.html","permalink":"http://aier02.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2018-08-05T02:19:10.292Z","updated":"2018-08-04T16:46:32.023Z","comments":false,"path":"repository/index.html","permalink":"http://aier02.com/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2018-08-05T02:38:10.236Z","updated":"2018-08-04T16:46:32.023Z","comments":false,"path":"tags/index.html","permalink":"http://aier02.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"2021 summary","slug":"summary_2021_1","date":"2022-01-09T13:38:55.302Z","updated":"2022-01-09T13:41:22.747Z","comments":true,"path":"2022/01/09/summary_2021_1/","link":"","permalink":"http://aier02.com/2022/01/09/summary_2021_1/","excerpt":"Here's something encrypted, password is required to continue reading.","text":"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 Hey, password is required here.","categories":[{"name":"personal summary","slug":"personal-summary","permalink":"http://aier02.com/categories/personal-summary/"}],"tags":[{"name":"plan","slug":"plan","permalink":"http://aier02.com/tags/plan/"}]},{"title":"inception","slug":"inception","date":"2019-01-15T08:23:32.427Z","updated":"2019-01-15T09:03:57.369Z","comments":true,"path":"2019/01/15/inception/","link":"","permalink":"http://aier02.com/2019/01/15/inception/","excerpt":"","text":"Inception 网络是 CNN 分类器发展史上一个重要的里程碑。在 Inception 出现之前，大部分流行 CNN 仅仅是把卷积层堆叠得越来越多，使网络越来越深，以此希望能够得到更好的性能。GoogLeNet 最大的特点就是使用了 Inception 模块，它的目的是设计一种具有优良局部拓扑结构的网络，即对输入图像并行地执行多个卷积运算或池化操作，并将所有输出结果拼接为一个非常深的特征图。因为 1x1,3x3 或 5x5 等不同的卷积运算与池化操作可以获得输入图像的不同信息，并行处理这些运算并结合所有结果将获得更好的图像表征 Inception v1问题：图像中突出部分的大小差别很大。例如，狗的图像可以是以下任意情况。每张图像中狗所占区域都是不同的，占据全图，图的中间部分，图的上方小部分。同一数据集中由于信息位置的差异巨大，为卷积操作选择的合适大小的卷积核也更加困难；信息分布更全局性的图像偏好大的卷积核，信息分布比较局部的图像偏好于小的卷积核。非常深的网络更容易过拟合，将梯度更新传输到整个网络十分困难。简单的堆叠较大的卷积层非常消耗计算资源解决方法：拓宽网络层的宽度而不是深度下图是「原始」Inception 模块。它使用 3 个不同大小的滤波器（1x1、3x3、5x5）对输入执行卷积操作，此外它还会执行最大池化。所有子层的输出最后会被级联起来，并传送至下一个 Inception 模块。为了降低算力成本，作者在 3x3 和 5x5 卷积层之前添加额外的 1x1 卷积层，来限制输入信道的数量。尽管添加额外的卷积操作似乎是反直觉的，但是 1x1 卷积比 5x5 卷积要廉价很多，而且输入信道数量减少也有利于降低算力成本。不过一定要注意，1x1 卷积是在最大池化层之后，而不是之前。 Inception v2问题：减少特征的表征性瓶颈。直观上来说，当卷积不会大幅度改变输入维度时，神经网络可能会执行地更好。过多地减少维度可能会造成信息的损失，这也称为「表征性瓶颈」。使用更优秀的因子分解方法，卷积才能在计算复杂度上更加高效。解决方案：将 5×5 的卷积分解为两个 3×3 的卷积运算以提升计算速度。尽管这有点违反直觉，但一个 5×5 的卷积在计算成本上是一个 3×3 卷积的 2.78 倍。所以叠加两个 3×3 卷积实际上在性能上会有所提升，如下图所示将 nxn 的卷积核尺寸分解为 1×n 和 n×1 两个卷积。例如，一个 3×3 的卷积等价于首先执行一个 1×3 的卷积再执行一个 3×1 的卷积。他们还发现这种方法在成本上要比单个 3×3 的卷积降低 33%.模块中的滤波器组被扩展（即变得更宽而不是更深），以解决表征性瓶颈。如果该模块没有被拓展宽度，而是变得更深，那么维度会过多减少，造成信息损失.","categories":[{"name":"cv","slug":"cv","permalink":"http://aier02.com/categories/cv/"}],"tags":[{"name":"blog","slug":"blog","permalink":"http://aier02.com/tags/blog/"}]},{"title":"BoW in cv","slug":"BoW_in_cv","date":"2019-01-12T02:59:03.404Z","updated":"2019-01-12T02:59:23.356Z","comments":true,"path":"2019/01/12/BoW_in_cv/","link":"","permalink":"http://aier02.com/2019/01/12/BoW_in_cv/","excerpt":"","text":"Bag of words词袋模型可以理解为直方图统计，都是对目标的频率统计，而没有序列信息；但和histogram不同的是，histogram统计的的是某个区间的频率，词袋模型统计的是words字典中每个单词出现的次数。比如现有两个文档12John likes to watch movies. Mary likes too.John also likes to watch football games.找出两个文档的并集，构建dictionary1&#123;\"John\":1, 'likes':2, \"to\":3, 'watch':4, 'movies':5, 'also':6, 'football':7, 'games':8, 'Mary':9, 'too':10&#125;则这两篇文档得到的BoW(刚开始用于自然语言处理和信息检索中的一种文档表示方法)12[1,2,1,1,1,0,0,0,1,1][1,1,1,1,0,1,1,1,0,0] BoW model in CV先提取图像集中的特征集合，然后使用聚类的方法得到若干类，利用这些类构建dictionary，相当于words，最后每个图像统计字典中word出现的频数作为输出向量，用于后续的检索、分类等操作。下图以提取图像的sift特征为例；SIFT的全称是Scale Invariant Feature Transform，尺度不变特征变换，由加拿大教授David G.Lowe提出的。SIFT特征对旋转、尺度缩放、亮度变化等保持不变性，是一种非常稳定的局部特征。之后在每一幅图像中统计sift特征点在dictionary上的频数分布，得到的向量就是该图像的BoW向量下图是图像集中包含人脸、自行车、吉他等。可以理解为每张图都是一个‘词袋’，包含了不同种类的数量不同的特征，然后统计该图中不同‘单词’的频数。","categories":[{"name":"cv","slug":"cv","permalink":"http://aier02.com/categories/cv/"}],"tags":[{"name":"blog","slug":"blog","permalink":"http://aier02.com/tags/blog/"}]},{"title":"gradient vanishing & exploding problem","slug":"gradient-vanishing-&-exploding-problem","date":"2019-01-11T07:33:27.121Z","updated":"2019-01-11T07:33:27.121Z","comments":true,"path":"2019/01/11/gradient-vanishing-&-exploding-problem/","link":"","permalink":"http://aier02.com/2019/01/11/gradient-vanishing-&-exploding-problem/","excerpt":"","text":"在面senetime的时候由resnet为何能解决随着网络加深而不再导致准确率下降的问题引出了梯度的问题，发现自己的对梯度消失和梯度爆炸问题认识非常有限，仅局限于激活函数的影响，也就是cs231n介绍激活函数的时候所介绍的饱和问题，但是显然若使用Relu函数不存在这种问题，并且resnet的巧妙之处也不是激活函数的创新；这里在查找了一些blog后进行个人的总结。","categories":[{"name":"dl","slug":"dl","permalink":"http://aier02.com/categories/dl/"}],"tags":[{"name":"gradient","slug":"gradient","permalink":"http://aier02.com/tags/gradient/"}]},{"title":"LSTMs","slug":"LSTMs","date":"2019-01-11T07:33:13.044Z","updated":"2019-01-11T08:59:06.676Z","comments":true,"path":"2019/01/11/LSTMs/","link":"","permalink":"http://aier02.com/2019/01/11/LSTMs/","excerpt":"","text":"毕设任务暂定为video classification，一种实现方式使用到了LSTM神经网络，先做简单的入门准备。 Recurrent Neural NetworksRNN(循环神经网络)的提出是传统的CNN难以应对tmporal context的情况下提出的，他的目的就是针对时许关系，把上一个时间的网络输出联合当前时间的输入作为新的当前时间的输入，以实现利用上个时间的信息。未展开的RNN结构展开的RNN结构一般RNN的问题在于伴随着gap的变大，网络的输出与之前的较久的输出的关系不断减弱，使得后面的网络无法学习到较早的知识，即存在长时间的依赖问题。 LSTM NetworksLong Short Term Memory networks的提出就是针对普通RNN固有的记忆时间短的问题，是一种特殊的RNN，善于解决长时间依赖问题，LSTM默认的行为时记住长时间的信息，所有的RNN都具有重复nn的链式结构标准RNN的链式结构LSTM的链式结构上图各种标示的意义黄色的矩形表示用于学习的神经网络，粉色的原点表示逐点的运算，横箭头表示向量的操作，两条线的合箭头表示串联，分箭头表示复制了该向量，并分别用于不同的部分。 The Core Idea Behind LSTMsLSTM中的关键时cell state，下图中顶部的水平线即为cell state的传送路径，用于信息传送,他能直接贯通整个LSTM；LSTM中用gate实现对cell state的信息移除或者添加，gate是一条有选择性的信息通路，它由sigmoid神经网络层和一个逐点乘法运算组成。通过sigmoid决定要通过的程度，与cell state相乘得到想要控制通过的cell state部分s型曲线的输出为[0,1]，0表示全部信息都不通过，相反1为全部信息通过。一个LSTM中有3个这样的gate，用于控制和保护cell state. Step-by-Step LSTM Walk ThroughLSTM中的第一步是决定哪些信息需要从cell state中移除，利用sigmoid layer实现这一操作，即通过‘forget gate layer’，控制信息是否通过，利用ht−1h_{t-1}h​t−1​​和xtx_tx​t​​的信息输出0到1，0表示全部‘忘记’，1表示‘记住’上一次全部的cell state.2.第二步是要决定哪些新的信息需要保存在cell state中，包括两个部分：input gate layer：一个sigmoid layer，决定了需要更新哪些值。Tanh layer:创建新的候选cell state向量3.第三步综合之前两步决定要遗忘的信息和要更新的信息，更新本次的cell state，新的cell state由两部分组成：ft∗ct−1f_t*c_{t-1}f​t​​∗c​t−1​​上一次的cell state 乘以遗忘系数得到目前需要的旧cell state 的信息程度。it∗ct‾i_t*\\overline{c_t}i​t​​∗​c​t​​​​​要更新的信息值对应的index乘以候选的cell state得到本次要更新后的候选cell state4.最后决定输出，先通过一个sigmoid layer决定cell state哪些部分需要输出(该操作和第三步的决定cell state哪些部分需要更新的操作一致)，同时将之前3步算出来的cell state通过tanh layer(压缩输出到[-1,1])，然后tanh layer 得到的值和sigmoid layer 得到的值进行相乘，得到最后要输出的部分。 Variants on Long Short Term Memory上述为正常的LSTM结构，但是在不同情况下可能存在不同LSTM，下面分别描述不同的变体。1.gate layer也关注cell state,即cell state也作为所有gate的输入，术语上说是给gate layer添加一个peephole2.直接使用成对的forget gate 和 input gate，同时决定遗忘的部分和更新的部分；而不是分别去决定哪些需要忘记和哪些需要添加。比较特别的变体是Gated Recurrent Unit，将forget gate和input gate合并为了一个update gate,同时合并了cell state 和hidden state","categories":[{"name":"cv","slug":"cv","permalink":"http://aier02.com/categories/cv/"}],"tags":[{"name":"blog","slug":"blog","permalink":"http://aier02.com/tags/blog/"}]},{"title":"epoch & iteration","slug":"epoch & iteration","date":"2019-01-10T11:15:52.935Z","updated":"2019-01-11T05:16:34.023Z","comments":true,"path":"2019/01/10/epoch & iteration/","link":"","permalink":"http://aier02.com/2019/01/10/epoch & iteration/","excerpt":"","text":"再使用pytorch进行训练时，经常会看到batch_size,iteration,step,epoch这几个词，他们的意思和关系分别如下：epoch:一个epoch表示所有训练的样本运算学习一遍iteration/step：表示每运行一个iteration/step，更新一次参数权重，即进行一次学习，每进行一次更新都需要batch_size个样本进行运算学习，根据运算的结果更新一次参数；对应训练时为一张卡一个进程读取了batch_size张图片，froward/backward一次后，学习了一次参数，计作一次stepbatch_size：一次参数更新运算所需的样本数量，即一张卡中读取的sample数量在多卡多机训练时，每进行一次step后，都要重新加载数据，即使用train_loader重新读取一个batch到GPU上。","categories":[{"name":"cv","slug":"cv","permalink":"http://aier02.com/categories/cv/"}],"tags":[{"name":"pytorch","slug":"pytorch","permalink":"http://aier02.com/tags/pytorch/"}]},{"title":"BN","slug":"BN","date":"2019-01-09T11:45:40.264Z","updated":"2019-01-09T11:45:41.115Z","comments":true,"path":"2019/01/09/BN/","link":"","permalink":"http://aier02.com/2019/01/09/BN/","excerpt":"","text":"L2正则化通常对稀疏的有尖峰的权重响亮施加比较大的惩罚，而偏向于均匀的参数，这样做的效果时鼓励神经但愿利用上层的所有输入，而不是部分输入，所以L2正则项加入之后，权重的绝对值大小都会整体倾向于减少，尤其不会出现特别大的值，即网络偏向于学习比较小的权重，所以L2正则化在深度学习中又叫做‘权重衰减’(weight decay),在机器学习中又叫做‘岭回归’(rifge regression)BN的第四步做的尺度变换和偏移，由于归一化后xix_ix​i​​基本被限制在了正态分布下，使得网络的表达能力下降，引入两个参数，使得网络在训练时自己进行学习。数据被压缩到中心区域，对于大多数激活函数而言，中间区域的梯度都是最大的或者说是有梯度的，是变化敏感区域，这样可以加快收敛速度。在训练时，对同一批次的数据均值和方差进行求解，进而进行归一化操作；而在预测的时候，所使用的均值和方差来源于训练集，比如在训练时，我们记录下每个batch的均值和方差，待训练完毕后，求整个训练样本的均值和方差的期望，作为预测时进行BN的均值和方差。测试阶段使用的BN公式为：关于BN层使用的位置，在CNN中一般是作用于激活函数之前。若遇到梯度爆炸或者收敛速度很慢时可以考虑使用BN。","categories":[{"name":"cv","slug":"cv","permalink":"http://aier02.com/categories/cv/"}],"tags":[{"name":"regularization","slug":"regularization","permalink":"http://aier02.com/tags/regularization/"}]},{"title":"pytorch argparser","slug":"pytorch_argparser","date":"2019-01-07T07:27:16.595Z","updated":"2019-01-10T11:12:26.691Z","comments":true,"path":"2019/01/07/pytorch_argparser/","link":"","permalink":"http://aier02.com/2019/01/07/pytorch_argparser/","excerpt":"","text":"argparse 是在pytorch中用于解析命令行参数和选项的标准模块，在编写main脚本时，常常有很多路径、选项等参数需要确定，为了提高代码的健壮性，需要提高命令选项用于修改main中的参数，而不至于为了修改参数而要打开源代码进行。 一般步骤import argparse 引用模块parser=argparse.ArgumentParser(description=‘介绍该解释器’) 创建一个parser对象parser.add_argument() 向解析器对象添加要关注的命令行参数和选项，一个add对应一个参数parser.parse_args() 调用parse_args方法进行解析，即可使用###add_argument(name or flags…[, action][nargs],[ const],[ default],[ type], choices],[ required],[help],[ metavar],[dest])其中：name or flags：命令行参数名或者选项，如上面的address或者-p,–port.其中命令行参数如果没给定，且没有设置defualt，则出错。但是如果是选项的话，则设置为Noneaction:action=‘store_true’,表示若出现name命令参数，则设置该参数为True，若无出现，则默认为False，反之action=“store_false”，表示若出现则设置为False,否则默认为True.nargs：命令行参数的个数，一般使用通配符表示，其中，’?‘表示只用一个，’*‘表示0到多个，’+'表示至少一个default：默认值type：参数的类型，默认是字符串string类型，还有float、int等类型help：和ArgumentParser方法中的参数作用相似，出现的场合也一致 注意args=parser.parse_args()时，比如parser.add_argument(’–load-path’,default=’’,type=str),则运行了python命令–load-path='example_file’后，在py文件中读取该参数时是通过args.load_path，注意不是load-path，模块已经把中间的-换成了下滑线。","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://aier02.com/categories/pytorch/"}],"tags":[{"name":"sensetime","slug":"sensetime","permalink":"http://aier02.com/tags/sensetime/"},{"name":"basic knowledge","slug":"basic-knowledge","permalink":"http://aier02.com/tags/basic-knowledge/"}]},{"title":"label smoothing","slug":"label-smoothing","date":"2019-01-07T03:44:48.760Z","updated":"2019-01-07T03:44:48.760Z","comments":true,"path":"2019/01/07/label-smoothing/","link":"","permalink":"http://aier02.com/2019/01/07/label-smoothing/","excerpt":"","text":"由于训练的样本通常会存在小量的错误标签，这些错误的标签会影响预测的结果，为了减少模型对训练集的依赖，增强模型的泛化能力，采用标签平滑的思路进行解决：在训练时假设标签可能存在错误，避免过分地相信模型的标签，“告诉”模型样本的标签不一定正确，则训练出来的模型对于少量的样本错误就有“免疫力”。在二分类任务中，训练样本为(xi,yix_i,y_ix​i​​,y​i​​),yiy_iy​i​​为样本标签，即0或者1，而在训练中每次迭代时，并不直接将(xi,yi)(x_i,y_i)(x​i​​,y​i​​)放入训练中集中，而是设置一个错误率ϵ\\epsilonϵ,以1−ϵ1-\\epsilon1−ϵ的概率将样本直接代入，以ϵ\\epsilonϵ的概率代入(xi,1−yi)(x_i,1-y_i)(x​i​​,1−y​i​​)，则模型在训练时，既有正确的标签输入，又有错误的标签输入，这样训练出来的模型不会“全力匹配”每一个标签，而只是在一定程度上匹配，因此出现错误标签时，模型受到的影响就会降低。当采用交叉熵来描述损失函数时，对于每一个样本而言，标签变换成了yi={ϵ,yi=01−ϵ,yi=1y_i = \\begin{cases} \\epsilon, &amp; y_i=0\\\\ 1-\\epsilon, &amp; y_i = 1 \\end{cases}y​i​​={​ϵ,​1−ϵ,​​​y​i​​=0​y​i​​=1​​即当标签为0时，不把0直接代入训练，而是替换成一个较小的数ϵ\\epsilonϵ,同理，当标签为1时，替换成1−ϵ1-\\epsilon1−ϵ。所谓平滑即两个极端的值0和1替换成不那么极端的值；在多分类任务中，仍然假设标签值在一定概率下不变，以一定概率变为其他值，若假设均匀分布，则把所有的标签1变为1−ϵ1-\\epsilon1−ϵ，所有的标签0变为ϵk−1\\frac \\epsilon {k-1}​k−1​​ϵ​​.","categories":[{"name":"cv","slug":"cv","permalink":"http://aier02.com/categories/cv/"}],"tags":[{"name":"regularization","slug":"regularization","permalink":"http://aier02.com/tags/regularization/"},{"name":"sensetime","slug":"sensetime","permalink":"http://aier02.com/tags/sensetime/"}]},{"title":"independent component analysis","slug":"independent_component","date":"2018-12-16T12:42:23.350Z","updated":"2018-12-16T14:59:53.745Z","comments":true,"path":"2018/12/16/independent_component/","link":"","permalink":"http://aier02.com/2018/12/16/independent_component/","excerpt":"","text":"","categories":[{"name":"cs229n","slug":"cs229n","permalink":"http://aier02.com/categories/cs229n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"singular value decomposition","slug":"singular_value_decomposit","date":"2018-12-16T12:41:34.425Z","updated":"2018-12-16T15:00:01.717Z","comments":true,"path":"2018/12/16/singular_value_decomposit/","link":"","permalink":"http://aier02.com/2018/12/16/singular_value_decomposit/","excerpt":"","text":"","categories":[{"name":"cs229n","slug":"cs229n","permalink":"http://aier02.com/categories/cs229n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"principal component analysis","slug":"principal_component","date":"2018-12-16T12:40:51.673Z","updated":"2018-12-16T15:00:11.897Z","comments":true,"path":"2018/12/16/principal_component/","link":"","permalink":"http://aier02.com/2018/12/16/principal_component/","excerpt":"","text":"","categories":[{"name":"cs229n","slug":"cs229n","permalink":"http://aier02.com/categories/cs229n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"mixture of Gaussian","slug":"mixture_of_Gau","date":"2018-12-16T12:39:57.674Z","updated":"2018-12-16T15:00:20.365Z","comments":true,"path":"2018/12/16/mixture_of_Gau/","link":"","permalink":"http://aier02.com/2018/12/16/mixture_of_Gau/","excerpt":"","text":"","categories":[{"name":"cs229n","slug":"cs229n","permalink":"http://aier02.com/categories/cs229n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"exception maximization","slug":"exception_maximization","date":"2018-12-16T12:39:07.498Z","updated":"2018-12-16T15:03:43.711Z","comments":true,"path":"2018/12/16/exception_maximization/","link":"","permalink":"http://aier02.com/2018/12/16/exception_maximization/","excerpt":"","text":"","categories":[{"name":"cs229n","slug":"cs229n","permalink":"http://aier02.com/categories/cs229n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"bayesian statistic","slug":"bayesian_statist","date":"2018-12-16T12:37:36.576Z","updated":"2018-12-16T15:00:54.868Z","comments":true,"path":"2018/12/16/bayesian_statist/","link":"","permalink":"http://aier02.com/2018/12/16/bayesian_statist/","excerpt":"","text":"","categories":[{"name":"cs229n","slug":"cs229n","permalink":"http://aier02.com/categories/cs229n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"feature selection","slug":"feature_selection","date":"2018-12-16T12:36:22.956Z","updated":"2018-12-16T15:01:29.782Z","comments":true,"path":"2018/12/16/feature_selection/","link":"","permalink":"http://aier02.com/2018/12/16/feature_selection/","excerpt":"","text":"","categories":[{"name":"cs229n","slug":"cs229n","permalink":"http://aier02.com/categories/cs229n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"empirial risk minimization","slug":"empirial_risk_minimize","date":"2018-12-16T12:35:49.024Z","updated":"2018-12-16T15:02:02.606Z","comments":true,"path":"2018/12/16/empirial_risk_minimize/","link":"","permalink":"http://aier02.com/2018/12/16/empirial_risk_minimize/","excerpt":"","text":"","categories":[{"name":"cs229n","slug":"cs229n","permalink":"http://aier02.com/categories/cs229n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"kernel method","slug":"kernel_method","date":"2018-12-16T12:34:21.064Z","updated":"2018-12-16T15:05:13.356Z","comments":true,"path":"2018/12/16/kernel_method/","link":"","permalink":"http://aier02.com/2018/12/16/kernel_method/","excerpt":"","text":"","categories":[{"name":"cs229n","slug":"cs229n","permalink":"http://aier02.com/categories/cs229n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"Lagrange multiplian","slug":"Lagrange_multiplian","date":"2018-12-16T12:32:02.432Z","updated":"2018-12-16T15:05:31.241Z","comments":true,"path":"2018/12/16/Lagrange_multiplian/","link":"","permalink":"http://aier02.com/2018/12/16/Lagrange_multiplian/","excerpt":"","text":"","categories":[{"name":"cs229n","slug":"cs229n","permalink":"http://aier02.com/categories/cs229n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"naive Bayes","slug":"naive_Bayse","date":"2018-12-16T12:29:24.745Z","updated":"2018-12-16T15:06:47.058Z","comments":true,"path":"2018/12/16/naive_Bayse/","link":"","permalink":"http://aier02.com/2018/12/16/naive_Bayse/","excerpt":"","text":"","categories":[{"name":"cs229n","slug":"cs229n","permalink":"http://aier02.com/categories/cs229n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"generative learning algorithms","slug":"generative_learning_algorithms","date":"2018-12-16T12:27:05.959Z","updated":"2018-12-16T12:27:49.137Z","comments":true,"path":"2018/12/16/generative_learning_algorithms/","link":"","permalink":"http://aier02.com/2018/12/16/generative_learning_algorithms/","excerpt":"","text":"","categories":[{"name":"cs229n","slug":"cs229n","permalink":"http://aier02.com/categories/cs229n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"logistic regression","slug":"logistic_regression","date":"2018-12-16T11:54:52.916Z","updated":"2018-12-16T12:23:40.477Z","comments":true,"path":"2018/12/16/logistic_regression/","link":"","permalink":"http://aier02.com/2018/12/16/logistic_regression/","excerpt":"","text":"​ ​ ​ ​ ​","categories":[{"name":"cs229n","slug":"cs229n","permalink":"http://aier02.com/categories/cs229n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"leetcode linked list","slug":"leetcode_linked_list","date":"2018-11-23T02:00:17.093Z","updated":"2018-12-09T07:13:02.720Z","comments":true,"path":"2018/11/23/leetcode_linked_list/","link":"","permalink":"http://aier02.com/2018/11/23/leetcode_linked_list/","excerpt":"","text":"2. Add Two Numbers（两个链表的对应节点值相加）You are given two non-empty linked lists representing two non-negative integers. The digits are stored in reverse order and each of their nodes contain a single digit. Add the two numbers and return it as a linked list.You may assume the two numbers do not contain any leading zero, except the number 0 itself.Example:123Input: (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)Output: 7 -&gt; 0 -&gt; 8Explanation: 342 + 465 = 807.12345678910111213141516171819202122232425262728# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution:# @return a ListNode def addTwoNumbers(self, l1, l2): #串行进位加法器 carry=0#记录进位 res=n=ListNode(0)#创建结果节点 #依次将l1和l2对应节点的val进行相加，每次加完后将指针往前移一位 while l1 or l2 or carry: #每次循环必须得先初始化两个节点的值为0，再根据节点的情况进行赋值 v1=v2=0 if l1: v1=l1.val l1=l1.next if l2: v2=l2.val l2=l2.next #使用divmod()实现求商和余数，从而得到进位和本位和 carry,val=divmod(v1+v2+carry,10) #创建对应的节点 n.next=ListNode(val) #指定下一个节点 n=n.next return res.next 19. Remove Nth Node From End of List（移除从后往前数的第n个节点）Given a linked list, remove the n-th node from the end of list and return its head.Example:123Given linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5, and n = 2.After removing the second node from the end, the linked list becomes 1-&gt;2-&gt;3-&gt;5.Note:Given n will always be valid.Follow up:Could you do this in one pass?12345678910111213141516171819202122232425262728293031# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def removeNthFromEnd(self, head, n): \"\"\" :type head: ListNode :type n: int :rtype: ListNode \"\"\" #找到从头开始搜索的要移除的节点的位置，记录为t l=1 temp=head.next while temp!=None: l+=1 temp=temp.next t=l-n if t==0: head=head.next else: temp=head j=1 while j&lt;t: temp=temp.next j+=1 temp.next=temp.next.next return head 23. Merge k Sorted Lists（合并k个已经排好序的链表）Merge k sorted linked lists and return it as one sorted list. Analyze and describe its complexity.Example:1234567Input:[ 1-&gt;4-&gt;5, 1-&gt;3-&gt;4, 2-&gt;6]Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6比较直接的做法是使用经典的合并算法，这里的做法是直接读取所有节点值，然后统一进行排序后以链表的形式进行表示。1234567891011121314151617181920212223# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def mergeKLists(self, lists): \"\"\" :type lists: List[ListNode] :rtype: ListNode \"\"\" ls=[] for t in lists: while t!=None: ls.append(t.val) t=t.next ls.sort() head=fhead=ListNode(0) for i in range(len(ls)): head.next=ListNode(ls[i]) head=head.next return fhead.next 25. Reverse Nodes in k-GroupGiven a linked list, reverse the nodes of a linked list k at a time and return its modified list.k is a positive integer and is less than or equal to the length of the linked list. If the number of nodes is not a multiple of k then left-out nodes in the end should remain as it is.Example:Given this linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5For k = 2, you should return: 2-&gt;1-&gt;4-&gt;3-&gt;5For k = 3, you should return: 3-&gt;2-&gt;1-&gt;4-&gt;512345678910111213141516171819202122232425262728# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def reverseKGroup(self, head, k): count,node=0,head while node and count&lt;k: node=node.next count+=1 if count&lt;k:return head else: #翻转后返回的是该段翻转之后的头节点prev，和下一个需要翻转的部分的头节点temp temp,prev=self.reverse(head,k) #本段翻转后，原来的head成了尾部，需要和下一段的头进行对接，故每次都要返回翻转之后的头prev head.next=self.reverseKGroup(temp,k) return prev #每次翻转后要保留断开位置的前一个节点和后一个节点 def reverse(self,head,k): prev,cur,nxt=None,head,head while k&gt;0: nxt=cur.next cur.next=prev prev=cur cur=nxt k-=1 return (cur,prev)","categories":[{"name":"algorithms","slug":"algorithms","permalink":"http://aier02.com/categories/algorithms/"}],"tags":[{"name":"linked list","slug":"linked-list","permalink":"http://aier02.com/tags/linked-list/"},{"name":"leetcode","slug":"leetcode","permalink":"http://aier02.com/tags/leetcode/"}]},{"title":"leetcode hash table","slug":"leetcode_hash_table","date":"2018-11-23T01:36:36.136Z","updated":"2018-12-09T07:12:55.099Z","comments":true,"path":"2018/11/23/leetcode_hash_table/","link":"","permalink":"http://aier02.com/2018/11/23/leetcode_hash_table/","excerpt":"","text":"hash table是指对目标值进行一定变换后映射到表中的某个位置（或者某个值），可以用list或者dict实现这个哈希表 1. Two SumGiven an array of integers, return indices of the two numbers such that they add up to a specific target.You may assume that each input would have exactly one solution, and you may not use the same element twice.Example:1234Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1].123456789101112class Solution: def twoSum(self, nums, target): if len(nums)&lt;=1: return False else: sum_dict=&#123;&#125; for i in range(len(nums)): if nums[i] in sum_dict: return [sum_dict[nums[i]],i] else: #用key值保存索引值，key对应的value为差值 sum_dict[target-nums[i]]=i这个sum_dict的关键在于他的转换是将数组中每个值与target的差值保存为key，查表时只要查找是否数组中存在这样的差值即可找到相应的两个和为target的元素。 3. Longest Substring Without Repeating CharactersGiven a string, find the length of the longest substring without repeating characters.Example 1:123Input: \"abcabcbb\"Output: 3 Explanation: The answer is \"abc\", with the length of 3.1234567891011121314class Solution(object): def lengthOfLongestSubstring(self, s): dic, res, start, = &#123;&#125;, 0, 0 for i, ch in enumerate(s): # when char already in dictionary if ch in dic: # check length from start of string to index res = max(res, i-start) # update start of string index to the next index start = max(start, dic[ch]+1) # add/update char to/of dictionary dic[ch] = i # answer is either in the begining/middle OR some mid to the end of string return max(res, len(s)-start)用dict记录每个出现过的字母和他的索引，若遇到重复的字母则更新res和重新记录的开始位置。 18. 4SumGiven an array nums of n integers and an integer target, are there elements a, b, c, and d in nums such that a + b + c + d = target? Find all unique quadruplets in the array which gives the sum of target.Note:The solution set must not contain duplicate quadruplets.Example:12345678Given array nums = [1, 0, -1, 0, -2, 2], and target = 0.A solution set is:[ [-1, 0, 0, 1], [-2, -1, 1, 2], [-2, 0, 0, 2]]1234567891011121314151617181920212223242526272829303132333435363738class Solution: def fourSum(self, nums, target): \"\"\" :type nums: List[int] :type target: int :rtype: List[List[int]] \"\"\" #先进行排序，这样做的好处是后期对两个数字进行搜索时可以更加方便的移动l和r指针 nums.sort() results=[] self.findNsum(nums,target,4,[],results) return results def findNsum(self,nums,target,N,result,results): #检查在剩余的nums中查找target的可能性 if len(nums)&lt;N or N&lt;2 or target&lt;nums[0]*N or target&gt;nums[-1]*N: return elif N==2: #在数组中首尾进行搜索和为target的两个位置 l,r=0,len(nums)-1 while l&lt;r: #找到目标值 if target==nums[l]+nums[r]: results.append(result+[nums[l],nums[r]]) l+=1 #当前后l指定的数字相同时，l+1，跳过这个重复的值 while l&lt;r and nums[l-1]==nums[l]: l+=1 #两个指针对应的值和小于target，因为nums[r]已经最大，所以往前移动l elif nums[l]+nums[r]&lt;target: l+=1 #往后移动r else: r-=1 else: #深度优先搜索，每次先确定一个值，在剩余的nums中寻找修改后的target for i in range(len(nums)-N+1): if i==0 or (i&gt;0 and nums[i-1]!=nums[i]): self.findNsum(nums[i+1:],target-nums[i],N-1,result+[nums[i]],results)先对nums进行排序，然后迭代的每次保留一个数字然后往后进行搜索，直到只剩下两个数字时使用两个指针从首尾进行搜索，特别注意相同数字的处理，当两个数字相同时，l要继续往前+1。 30. Substring with Concatenation of All WordsYou are given a string, s, and a list of words, words, that are all of the same length. Find all starting indices of substring(s) in s that is a concatenation of each word in words exactly once and without any intervening characters.Example 1:123456Input: s = \"barfoothefoobarman\", words = [\"foo\",\"bar\"]Output: [0,9]Explanation: Substrings starting at index 0 and 9 are \"barfoor\" and \"foobar\" respectively.The output order does not matter, returning [9,0] is fine too.Example 2:1234Input: s = \"wordgoodstudentgoodword\", words = [\"word\",\"student\"]Output: []123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Solution: def findSubstring(self, s, words): if len(words) == 0: return [] # initialize d, l, ans l = len(words[0]) d = &#123;&#125; #用字典d记录words中出现的各个word的次数 for w in words: if w in d: d[w] += 1 else: d[w] = 1 i = 0 ans = [] # sliding window(s) for k in range(l): #left指针用于指示左端第一个有效字符的起始位置 #count用于记录当前有效的word数目 left = k subd = &#123;&#125; count = 0 #处理开始索引k，k必定在一个l的范围内 for j in range(k, len(s)-l+1, l): tword = s[j:j+l] # valid word，即该字符串出现在words中 if tword in d: if tword in subd: subd[tword] += 1 else: subd[tword] = 1 count += 1 #纠正某个单词的数目，若超过了words中的数目则需要进行前向调整 while subd[tword] &gt; d[tword]: subd[s[left:left+l]] -= 1 left += l count -= 1 if count == len(words): ans.append(left) # not valid #遇到无效的单词时，需要重新计算left的位置，默认下一个单词有效，同时需要重新统计字典 #这种解法关键在于使用两个字典分别统计words中各个单词出现的次数，当统计到的有效单词数目一致时则认为是有效的连续字串 else: left = j + l subd = &#123;&#125; count = 0 return ans这里的hash tabel为一个字典，记录了words中各个单词的数目。 36. Valid SudokuDetermine if a 9x9 Sudoku board is valid. Only the filled cells need to be validated according to the following rules:Each row must contain the digits 1-9 without repetition.Each column must contain the digits 1-9 without repetition.Each of the 9 3x3 sub-boxes of the grid must contain the digits 1-9 without repetition.Example 1:12345678910111213Input:[ [\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"], [\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"], [\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"], [\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"], [\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"], [\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"], [\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"], [\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"], [\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]]Output: true123456789101112131415161718192021class Solution: def isValidSudoku(self, board): \"\"\" :type board: List[List[str]] :rtype: bool \"\"\" #set中存储的是不重复的元素 big = set() for i in range(0,9): for j in range(0,9): if board[i][j]!='.': cur = board[i][j] #记录board中数字出现的列，行，还有所在单独的3x3的区域的位置，9x9分为了9个区域，用两个索引可以表示3x3在9x9的位置 if (i,cur) in big or (cur,j) in big or (int(i/3),int(j/3),cur) in big: return False #一行中是否有重复数字 big.add((i,cur)) #一列中是否有重复数字 big.add((cur,j)) big.add((int(i/3),int(j/3),cur)) return True关键在于如何存储表示第i行出现了某个数字，第j列出现了某个数字，第n个区域出现了某个数字。通过行和列确定区域所在的位置。 37. Sudoku SolverWrite a program to solve a Sudoku puzzle by filling the empty cells.A sudoku solution must satisfy all of the following rules:Each of the digits 1-9 must occur exactly once in each row.Each of the digits 1-9 must occur exactly once in each column.Each of the the digits 1-9 must occur exactly once in each of the 9 3x3 sub-boxes of the grid.Empty cells are indicated by the character '.'.1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution: #检查在(row,col)位置插入k值后是否还是有效的数独板 def isValidSudoku(self, board, row, col, k): for i in range(9): if board[row][i] != '.' and board[row][i] == k: return False for j in range(9): if board[j][col] != '.' and board[j][col] == k: return False r1 = (row // 3) * 3 c1 = (col // 3) * 3 for i in range(r1, r1+3): for j in range(c1, c1+3): if board[i][j] != '.' and board[i][j] == k: return False return True def solveSudoku(self, board): self.board = board self.solve(board) def solve(self, board): \"\"\" :type board: List[List[str]] :rtype: void Do not return anything, modify board in-place instead. \"\"\" for i in range(9): for j in range(9): #找到空缺的位置 if board[i][j] == '.': for k in range(1,10): #检查在（i，j）插入k之后是否有效 if self.isValidSudoku(board, i, j, str(k)): #令（i，j）为k board[i][j] = str(k) #递归检查是否能solve if self.solve(board): return True #无效则回溯 board[i][j] = '.' #进行了所有尝试都无效则返回false return False #前面没有返回flase则成功 return True每次尝试在空缺位置插入一个值，检查插入后board是否还是有效的，无效则进行回溯，尝试下一个值。 49. Group AnagramsGiven an array of strings, group anagrams together.Example:1234567Input: [\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"],Output:[ [\"ate\",\"eat\",\"tea\"], [\"nat\",\"tan\"], [\"bat\"]]12345678910111213141516171819202122232425262728293031323334353637class Solution: def groupAnagrams(self, strs): \"\"\" :type strs: List[str] :rtype: List[List[str]] \"\"\" #字符串匹配问题，如何实现快速匹配而不是一个一个字母匹配 #构造一个hash函数，使得含有相同字母的str都映射到统一个位置 #方法一是进行排序，没有构造hash function \"\"\" dic=&#123;&#125; for s in strs: sp=''.join(sorted(s)) if sp in dic: dic[sp].append(s) else: dic[sp]=[s] return list(dic.values()) \"\"\" #方法二是构造一个tuple记录字母出现的位置 def convert(s): res=[0]*26 for c in s: res[ord(c)-ord('a')]+=1 return tuple(res) #tuple对应的索引 dic=&#123;&#125; #记录结果 res=[] for s in strs: t=convert(s) if t in dic: res[dic[t]].append(s) else: res.append([s]) dic[t]=len(res)-1 return res构造一个tuple表，使得字符串映射到字符位置固定的tuple中，这样做能使得相同字母构造的不同字符串能到映射到相同的位置。hash table的核心思想是使得具有某种规律的输入映射到相同的输出，在python 中用字典的形式存储这种key-value的对应关系，具体的关系要根据实际问题进行设计","categories":[{"name":"algorithms","slug":"algorithms","permalink":"http://aier02.com/categories/algorithms/"}],"tags":[{"name":"leetcode","slug":"leetcode","permalink":"http://aier02.com/tags/leetcode/"},{"name":"hash table","slug":"hash-table","permalink":"http://aier02.com/tags/hash-table/"}]},{"title":"VGG","slug":"VGG","date":"2018-11-12T03:28:24.262Z","updated":"2018-11-19T11:38:09.607Z","comments":true,"path":"2018/11/12/VGG/","link":"","permalink":"http://aier02.com/2018/11/12/VGG/","excerpt":"","text":"read paper online Abstract主要工作是观察加深网络深度在大型图像分类问题上的效果，使用3x3的卷积核来加深CNNs到16-19层，这种做法使得该模型在ILSVCR2014取得了第一和第二的成绩。 1 INTRODUCTION介绍了今年convnet取得的成就，越来越多人尝试从不同方面改进convnet取实现更高的分类准确率；有的修改感受野的大小，使用小的步长，或者从训练和测试的时候使用图片的大小和数量入手。而本文关注的是convnet的深度，所有卷积层使用的filter均为3x3，增加了整体的深度。结果取得了当时在ILSVR中最好的结果，而且在其他数据集中都是最好的。最后说明了本文各个section和appendix中的主要内容。 2 CONVNET CONFIGURATIONS所有卷积层的配置使用同一个规则。 2.1 ARCHITECTURE固定大小为224x224的RGB图片，预处理是每张图片减去整个训练集图像的RGB均值，卷积核为3x3，另外1x1的卷积核可以看作是输入channel的线性变换，卷积步长为1，padding为1，空间池化为在某些卷积层后使用最大池化层2x2,步长为2，但不是所有卷积层都使用；所有卷积层后接3层fc，前两层为4096个channel的fc，第三层为1000个channel；整个网络的最后一层为sofmax。所有隐藏层都使用Relus作为激活函数，只有一层使用了LRN。 2.2 CONFIGURATIONS卷积层中的channel从64每次经过一个pooling层则翻倍最后到512channel。 2.3 DISCUSSION和目前为止的模型不同之处在于在开始的conv layer不使用大的感受野，而是在整个网络中都只使用3x3，显然两个这样的卷积层堆叠起来的感受野为5x5，中间不经过pooling层，同理3层堆叠为7x7，这样做的作用是一方面把使用3个RLUs替代一个，使得决策函数更加有影响力；另一方面3个3x3的参数相较于一个7x7的参数更少，前者为3(32C2),即27C2,后者为72C2=49C2;","categories":[{"name":"paper","slug":"paper","permalink":"http://aier02.com/categories/paper/"}],"tags":[{"name":"cv","slug":"cv","permalink":"http://aier02.com/tags/cv/"},{"name":"paper reading","slug":"paper-reading","permalink":"http://aier02.com/tags/paper-reading/"}]},{"title":"AlexNet","slug":"AlexNet","date":"2018-10-30T02:56:58.917Z","updated":"2018-11-03T09:23:15.726Z","comments":true,"path":"2018/10/30/AlexNet/","link":"","permalink":"http://aier02.com/2018/10/30/AlexNet/","excerpt":"","text":"read paper online Abstract训练集和测试集均来自ImageNet网络框架的结构为5层conv layer，接一个max pooling和3层fully connected layer训练过程中使用的方法，高效的GPU代码和减少overfitting的dropout项目达到的成就，当时最好的测试误差率 1 Introduction当时的分类方法主要是高效地运用机器学习方法，要提高分类的表现可以从增大数据集、学习更强的模型和减少overfitting入手。更大的数据集:ImageNet数据集拥有1500万张高清图片并且含有22000个种类更好的模型:CNN模型能有效地处理大量的图片，他的参数相较于传统的前向神经网络要少，更加容易去训练。更好的训练方法:在GPU上高效地实现二维卷积能有效地加快训练的过程。该paper特别的贡献，以及当时硬件的限制。 2 The Dataset介绍具体使用的数据集的来源、图片大小、图片种类、图片数量,以及训练过程中加入的图像预处理。paper中使用的pre-processing为图像下采样为256x256，pixel值减去整个训练集的中值，最后使用的是RGB格式的纯pixel值。top1就是你预测的label取最后概率向量里面最大的那一个作为预测结果，你的预测结果中概率最大的那个类必须是正确类别才算预测正确。而top5就是最后概率向量最大的前五名中出现了正确概率即为预测正确。故top-1 error rate 和 top-5 error rate都是在该错误率定义下在test data set中的概率。 3 The Architecture介绍使用的CNN框架中新颖的设计 3.1 ReLU Nonlinearity训练时使用GD 算法,饱和的非线性函数如tanh(x) 和 sigmoid function (1 + e−x)−1 使得训练的时间要比不饱和的非线性函数f(x) = max(0,x),称为Rectified Linear Units要长。通过图例说明饱和的activation function比不饱和的收敛得更慢。 3.2 Training on Multiple GPUs受限于硬件设备，将整个网络(神经元)平分到2个GPU中进行并行计算，GPU能够直接读写别的GPU的内存而不占用主机的内存;两个GPU的沟通只在特定的layer进行,沟通是指两者产生feature maps作为对方的输入,不沟通就是只用该GPU产生的feature maps作为下一层的输入。 3.3 Local Response Normalization（no idea==）Relus不需要为了避免饱和而进行输入的规范化， 3.4 Overlapping Poolingpooling unit的strke和size一致，为tradiitonal local pooling，strike小于size则会发生overlapping。 3.5 Overall Architecturefully connected layers中的神经元是一个和feature map大小一致的filter，全连接层的目的是高度提取特征，将feature map浓缩为一个数字，用于后面的分类或者回归。在最后一个卷积层输出到全连接层中，将输入看作一个neuron表示一张feature map，所有输入都得连接到每一个全连接层的neuron中，该过程和普通的卷积层类似，不同点在于全连接层的每一个w与单个feature map大小一致而且每一个输出都为一个数字，而卷积层的大小自定而且输出为一张feature map。 4 Reducing Overfitting整个网络中有六千万个参数，尽管从image到label的映射中存在10位二进制的约束，但是如此多的参数无可避免会存在过拟合。 4.1 Data Augmentation一般先提及以往的方法，指出不足之处后提出自己的做法，突出自己的优势。在GPU进行训练的过程中，利用CPU对还未训练的图像进行小量的改变，故可以说数据增强在计算代价上面是“免费的”。（CPU的计算代价远小于GPU）图像平移和水平翻转:从原图和翻转后的图像中(256x256)分别随机选取224x224的区域作为增强后的图像进行训练，这种做法虽然增加了训练集的大小，但是显然部分数据存在内部的依赖(那为啥还这么做呢？给出理由),实际情况是不使用这种增强的模式，训练会出现严重的overfitting，迫使使用小的网络，权衡之后这种方式还是有效的。在测试的时候，分别在原图和翻转后的图像定位选取5个子区域，四个角的224x224，还有中心位置的224x224，即一张原图产生了10张用于测试的图像,进行预测时即对这十张图片的softmax得到的值进行平均得到某个class的最大值为预测值。改变训练图像的RGB通道强度，对训练集中的RGB像素进行PCA，然后对三个通道的值进行成比例的修改。(这里没看懂) 4.2 Dropout集成学习对于减少错误率非常有效，但是对于大型的网络而言该做法计算代价太大，dropout能同时兼顾两者，既能实现不同的网络，同时能降低训练成本；对于每一个隐藏层的neuron，以0.5的概率对他的输出进行归零，即该神经元在本次训练(包括fp和bp)均失效；每一个epoch均可能是不同的结构，这种做法会导致训练次数的增多(大致一倍)才使得模型收敛，在test的时候，根据概率论的知识，每一个神经元的输出要乘以0.5，才和训练时的设置保持一致；本文中的dropout在后两层的全连接层实现。 5 Details of learning训练过程中使用了SGD算法，一个batch为128张图片，momentum为0.9，weight decay为0.0005，实验发现这个0.0005对于减少错误率很重要(玄学？)vi+1:=0.9∗vi−0.0005∗ϵ∗wi−ϵ∗&lt;∂L∂W∣wi&gt;Diwi+1:=wi+vi+1v_{i+1}:=0.9*v_{i}-0.0005*\\epsilon*w_i-\\epsilon*&lt;\\frac{\\partial L}{\\partial W}|_{w_i}&gt;_{D_i} w_{i+1}:=w_{i}+v_{i+1}v​i+1​​:=0.9∗v​i​​−0.0005∗ϵ∗w​i​​−ϵ∗&lt;​∂W​​∂L​​∣​w​i​​​​&gt;​D​i​​​​w​i+1​​:=w​i​​+v​i+1​​i是epoch的轮数，v是动量变量，ϵ\\epsilonϵ是learning rate，&lt;∂L∂W∣wi&gt;Di&lt;\\frac{\\partial L}{\\partial W}|_{w_i}&gt;_{D_i}&lt;​∂W​​∂L​​∣​w​i​​​​&gt;​D​i​​​​是在第i次的batch中损失函数关于w的导数的平均值。对于每一层中的w的初始化，使用的是均值为0，标准差为0.01的高斯分布，而对于2，4，5卷积层和全连接层的bias，初始化为1，其他bias初始化为0；使得训练初始阶段Relus有正数输入，训练加快。所有层使用相同的初始化学习率0.01，然后在训练过程中手工调整，启发式做法是当验证的错误率不在减少时，对当前的学习率进行/10操作。 6 Results和其他model进行对比，突出自己的成绩，横向对比。 7 Discussion强调卷积神经网络大且深的重要性，并对未来的工作进行展望，本文为希望用在视频序列帧中。 References所有引用的文章出处。","categories":[{"name":"paper","slug":"paper","permalink":"http://aier02.com/categories/paper/"}],"tags":[{"name":"cv","slug":"cv","permalink":"http://aier02.com/tags/cv/"},{"name":"paper reading","slug":"paper-reading","permalink":"http://aier02.com/tags/paper-reading/"}]},{"title":"backpropagation","slug":"backpropagation","date":"2018-10-29T08:04:42.275Z","updated":"2018-11-21T08:28:53.411Z","comments":true,"path":"2018/10/29/backpropagation/","link":"","permalink":"http://aier02.com/2018/10/29/backpropagation/","excerpt":"","text":"Introductionbackpropagation, which is a way of computing gradients of expressions through recursive application of chain rule. 反向传播算法通过链式法则计算梯度in practice we usually only compute the gradient for the parameters (e.g. W,b) so that we can use it to perform a parameter update.一般实际中计算的是权重的梯度，用于更新权重，也可能计算输入x的梯度，用于可视化和解释是神经网络的工作 Simple expressions and interpretation of the gradientthe derivative on each variable tells you the sensitivity of the whole expression on its value，导函数表明了整个函数对于不同自变量的敏感程度f(x+h)=f(x)+hdf(x)dxf(x + h) = f(x) + h \\frac{df(x)}{dx}f(x+h)=f(x)+h​dx​​df(x)​​表明了f(x)随x变化的幅度，即x变化h后，f(x+h)如何变化,自变量的变化量和函数值的变化量的关系。Even though the gradient is technically a vector, we will often use terms such as “the gradient on x” instead of the technically correct phrase “the partial derivative on x” for simplicity.出于方便，notebook中将对于x的偏导数说成对于x的梯度he derivatives tell us nothing about the effect of such large changes on the inputs of a function; They are only informative for tiny, infinitesimally small changes on the inputs, as indicated by the limh→0\\lim_{h \\rightarrow 0}lim​h→0​​ in its definition. Compound expressions with chain rule利用微积分的知识求某个函数的导数有时候是十分困难的，或者相当的繁琐，这种情况下如何快速求出函数对某个参数的偏导数呢？此时，chain rule就很关键了。譬如存在函数f(x,y,z)=(x+y)zf(x,y,z)=(x+y)zf(x,y,z)=(x+y)z,则使用chain rule有：1234567891011121314# set some inputsx = -2; y = 5; z = -4# perform the forward passq = x + y # q becomes 3f = q * z # f becomes -12# perform the backward pass (backpropagation) in reverse order:# first backprop through f = q * zdfdz = q # df/dz = q, so gradient on z becomes 3dfdq = z # df/dq = z, so gradient on q becomes -4# now backprop through q = x + ydfdx = 1.0 * dfdq # dq/dx = 1. And the multiplication here is the chain rule!dfdy = 1.0 * dfdq # dq/dy = 1 Intuitive understanding of backpropagationEvery gate in a circuit diagram gets some inputs and can right away compute two things: 1. its output value and 2. the local gradient of its inputs with respect to its output value.将函数拆分为各个初等函数的混合计算，可视化为一个circuit diagram，图中的每个节点为一个基本计算。每个节点都可以独立计算他的output和local gradientBackpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher Modularity: Sigmoid examplesigmoid function(logistic function):\\sigma(x) = \\frac{1}{1+e^{-x}} \\\\\\\\ \\rightarrow \\hspace{0.3in} \\frac{d\\sigma(x)}{dx} = \\frac{e^{-x}}{(1+e^{-x})^2} = \\left( \\frac{1 + e^{-x} - 1}{1 + e^{-x}} \\right) \\left( \\frac{1}{1+e^{-x}} \\right) = \\left( 1 - \\sigma(x) \\right) \\sigma(x)","categories":[{"name":"cs231n","slug":"cs231n","permalink":"http://aier02.com/categories/cs231n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"},{"name":"chain rule","slug":"chain-rule","permalink":"http://aier02.com/tags/chain-rule/"},{"name":"backpropagation","slug":"backpropagation","permalink":"http://aier02.com/tags/backpropagation/"}]},{"title":"leetcode summary 26-30","slug":"leetcode-summary-26-30","date":"2018-10-24T03:20:21.962Z","updated":"2018-10-30T02:56:49.273Z","comments":true,"path":"2018/10/24/leetcode-summary-26-30/","link":"","permalink":"http://aier02.com/2018/10/24/leetcode-summary-26-30/","excerpt":"","text":"26. Remove Duplicates from Sorted ArrayGiven a sorted array nums, remove the duplicates in-place, such that each element appear only once and return the new length.Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory.题目有另外的提示就是要求返回一个int值的length，检测器就会检查modified的nums前length个值是否为排除了重复值后的正确答案。1234567891011121314151617class Solution: def removeDuplicates(self, nums): \"\"\" :type nums: List[int] :rtype: int \"\"\" if len(nums)&lt;=1:return len(nums) l,r,length=0,1,len(nums) while True: if r==length: break else: if nums[r]&gt;nums[l]: l+=1 nums[l]=nums[r] r+=1 return l+1使用两个pointer对nums数组进行遍历，每发现一个新的元素，则后一个pointer l+1，同时赋值nums[l]为nums[r]，r指针用于遍历数组,最终使得nums的前l+1个元素均不重复。 27. Remove ElementGiven an array nums and a value val, remove all instances of that value in-place and return the new length.Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory.The order of elements can be changed. It doesn’t matter what you leave beyond the new length.1234567891011121314151617181920212223242526272829class Solution: def removeElement(self, nums, val): \"\"\" :type nums: List[int] :type val: int :rtype: int \"\"\" if val not in nums:return len(nums) res=0 l,r,length=0,1,len(nums) while True: if r&gt;=length or l&gt;=length: break else: if nums[l]==val: if nums[r]!=val: nums[l]=nums[r] nums[r]=val l+=1 r+=1 else: r+=1 else: l+=1 r=l+1 for num in nums: if num!=val: res+=1 return res使用两个pointer，l指针寻找等于val的索引，r指针寻找在l之后的位置中不等于val的索引，若找到满足条件的r，则交换l和r索引指示的位置的值，l+1往后继续寻找val的位置，注意r无论如何必定在l之后。 28. Implement strStr()Implement strStr().Return the index of the first occurrence of needle in haystack, or -1 if needle is not part of haystack.For the purpose of this problem, we will return 0 when needle is an empty string1234567891011121314151617181920212223242526272829303132class Solution: def strStr(self, haystack, needle): \"\"\" :type haystack: str :type needle: str :rtype: int \"\"\" ph,pn,hlength,nlength=0,0,len(haystack),len(needle) num=0 flag=0 res=0 while True: if num==nlength or ph==hlength: break else: if haystack[ph]==needle[pn]: if flag==0: flag=1 res=ph pn+=1 num+=1 else: if flag==1: pn=0 num=0 flag=0 ph=res ph+=1 if num==nlength: return res else: return -1使用两个pointer分别遍历haystack和needle，tips在于使用flag表示是否找到两者一样的初始char，同时res记录第一个相同在haystack中的位置，前向遍历时若发现两个字母不相同，则检查flag是否为1，若为1则表示部分匹配，此时需要重置needle的指针pn和haystack指针ph为刚才记录的位置(后面会+1，表示继续往前找，而不用管前面的字母).方法二：利用python的切片特性，没有使用两个指针123456789101112131415def strStr(self, haystack, needle): \"\"\" :type haystack: str :type needle: str :rtype: int \"\"\" a = len(needle) b = len(haystack) i = 0 while a &lt;= b: if needle == haystack[i:i+a]: return i i += 1 b -= 1 return -1 29. Divide Two IntegersGiven two integers dividend and divisor, divide two integers without using multiplication, division and mod operator.Return the quotient after dividing dividend by divisor.The integer division should truncate toward zero.123456789101112131415class Solution: def divide(self, dividend, divisor): \"\"\" :type dividend: int :type divisor: int :rtype: int \"\"\" if dividend==0:return 0 i,result,p,q=map(abs,(0,0,dividend,divisor)) while q&lt;&lt;i &lt;=p:i+=1 for j in reversed(range(i)): if q&lt;&lt;j &lt;=p: p,result=p-(q&lt;&lt;j),result+(1&lt;&lt;j) if (dividend&lt;0)!=(divisor&lt;0) or result&lt;(-1&lt;&lt;31):result=-result return min(result,(1&lt;&lt;31)-1)使用移位操作实现乘除，基于二进制的思想，统计最多移位的位数，然后想平常手工计算两个数相除一样，上一位，除数变小，在继续同样的步骤。 30. Substring with Concatenation of All WordsYou are given a string, s, and a list of words, words, that are all of the same length. Find all starting indices of substring(s) in s that is a concatenation of each word in words exactly once and without any intervening characters.class Solution:​ def findSubstring(self, s, words):​ if len(words) == 0:​ return []# initialize d, l, ans​ l = len(words[0])​ d = {}​ for w in words:​ if w in d:​ d[w] += 1​ else:​ d[w] = 1​ i = 0​ ans = []12345678910111213141516171819202122232425262728293031# sliding window(s)for k in range(l):#left指针用于指示左端第一个有效字符的起始位置#count用于记录当前有效的word数目 left = k subd = &#123;&#125; count = 0 for j in range(k, len(s)-l+1, l): tword = s[j:j+l] # valid word if tword in d: if tword in subd: subd[tword] += 1 else: subd[tword] = 1 count += 1 #纠正某个单词的数目，若超过了words中的数目则需要进行前向调整 while subd[tword] &gt; d[tword]: subd[s[left:left+l]] -= 1 left += l count -= 1 if count == len(words): ans.append(left) # not valid #遇到无效的单词时，需要重新计算left的位置，默认下一个单词有效，同时需要重新统计字典 #这种解法关键在于使用两个字典分别统计words中各个单词出现的次数，当统计到的有效单词数目一致时则认为是有效的连续子串 else: left = j + l subd = &#123;&#125; count = 0return ans一开始的想法是通过寻找words中所有可能的单词搭配，组合数为n!，苦于寻找快速找到所有组合的方法，在看了discussion后发现自己其实没有很好的理解题目，其实问题的关键是找到由words中单词随意组成的子串的位置，并不要求返回具体的子串是什么，同样不关注子串是否重复，所以比较理想的方法是不用字符串匹配，毕竟倘若要字符串匹配，需要做到每一个字符进行比较；使用字典的形式统计单词出现的次数，同样能实现题目的要求。","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://aier02.com/categories/leetcode/"}],"tags":[{"name":"array","slug":"array","permalink":"http://aier02.com/tags/array/"},{"name":"two pointers","slug":"two-pointers","permalink":"http://aier02.com/tags/two-pointers/"},{"name":"backtracking","slug":"backtracking","permalink":"http://aier02.com/tags/backtracking/"}]},{"title":"leetcode summary 20-25","slug":"leetcode_summary_20-25","date":"2018-10-24T01:07:58.319Z","updated":"2018-10-29T08:04:24.604Z","comments":true,"path":"2018/10/24/leetcode_summary_20-25/","link":"","permalink":"http://aier02.com/2018/10/24/leetcode_summary_20-25/","excerpt":"","text":"20 Valid ParenthesesGiven a string containing just the characters '(', ')', '{', '}', '[' and ']', determine if the input string is valid.An input string is valid if:Open brackets must be closed by the same type of brackets.Open brackets must be closed in the correct order.Note that an empty string is also considered valid.利用stack的后进新出的特性对输入尽心匹配123456789101112131415161718192021222324class Solution(object): def isValid(self, s): \"\"\" :type s: str :rtype: bool \"\"\" mdict = &#123;')':'(','&#125;':'&#123;',']':'['&#125; my_stack = [] for item in s: if mdict.get(item): #check if item is closing parentheses if my_stack == []: # if it is closing and stack is empty - return false return False if mdict.get(item) != my_stack.pop(): return False else: # if not - add to stack my_stack.append(item) return my_stack == []关键在于创建一个stack用于存储所有开括号，python中list自带pop函数使得list成为一个栈；依次读取s的值，每当遇到一个闭括号，则检查stack非空且顶端的符号是否为对应的开括号，是则正确关闭，否则error；每遇到一个开括号则放入栈中，等待匹配；开闭括号的对应关系通过dict存储。###21 Generate ParenthesesGiven n pairs of parentheses, write a function to generate all combinations of well-formed parentheses.For example, given n = 3, a solution set is:1234567[ \"((()))\", \"(()())\", \"(())()\", \"()(())\", \"()()()\"]回溯算法的定义：回溯算法也叫试探法，它是一种系统地搜索问题的解的方法。回溯算法的基本思想是：从一条路往前走，能进则进，不能进则退回来，换一条路再试。回溯算法实际上一个类似枚举的搜索尝试过程，主要是在搜索尝试过程中寻找问题的解，当发现已不满足求解条件时，就“回溯”返回，尝试别的路径。回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法。适用于求解组合数较大的问题。对于回溯问题，总结出一个递归函数模板，包括以下三点递归函数的开头写好跳出条件，满足条件才将当前结果加入总结果中已经拿过的数不再拿 if(s.contains(num)){continue;}遍历过当前节点后，为了回溯到上一步，要去掉已经加入到结果list中的当前节点。12345678910111213141516171819202122232425262728class Solution: def generateParenthesis(self, n): \"\"\" :type n: int :rtype: List[str] \"\"\" path,result='',[] self.backtrack(n,0,0,path,result) return result def backtrack(self,n,num_open,num_close,path,result): if num_close==n: result.append(path) return else: if num_open&lt;n: path+='(' num_open+=1 self.backtrack(n,num_open,num_close,path,result) #回退到前一步 path=path[:-1] num_open-=1 if num_close&lt;num_open: path+=')' num_close+=1 self.backtrack(n,num_open,num_close,path,result) #path=path[:-1] #num_close-=1 returnbacktrack函数体现了回溯算法的思想，实际上类似于dfs，当“不满足条件时则回退到上一步”,在代码实现中则是通过回退num_open或者num_close的值和path实现这一操作，因为添加path的操作实际只有两种，添加“(”或者“)”，故根据if的顺序，在python中如果为顺序执行if的话，只用回退第一个if即可，若两个if的顺序不确定，更加规范的操作是都得回退。在运行过程中，“不满足条件”编码实现为“满足条件则加入到result中”，这种做法实际上更加符合枚举的想法;该代码实际上是每一个位置先加“(”进行深度优先搜索，直到找到满足条件的path，然后回退到该位置，添加“)”继续进行深度优先搜索。 23. Merge k Sorted ListsMerge k sorted linked lists and return it as one sorted list. Analyze and describe its complexity.Example:1234567Input:[ 1-&gt;4-&gt;5, 1-&gt;3-&gt;4, 2-&gt;6]Output: 1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;61234567891011121314151617181920212223# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def mergeKLists(self, lists): \"\"\" :type lists: List[ListNode] :rtype: ListNode \"\"\" ls=[] for t in lists: while t!=None: ls.append(t.val) t=t.next ls.sort() head=fhead=ListNode(0) for i in range(len(ls)): head.next=ListNode(ls[i]) head=head.next return fhead.next没有使用递归依次计算两个sorted lists，而是直接获取所有链表中的数值，然后sort()，根据得到的已经排好序的序列创建新的链表，明显时间复杂度为O(n)（忽略list自带的sort()函数的时间复杂度） 24. Swap Nodes in PairsGiven a linked list, swap every two adjacent nodes and return its head.Example:1Given 1-&gt;2-&gt;3-&gt;4, you should return the list as 2-&gt;1-&gt;4-&gt;3.Note:Your algorithm should use only constant extra space.You may not modify the values in the list’s nodes, only nodes itself may be changed.123456789101112131415161718192021222324252627282930313233# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: def swapPairs(self, head): \"\"\" :type head: ListNode :rtype: ListNode \"\"\" if head==None or head.next==None: return head else: temp=head cur=head.next prev=None root=cur while True: node=cur.next cur.next=temp temp.next=node if prev==None: prev=temp else: prev.next=cur prev=temp if node==None or node.next==None: break cur=node.next temp=node return root每两个节点互换并且不能只更改node的val，则只需处理节点之间的next关系，每次对两个节点进行操作，比如1-&gt;2-&gt;3-&gt;4,将2的next指向1，1的next指向3，同时注意另存一个节点记录前一组的最后一个节点，即该例子中的1，3，用于连接前一组转换后的的最后一个节点和后一组转换后的第一个节点，即例子中的1.next指向4.翻转的关键在于指定节点prev。 25. Reverse Nodes in k-GroupGiven a linked list, reverse the nodes of a linked list k at a time and return its modified list.k is a positive integer and is less than or equal to the length of the linked list. If the number of nodes is not a multiple of k then left-out nodes in the end should remain as it is.Example:Given this linked list: 1-&gt;2-&gt;3-&gt;4-&gt;5For k = 2, you should return: 2-&gt;1-&gt;4-&gt;3-&gt;5For k = 3, you should return: 3-&gt;2-&gt;1-&gt;4-&gt;5Note:Only constant extra memory is allowed.You may not alter the values in the list’s nodes, only nodes itself may be changed.针对整个链表的翻转实现,注意每次只要处理一个节点cur和该节点的前向节点prev，同时在改变cur的next之前另存nxt指向原来的next节点，以便于完成一对节点的翻转后cur指向cur的next。123456789101112class Solution: def reverseList(self, head): if not head or not head.next: return head #prev,cur,nxt分别记录前一个节点，正在处理的节点和后一个节点 prev, cur, nxt = None, head, head while cur: nxt = cur.next cur.next = prev prev = cur cur = nxt return prev扩展到k个节点的翻转，每次翻转k个节点的链表:1234567891011121314151617181920212223242526# Definition for singly-linked list.# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: #处理分组和组间的连接关系 def reverseKGroup(self, head, k): count, node = 0, head while node and count &lt; k: node = node.next count += 1 if count &lt; k: return head new_head, prev = self.reverse(head, count) head.next = self.reverseKGroup(new_head, k) return prev #处理每一个组的k个节点的翻转 def reverse(self, head, count): prev, cur, nxt = None, head, head while count &gt; 0: nxt = cur.next cur.next = prev prev = cur cur = nxt count -= 1 return (cur, prev)关键在于reverse函数返回值cur指示连续的k个节点翻转后的下一个节点，最为新的head节点，prev指示前一组翻转后的节点的最后一个节点，用于和前前一组的原头节点(即翻转后的尾节点)进行连接。实际问题的关键在于将整条链表以k个节点为一组进行划分，每次对k个节点的子链表进行翻转，对于某个子链表，翻转后要注意确定他的下一组的头节点new_head，从而获得他的下一组翻转后的头节点prev，将改子链表翻转后的尾节点，即原来的头节点和他的下一组的翻转后的头节点连接起来，同时要返回该子链表翻转后的头节点prev，供他的前一组使用。总而言之就是要注意每一组翻转前后的头尾节点，处理相邻组的连接关系。","categories":[{"name":"leetcode","slug":"leetcode","permalink":"http://aier02.com/categories/leetcode/"}],"tags":[{"name":"backtracking","slug":"backtracking","permalink":"http://aier02.com/tags/backtracking/"},{"name":"linked list","slug":"linked-list","permalink":"http://aier02.com/tags/linked-list/"},{"name":"string","slug":"string","permalink":"http://aier02.com/tags/string/"},{"name":"stack","slug":"stack","permalink":"http://aier02.com/tags/stack/"}]},{"title":"common knowledge","slug":"common_knowledge","date":"2018-10-13T02:28:06.184Z","updated":"2018-10-30T02:57:03.946Z","comments":true,"path":"2018/10/13/common_knowledge/","link":"","permalink":"http://aier02.com/2018/10/13/common_knowledge/","excerpt":"","text":"NP问题NP问题:首先需要介绍P(Polynomial,多项式)问题.P问题是可以在多项式时间内被确定机(通常意义的计算机)解决的问题.NP(Non-Deterministic Polynomial, 非确定多项式)问题,是指可以在多项式时间内被非确定机(他可以猜,他总是能猜到最能满足你需要的那种选择,如果你让他解决n皇后问题,他只要猜n次就能完成----每次都是那么幸运)解决的问题.这里有一个著名的问题----千禧难题之首,是说P问题是否等于NP问题,也即是否所有在非确定机上多项式可解的问题都能在确定机上用多项式时间求解###L1-norm和L2-norml1-norm是指曼哈顿距离，即向量的各个元素的绝对值之和；l2-norm是指欧几里得距离，即向量的各个元素的平方和。Lp范数是指向量的各元素的p次方之和开p次方鲁棒性（Robustness）：最小绝对值偏差的方法应用领域很广，相比最小均方的方法，它的鲁棒性更好，LAD能对数据中的异常点有很好的抗干扰能力，异常点可以安全的和高效的忽略，这对研究帮助很大。如果异常值对研究很重要，最小均方误差则是更好的选择。对于L2-norm，由于是均方误差，如果误差&gt;1的话，那么平方后，相比L1-norm而言，误差就会被放大很多。因此模型会对样例更敏感。如果样例是一个异常值，模型会调整最小化异常值的情况，以牺牲其它更一般样例为代价，因为相比单个异常样例，那些一般的样例会得到更小的损失误差内置的特征选择（Built-in feature selection）：这是L1-norm经常被提及的一个优点，而L2-norm没有。这实际上是L1-norm的一个结果，L1-norm往往会使系数变得稀疏（sparse coefficients）。假设模型有100个系数，但是有10个非零的系数，这就是说，其它90个预测器在预测目标值上是没有用的。L2-norm往往会有非稀疏的系数（non-sparse coefficients），没有这个特点。稀疏性（Sparsity）：这主要是一个向量或矩阵中只有很少的非零（non-zero）条目（entries）。L1-norm有能产生许多零值或非常小的值的系数的属性，很少有大的系数。L1-norm得到的稀疏矩阵能用于选择特征，稀疏矩阵中大多数为0，即对loss没用的特征，故只需关注少数非0的特征；稀疏矩阵指的是很多元素为0，只有少数元素是非零值的矩阵，即得到的线性回归模型的大部分系数都是0. 通常机器学习中特征数量很多，例如文本处理时，如果将一个词组（term）作为一个特征，那么特征数量会达到上万个（bigram）。在预测或分类时，那么多特征显然难以选择，但是如果代入这些特征得到的模型是一个稀疏模型，表示只有少数特征对这个模型有贡献，绝大部分特征是没有贡献的，或者贡献微小（因为它们前面的系数是0或者是很小的值，即使去掉对模型也没有什么影响），此时我们就可以只关注系数是非零值的特征。这就是稀疏模型与特征选择的关系","categories":[{"name":"cs231n","slug":"cs231n","permalink":"http://aier02.com/categories/cs231n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"CS231n-2017-Summary","slug":"CS231n_2017_Summary","date":"2018-10-08T14:46:19.006Z","updated":"2018-12-17T02:48:08.053Z","comments":true,"path":"2018/10/08/CS231n_2017_Summary/","link":"","permalink":"http://aier02.com/2018/10/08/CS231n_2017_Summary/","excerpt":"","text":"Something new to me when I read such a good notebook about CS231n-2017-Summary CNNs常用same策略同时保存图像边缘信息Padding strategy:in order to maintain our full size of the input. If we didn’t do padding zero the input will be shrinking too fast and we will lose a lot of data.Give a stride of 1 its common to pad to this equation: (F-1)/2 where F is the filter size, zero padding from both sides.If we pad this way we call this same convolution.If we have input of shape (32,32,3) and ten filters with shape is (5,5) with stride 1 and pad 2;Output size will be (32,32,10) # We maintain the size.Size of parameters per filter = 5x5x3 + 1 = 76(+1 for bias)All parameters 76x10=760So here are the parameters for the Conv layer:Number of filters K.Usually a power of 2.Spatial content size F.3,5,7 …The stride S.Usually 1 or 2 (If the stride is big there will be a downsampling but different of pooling)Amount of PaddingIf we want the input shape to be as the output shape, based on the F if 3 its 1, if F is 5 the 2 and so on一般而言pooling层是不可(用)学习的Pooling makes the representation smaller and more manageable.Pooling Operates over each activation map independently.Example of pooling is the maxpooling.Parameters of max pooling is the size of the filter and the stride&quot;Example 2x2 with stride 2 # Usually the two parameters are the same 2 , 2Also example of pooling is average pooling.In this case it might be learnable. Training neural networks IAs a revision here are the Mini batch stochastic gradient descent algorithm steps,小批量的随机梯度下降算法的步骤:Loop:Sample a batch of data.Forward prop it through the graph (network) and get loss.(define loss)Backprop to calculate the gradients.(chain rule)Update the parameters using the gradients(learning rate)Activation functions,用于引入非线性因素，单纯的线性模型表达能力不足。Sigmoid:Squashes the numbers between [0,1]Used as a firing rate like human brains.Sigmoid(x) = 1 / (1 + e^-x)Problems with sigmoid:big values neurons kill the gradients.Gradients are in most cases near 0 (Big values/small values), that kills the updates if the graph/network are large.Not Zero-centered.Didn’t produce zero-mean data.exp() is a bit compute expensive.just to mention. We have a more complex operations in deep learning like convolution.Tanh:Squashes the numbers between [-1,1]Zero centered.Still big values neurons “kill” the gradients.Tanh(x) is the equation.tanh(x)=sinh(x)cosh(x)tanh(x)=\\frac {sinh(x)}{cosh(x)}tanh(x)=​cosh(x)​​sinh(x)​​,sinh(x)=ex−e−x2sinh(x)=\\frac {e^{x}-e^{-x}}{2}sinh(x)=​2​​e​x​​−e​−x​​​​,cosh(x)=ex+e−x2cosh(x)=\\frac {e^{x}+e^{-x}}{2}cosh(x)=​2​​e​x​​+e​−x​​​​,Proposed by Yann Lecun in 1991RELU (Rectified linear unit):RELU(x) = max(0,x)Doesn’t kill the gradients.Only small values that are killed. Killed the gradient in the halfComputationally efficient.Converges much faster than Sigmoid and Tanh (6x)More biologically plausible than sigmoid.Proposed by Alex Krizhevsky in 2012 Toronto university. (AlexNet)Problems:Not zero centered.If weights aren’t initialized good, maybe 75% of the neurons will be dead and thats a waste computation. But its still works. This is an active area of research to optimize this.To solve the issue mentioned above, people might initialize all the biases by 0.01Leaky RELU:leaky_RELU(x) = max(0.01x,x)Doesn’t kill the gradients from both sides.Computationally efficient.Converges much faster than Sigmoid and Tanh (6x)Will not die.PRELU is placing the 0.01 by a variable alpha which is learned as a parameter.Exponential linear units (ELU):1234ELU(x) = &#123; x if x &gt; 0 alpah *(exp(x) -1) if x &lt;= 0 # alpah are a learning parameter&#125;It has all the benefits of RELUCloser to zero mean outputs and adds some robustness to noise.problemsexp() is a bit compute expensive.Maxout activations:maxout(x) = max(w1.T*x + b1, w2.T*x + b2)Generalizes RELU and Leaky RELUDoesn’t die!Problems:oubles the number of parameters per neuronIn practice:Use RELU. Be careful for your learning rates.Try out Leaky RELU/Maxout/ELUTry out tanh but don’t expect much.Don’t use sigmoid!Data preprocessing:Normalize the data:减去均值后除以标准差1234567# Zero centered data. (Calculate the mean for every input).# On of the reasons we do this is because we need data to be between positive and negative and not all the be negative or positive. X -= np.mean(X, axis = 1)#np.mean()中的参数axis指定了哪个维度被压缩成1，例如axis=0,则输出的结果为一行，即求得输入x的每一列的平均，压缩成一行，同理axis=1，则输出为一列，该结果中的每一行为按照行进行平均的值。# Then apply the standard deviation. Hint: in images we don't do this.X /= np.std(X, axis = 1)To normalize images:对图像进行标准化Subtract the mean image (E.g. Alexnet)Mean image shape is the same as the input images.Or Subtract per-channel meanMeans calculate the mean for each channel of all images. Shape is 3 (3 channels)First idea is to initialize the w’s with small random numbers:12W = 0.01 * np.random.rand(D, H)# Works OK for small networks but it makes problems with deeper networks!The standard deviations is going to zero in deeper networks. and the gradient will vanish sooner in deep networks.使用任意小的数字进行对w初始化，随着网络的加深可能导致梯度消失问题(每一层的输入很小)。12W = 1 * np.random.rand(D, H) # Works OK for small networks but it makes problems with deeper networks!The network will explode with big numbers!，使用大于一的初值可能会导致深层网络中梯度爆炸问题。Xavier initialization:1W = np.random.rand(in, out) / np.sqrt(in)It works because we want the variance of the input to be as the variance of the output.But it has an issue, It breaks when you are using RELU.He initialization(Solution for the RELU issue):1W = np.random.rand(in, out) / np.sqrt(in/2)Solves the issue with RELU. Its recommended when you are using RELUBatch normalization:is a technique to provide any layer in a Neural Network with inputs that are zero mean/unit variance.It speeds up the training. You want to do this a lot.Made by Sergey Ioffe and Christian Szegedy at 2015.We make a Gaussian activations in each layer. by calculating the mean and the variance.Usually inserted after (fully connected or Convolutional layers) and (before nonlinearity).Steps (For each output of a layer)First we compute the mean and variance^2 of the batch for each feature.We normalize by subtracting the mean and dividing by square root of (variance^2 + epsilon)epsilon to not divide by zeroThen we make a scale and shift variables:1Result = gamma * normalizedX + betagamma and beta are learnable parameters.it basically possible to say “Hey!! I don’t want zero mean/unit variance input, give me back the raw input - it’s better for me.”Hey shift and scale by what you want not just the mean and variance!Baby sitting the learning processPreprocessing of data.Choose the architecture.Make a forward pass and check the loss (Disable regularization). Check if the loss is reasonable.Add regularization, the loss should go up!Disable the regularization again and take a small number of data and try to train the loss and reach zero loss.You should overfit perfectly for small datasets.Take your full training data, and small regularization then try some value of learning rate.If loss is barely changing, then the learning rate is small.If you got NAN then your NN exploded and your learning rate is high.Get your learning rate range by trying the min value (That can change) and the max value that doesn’t explode the network.Do Hyperparameters optimization to get the best hyperparameters values.Hyperparameter OptimizationTry Cross validation strategy.Run with a few ephocs, and try to optimize the ranges.Its best to optimize in log space.Adjust your ranges and try again.Its better to try random search instead of grid searches (In log space) Training neural networks IIOptimization algorithms:Problems with stochastic gradient descent:随机梯度下降算法的问题if loss quickly in one direction and slowly in another (For only two variables), you will get very slow progress along shallow dimension, jitter along steep direction. Our NN will have a lot of parameters then the problem will be more.Local minimum or saddle points；局部最小或者鞍点问题何为鞍点？鞍点（Saddle point）在微分方程中，沿着某一方向是稳定的，另一条方向是不稳定的奇点，叫做鞍点。在泛函中，既不是极大值点也不是极小值点的临界点，叫做鞍点。在矩阵中，一个数在所在行中是最大值，在所在列中是最小值，则被称为鞍点。在物理上要广泛一些，指在一个方向是极大值，另一个方向是极小值的点If SGD went into local minimum we will stuck at this point because the gradient is zero.遇到极小值点会stuckAlso in saddle points the gradient will be zero so we will stuck.Saddle points says that at some point:鞍点在gradient上的表现Some gradients will get the loss up.Some gradients will get the loss down.And that happens more in high dimensional (100 million dimension for example)The problem of deep NN is more about saddle points than about local minimum because deep NN has high dimensions (Parameters)Mini batches are noisy because the gradient is not taken for the whole batch.SGD + momentum:引入momentum，解决在鞍点或者极小值点处出现gradient为0而无法及继续更新参数的情况Build up velocity as a running mean of gradients:123# Computing weighted average. rho best is in range [0.9 - 0.99]V[t+1] = rho * v[t] + dxx[t+1] = x[t] - learningRate * V[t+1]V[0] is zero.Solves the saddle point and local minimum problems.It overshoots the problem and returns to it back.Nestrov momentum:1234dx = compute_gradient(x)old_v = vv = rho * v - learning_rate * dxx+= -rho * old_v + (1+rho) * vAdaGrad12345678grad_squared = 0while(True): dx = compute_gradient(x) # here is a problem, the grad_squared isn't decayed (gets so large) grad_squared += dx * dx x -= (learning_rate*dx) / (np.sqrt(grad_squared) + 1e-7)RMSProp12345678grad_squared = 0while(True): dx = compute_gradient(x) #Solved ADAgra grad_squared = decay_rate * grad_squared + (1-grad_squared) * dx * dx x -= (learning_rate*dx) / (np.sqrt(grad_squared) + 1e-7)People uses this instead of AdaGradAdam,最常用的优化器，结合了momentum和RMSPropCalculates the momentum and RMSProp as the gradients.It need a Fixing bias to fix starts of gradients.Is the best technique so far runs best on a lot of problems.With beta1 = 0.9 and beta2 = 0.999 and learning_rate = 1e-3 or 5e-4 is a great starting point for many models!Learning decay学习率退火，避免因为网络的不断加深而导致学习率相对参数而言过大Ex. decay learning rate by half every few epochs.To help the learning rate not to bounce out.Learning decay is common with SGD+momentum but not common with Adam.Dont use learning decay from the start at choosing your hyperparameters. Try first and check if you need decay or not.Regularization:损失函数中引入正则化项；集成学习；drop out修改网络结构；数据增强So far we have talked about reducing the training error, but we care about most is how our model will handle unseen data!上述优化更多的是在做如何更新参数使得error减少，但我们更加关心的是模型的泛化能力What if the gab of the error between training data and validation data are too large?This error is called high variance.Model Ensemble:Algorithm:Train multiple independent models of the same architecture with different initializations.At test time average their results.It can get you extra 2% performance.It reduces the generalization error.You can use some snapshots of your NN at the training ensembles them and take the results.Regularization solves the high variance problem. We have talked about L1, L2 Regularization.L0-norm用于统计向量中非零元素的个数Some Regularization techniques are designed for only NN and can do better.Drop out:使得activation 函数失效，即让其输出在任何输入下都为0In each forward pass, randomly set some of the neurons to zero. Probability of dropping is a hyperparameter that are 0.5 for almost cases.训练过程中随机将部分神经元设置为失活So you will chooses some activation and makes them zero.It works because:It forces the network to have redundant representation; prevent co-adaption of features!If you think about this, It ensemble some of the models in the same model!相当于集成学习，在一个模型中将他的多个不同的子模型进行集成At test time we might multiply each dropout layer by the probability of the dropout.Sometimes at test time we don’t multiply anything and leave it as it is.With drop out it takes more time to train.Dropout是一种在深度学习环境中应用的正规化手段。它是这样运作的：在一次循环中我们先随机选择神经层中的一些单元并将其临时隐藏，然后再进行该次循环中神经网络的训练和优化过程。在下一次循环中，我们又将隐藏另外一些神经元，如此直至训练结束。在训练时，每个神经单元以概率p被保留(dropout丢弃率为1-p)；在测试阶段，每个神经单元都是存在的，权重参数w要乘以p，成为：pw。测试时需要乘上p的原因：考虑第一隐藏层的一个神经元在dropout之前的输出是x，那么dropout之后的期望值是E=px+(1−p)0 ，在测试时该神经元总是激活，为了保持同样的输出期望值并使下一层也得到同样的结果，需要调整x→pxx→px. 其中p是Bernoulli分布（0-1分布）中值为1的概率Data augmentation:Another technique that makes Regularization.Change the data!For example flip the image, or rotate it.Example in ResNet:Training: Sample random crops and scales:Pick random L in range [256,480]Resize training image, short side = LSample random 224x224 patch.Testing: average a fixed set of cropsResize image at 5 scales: {224, 256, 384, 480, 640}For each size, use 10 224x224 crops: 4 corners + center + flipsApply Color jitter or PCATranslation, rotation, stretching.Drop connectLike drop out idea it makes a regularization.Instead of dropping the activation, we randomly zeroing the weights.Transfer learning:Some times your data is overfitted by your model because the data is small not because of regularization.自己的数据集太小You need a lot of data if you want to train/use CNNs.Steps of transfer learningTrain on a big dataset that has common features with your dataset. Called pretraining.找到一个和你的小的数据集特征类似的大的数据集，并用你的模型在该数据集中进行训练Freeze the layers except the last layer and feed your small dataset to learn only the last layer.将模型中除了最后一层外的所有层结构进行冻结，然后在小的数据集中进行训练，以学习最后一层Not only the last layer maybe trained again, you can fine tune any number of layers you want based on the number of data you have Deep learning softwareCPU vs GPUGPU The graphics card was developed to render graphics to play games or make 3D media,. etc.NVIDIA vs AMDDeep learning choose NVIDIA over AMD GPU because NVIDIA is pushing research forward deep learning also makes it architecture more suitable for deep learning.CPU has fewer cores but each core is much faster and much more capable; great at sequential tasks. While GPUs has more cores but each core is much slower “dumber”; great for parallel tasks.CPU的核心更少，但是更快，胜任串行任务；GPU的核心更多，但是更慢，胜任并行任务GPU cores needs to work together. and has its own memory.GPU各个核心需要并行工作，而且GPU有自己的内存，称为显存Matrix multiplication is from the operations that are suited for GPUs. It has MxN independent operations that can be done on parallel.矩阵乘法适用于GPU中Convolution operation also can be paralyzed because it has independent operations.卷积操作也能并行化Programming GPUs frameworks:CUDA(NVIDIA only)Write c-like code that runs directly on the GPU.Its hard to build a good optimized code that runs on GPU. Thats why they provided high level APIs.Higher level APIs: cuBLAS, cuDNN, etcCuDNN has implemented back prop. , convolution, recurrent and a lot more for you!In practice you won’t write a parallel code. You will use the code implemented and optimized by others!If you aren’t careful, training can bottleneck on reading data and transferring to GPU. So the solutions are:训练过程中在读取数据和迁移到gpu的过程中可能出现瓶颈Read all the data into RAM. # If possible将所有数据读如到内存Use SSD instead of HDD使用固态硬盘Use multiple CPU threads to prefetch data!使用多条CPU线程去预读取数据While the GPU are computing, a CPU thread will fetch the data for you.A lot of frameworks implemented that for you because its a little bit painful!Deep learning FrameworksIts super fast moving!Currently available frameworks:Tensorflow (Google)Caffe (UC Berkeley)Caffe2 (Facebook)Torch (NYU / Facebook)PyTorch (Facebook)Theano (U monteral)Paddle (Baidu)CNTK (Microsoft)MXNet (Amazon)The instructor thinks that you should focus on Tensorflow and PyTorch.The point of deep learning frameworks:Easily build big computational graphs.方便地构建计算图Easily compute gradients in computational graphs.方便地通过计算图计算梯度Run it efficiently on GPU (cuDNN - cuBLAS)支持GPUNumpy doesn’t run on GPU.Numpy不能使用GPUMost of the frameworks tries to be like NUMPY in the forward pass and then they compute the gradients for you.很多框架尽力去靠拢NUMPY，同时又能支持GPU**Tensorflow (Google)**静态架构Code are two parts:Define computational graph.定义好计算图Run the graph and reuse it many times.运行计算图并多次使用Tensorflow uses a static graph architecture.静态的图结构Tensorflow variables live in the graph. while the placeholders are feed each run.variable在计算图中生存，placeholders用于在计算图中占位，运行时填入数据Global initializer function initializes the variables that lives in the graph.Use predefined optimizers and losses.使用预先定义的优化器和损失函数You can make a full layers with layers.dense function.Keras(High level wrapper):Keras is a layer on top pf Tensorflow, makes common things easy to do.So popular!Trains a full deep NN in a few lines of codes.There are a lot high level wrappers:KerasTFLearnTensorLayertf.layers #Ships with tensorflowtf-Slim #Ships with tensorflowtf.contrib.learn #Ships with tensorflowSonnet # New from deep mindTensorflow has pretrained models that you can use while you are using transfer learning.迁移学习的时候可以使用多个预训练模型Tensorboard adds logging to record loss, stats. Run server and get pretty graphs! Tensorboard添加了日志用于跟踪损失It has distributed code if you want to split your graph on some nodes.Tensorflow is actually inspired from Theano. It has the same inspirations and structure.PyTorch (Facebook)Has three layers of abstraction:Tensor: ndarraybut runs on GPU,Like numpy arrays in tensorflowVariable: Node in a computational graphs; stores data and gradient #Like Tensor, Variable, PlaceholdersModule: A NN layer; may store state or learnable weights#Like tf.layers in tensorflowIn PyTorch the graphs runs in the same loop you are executing which makes it easier for debugging. This is called a dynamic graph.动态图架构，容易进行debuggingIn PyTorch you can define your own autograd functions by writing forward and backward for tensors. Most of the times it will implemented for you.一般只用重写forward函数Torch.nn is a high level api like keras in tensorflow. You can create the models and go on and on.You can define your own nn module!Also Pytorch contains optimizers like tensorflow.It contains a data loader that wraps a Dataset and provides minbatches, shuffling and multithreading.自己编写数据加载器很重要PyTorch contains the best and super easy to use pretrained modelsPyTorch contains Visdom that are like tensorboard. but Tensorboard seems to be more powerful.提供Visdom用于记录日志PyTorch is new and still evolving compared to Torch. Its still in beta state.PyTorch is best for research.对于research更加常用Tensorflow builds the graph once, then run them many times (Called static graph)定义一次网络即可多次使用，有专门的保存方式,在工业中更常用In each PyTorch iteration we build a new graph (Called dynamic graph)每次使用都要重新搭建网络,在研究中更常用Tensorflow fold make dynamic graphs easier in Tensorflow through dynamic batching.Dynamic graph applications include: recurrent networks and recursive networks.Caffe2 uses static graphs and can train model in python also works on IOS and AndroidTensorflow/Caffe2 are used a lot in production especially on mobile. CNN architecturesFocuses on CNN architectures that won ImageNet competition since 2012.These architectures includes: AlexNet, VGG, GoogLeNet, and ResNet.Also we will discuss some interesting architectures as we go.The first ConvNet that was made was LeNet-5 architectures are:by Yann Lecun at 1998.Architecture are: CONV-POOL-CONV-POOL-FC-FC-FCAlexNet (2012):ConvNet that started the evolution and wins the ImageNet at 2012.Architecture are: CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MAXPOOL3-FC6-FC7-FC8Contains exactly 8 layers the first 5 are Convolutional and the last 3 are fully connected layers.Some other details:First use of RELU.Norm layers but not used any more.heavy data augmentationDropout 0.5batch size 128SGD momentum 0.9Learning rate 1e-2 reduce by 10 at some iterations7 CNN ensembles!AlexNet was trained on GTX 580 GPU with only 3 GB which wasn’t enough to train in one machine so they have spread the feature maps in half. The first AlexNet was distributed!Its still used in transfer learning in a lot of tasks.Total number of parameters are 60 millionVGGNet (2014) (Oxford)Deeper network with more layers.Contains 19 layers.Won on 2014 with GoogleNet with error 7.3%Smaller filters with deeper layers.The great advantage of VGG was the insight that multiple 3 × 3 convolution in sequence can emulate the effect of larger receptive fields, for examples 5 × 5 and 7 × 7.Used the simple 3 x 3 Conv all through the network.Has a similar details in training like AlexNet. Like using momentum and dropout.在卷积神经网络中，感受野的定义是 卷积神经网络每一层输出的特征图（feature map）上的像素点在原始图像上映射的区域大小，即每一层feature的感受野都是对于原始输入图像而言的，而不是上一层的输入,但是在计算过程中，需要逐层计算该层在往上的每一层的感受野;公式 (N-1)_RF = f(N_RF, stride, ksize) = (N_RF - 1) * stride(convN) +ksize(convN)，其中，RF是感受野。N_RF和RF有点像，N代表 neighbour，指的是第n层的 a feature在n-1层的RF,显然第N层feature map在第N层的RF=1,在N-1层的RF=ksize__convN，计算RF时不需要考虑padding的影响。Feature Map的尺寸=(input_size + 2 * padding_size − ksize)/stride+1根据定义 感受野是决定某一层输出结果中一个元素所对应的输入层的区域大小这里指的是要求解的那层的一个元素也就是最初输入的out=1:rfsize = f(out, stride, ksize) = (out - 1) * stride + ksize感受野近似于用feature map为1时反推input_size ，只是不考虑paddingGoogleNet (2014)Deeper network with more layers.Contains 22 layers.It has Efficient Inception module.Only 5 million parameters! 12x less than AlexNetWon on 2014 with VGGNet with error 6.7%Inception module:内含并行操作的多种conv,将他们各自的输出在最后按depth堆叠成一个输出.Design a good local network topology (network within a network (NiN)) and then stack these modules on top of each other.It consists of:Apply parallel filter operations on the input from previous layerMultiple convs of sizes (1 x 1, 3 x 3, 5 x 5)Adds padding to maintain the sizes.Pooling operation. (Max Pooling)Adds padding to maintain the sizes.Concatenate all filter outputs together depth-wise.For example:Input for inception module is 28 x 28 x 256Then the parallel filters applied:(1 x 1), 128 filter # output shape (28,28,128)(3 x 3), 192 filter # output shape (28,28,192)(5 x 5), 96 filter # output shape (28,28,96)(3 x 3) Max pooling # output shape (28,28,256)After concatenation this will be (28,28,672)By this design -We call Naive- it has a big computation complexity.The last example will make:[1 x 1 conv, 128] ==&gt; 28 * 28 * 128 * 1 * 1 * 256 = 25 Million approx[3 x 3 conv, 192] ==&gt; 28 * 28 * 192 *3 *3 * 256 = 346 Million approx[5 x 5 conv, 96] ==&gt; 28 * 28 * 96 * 5 * 5 * 256 = 482 Million approxIn total around 854 Million operation!Solution:bottleneck layers that use 1x1 convolutions to reduce feature depth.Inspired from NiN (Network in network)So GoogleNet stacks this Inception module multiple times to get a full architecture of a network that can solve a problem without the Fully connected layers.Just to mention, it uses an average pooling layer at the end before the classification step.Full architecture:In February 2015 Batch-normalized Inception was introduced as Inception V2. Batch-normalization computes the mean and standard-deviation of all feature maps at the output of a layer, and normalizes their responses with these values.ResNet (2015) (Microsoft Research)152-layer model for ImageNet. Winner by 3.57% which is more than human level error.This is also the very first time that a network of &gt; hundred, even 1000 layers was trained.Swept all classification and detection competitions in ILSVRC’15 and COCO’15!What happens when we continue stacking deeper layers on a “plain” Convolutional neural network?The deeper model performs worse, but it’s not caused by overfitting!网络变深,准确率反而下降The learning stops performs well somehow because deeper NN are harder to optimize!网络越深，越难优化，容易出现vanish gradientThe deeper model should be able to perform at least as well as the shallower model.A solution by construction is copying the learned layers from the shallower model and setting additional layers to identity mapping.如何保证加深的网络能够学到新的知识，并保证旧的知识也保存了下来？identity mapping的存在就是这个价值。Residual block:Microsoft came with the Residual block which has this architecture:12# Instead of us trying To learn a new representation, We learn only ResidualY = (W2* RELU(W1x+b1) + b2) + XSay you have a network till a depth of N layers. You only want to add a new layer if you get something extra out of adding that layer.One way to ensure this new (N+1)th layer learns something new about your network is to also provide the input(x) without any transformation to the output of the (N+1)th layer. This essentially drives the new layer to learn something different from what the input has already encoded.这样可以驱使新的网络层学习与前N层已经学到的东西不同的内容The other advantage is such connections help in handling the Vanishing gradient problem in very deep networks.这样的结构有助于解决在很深的网络中导致的梯度消失问题With the Residual block we can now have a deep NN of any depth without the fearing that we can’t optimize the network.ResNet with a large number of layers started to use a bottleneck layer similar to the Inception bottleneck to reduce the dimensions.Resnet中包含了大量的类似inception bottleneck设计的bottlenect layer用于减少维度Full ResNet architecture:Stack residual blocks.Every residual block has two 3 x 3 conv layers.Additional conv layer at the beginning.No FC layers at the end (only FC 1000 to output classes)Periodically, double number of filters and downsample spatially using stride 2 (/2 in each dimension)Training ResNet in practice:Batch Normalization after every CONV layer.Xavier/2 initialization from He et al.SGD + Momentum (0.9)Learning rate: 0.1, divided by 10 when validation error plateausMini-batch size 256Weight decay of 1e-5No dropout used.ResNets Improvements:(2016) Identity Mappings in Deep Residual NetworksFrom the creators of ResNet.Gives better performance.(2016) Wide Residual NetworksArgues that residuals are the important factor, not depth50-layer wide ResNet outperforms 152-layer original ResNetIncreasing width instead of depth more computationally efficient (parallelizable)(2016) Deep Networks with Stochastic DepthMotivation: reduce vanishing gradients and training time through short networks during training.Randomly drop a subset of layers during each training passUse full deep network at test time.Beyond ResNets:(2017) FractalNet: Ultra-Deep Neural Networks without ResidualsArgues that key is transitioning effectively from shallow to deep and residual representations are not necessary.Trained with dropping out sub-pathsFull network at test time.(2017) Densely Connected Convolutional Networks(2017) SqueezeNet: AlexNet-level Accuracy With 50x Fewer Parameters and &lt;0.5Mb Model SizeGood for production.It is a re-hash of many concepts from ResNet and Inception, and show that after all, a better design of architecture will deliver small network sizes and parameters without needing complex compression algorithms.Conclusion:ResNet current best default.Trend towards extremely deep networksIn the last couple of years, some models all using the shortcuts like “ResNet” to eaisly flow the gradients. Recurrent Neural networksRecurrent Neural Networks RNN Models:One to manyExample: Image Captioningimage ==&gt; sequence of wordsMany to OneExample: Sentiment Classificationsequence of words ==&gt; sentimentMany to manyExample: Machine Translationseq of words in one language ==&gt; seq of words in another languageExample: Video classification on frame levelSo what is a recurrent neural network?Recurrent core cell that take an input x and that cell has an internal state that are updated each time it reads an input.The RNN block should return a vector.We can process a sequence of vectors x by applying a recurrence formula at every time step:1h[t] = fw (h[t-1], x[t]) # Where fw is some function with parameters WThe same function and the same set of parameters are used at every time step.(Vanilla) Recurrent Neural Network:12h[t] = tanh (W[h,h]*h[t-1] + W[x,h]*x[t]) # Then we save h[t]y[t] = W[h,y]*h[t]This is the simplest example of a RNN.RNN works on a sequence of related data.Recurrent NN Computational graph:h0 are initialized to zero.Gradient of W is the sum of all the W gradients that has been calculated!A many to many graph:​","categories":[{"name":"cs231n","slug":"cs231n","permalink":"http://aier02.com/categories/cs231n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"}]},{"title":"optimization","slug":"optimization","date":"2018-10-05T12:44:14.537Z","updated":"2018-10-05T15:46:13.106Z","comments":true,"path":"2018/10/05/optimization/","link":"","permalink":"http://aier02.com/2018/10/05/optimization/","excerpt":"","text":"OptimizationOptimization is the process of finding the set of parameters W that minimize the loss function,如何改变w使得损失函数不断减小 Strategy #1 random search核心思想:一次性找到使得损失函数最小的w取值貌似很难？(random search,a bad idea），但是iterative refinement直观上是可行的，故可以给予w一个random 值，然后迭代的改进，使得loss每次都比上次要小。Our strategy will be to start with random weights and iteratively refine them over time to get lower loss类似于一个戴眼罩的hiker，在多个角度下尝试往下走，直到山底。如在CIFAR-10中，hills are 30730 dimensional. Strategy #2 random local search策略一相当于随意选定某条路径前进，若根据该路径直接走到的地方更低，则选择该方向最优；而策略二相当于先随意选定某条路径，然后循环地在这个路径上进行随意的方向修改，若走到的地方更低，则进行路径的修改，直到循环结束。 Strategy #3 follow gradient不用刻意规划路线，而是要找到每一步的最优方向，即loss下降的方向gradinet，也就是当时脚下的hills的斜坡方向所谓gradient就是a generalization of slope for functions that don’t take a single number but a vector of numbers，由在所求函数对整个输入空间中的各个维度的derivative，即偏导数组成。一维导数的定义:df(x)dx=limh→0f(x+h)−f(x)h\\frac{df(x)}{dx} = \\lim_{h\\ \\to 0} \\frac{f(x + h) - f(x)}{h}​dx​​df(x)​​=​h →0​lim​​​h​​f(x+h)−f(x)​​When the functions of interest take a vector of numbers instead of a single number,(即自变量不止一个,对不同的变量进行求导) we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension.gradient是函数增长速率最快的方向,The gradient tells us the slope of the loss function along every dimension, which we can use to make an update。 Computing gradient两种方式:数值解和分析解 Numerical gradient实质是根据一维导数的公式进行计算，每次在输入x的一个维度上进行数值求解，即让该维度上旧的值加上指定的h(尽可能小，公式中是goes toward zero)，算得f(x+h)后再算(f(x+h)-f(x))/h，以求得在该维度上的偏导数的近似值,计算完某个维度后注意要设置回原来的输入再进行下个维度的计算.it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])创建了一个numpy数组的迭代器，这里的flags表示对数组进行多重索引，op_flags表示迭代器对数组x可执行读写操作。print it.multi_index可以得到数组元素所有的索引(以元组形式返回)，如(0,0),(0,1)在实际应用中，计算梯度的时候常用centered difference formulaf(x+h)−f(x−h)2h\\frac{f(x+h)-f(x-h)}{2h}​2h​​f(x+h)−f(x−h)​​根据gradient，在每个维度上朝着loss funcitn增长速率最快的方向的负方向进行step_size的更新。Update in negative gradient direction. In the code above, notice that to compute W_new we are making an update in the negative direction of the gradient df since we wish our loss function to decrease, not increase.step_size的作用:the gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should step效率问题:显然对于每个维度(数组索引)进行数值求解，则对于gradient的求解是O(n)，n为w的维度，或者wijw_{ij}w​ij​​的个数；对于神经网络而言，参数个数太多，数值解扩展性太差。 Analytic gradient通过微积分进行导数的推导,得到导数确切的值，而不是近似解。求导数容易出错，所以经常讲分析解和数值解进行比较，称为gradient check。举例SVM loss function，对于单个示例xix_ix​i​​:Li=∑j≠yi[max(0,wjTxi−wyiTxi+Δ)]L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \\Delta) \\right]L​i​​=​j≠y​i​​​∑​​[max(0,w​j​T​​x​i​​−w​y​i​​​T​​x​i​​+Δ)]对于wyiw_{y_i}w​y​i​​​​,∇wyiLi=−(∑j≠yi1(wjTxi−wyiTxi+Δ&gt;0))xi\\nabla_{w_{y_i}} L_i = - \\left( \\sum_{j\\neq y_i} \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta &gt; 0) \\right) x_i∇​w​y​i​​​​​​L​i​​=−​⎝​⎛​​​j≠y​i​​​∑​​1(w​j​T​​x​i​​−w​y​i​​​T​​x​i​​+Δ&gt;0)​⎠​⎞​​x​i​​where 𝟙 is the indicator function that is one if the condition inside is true or zero otherwise对于wjw_jw​j​​,∇wjLi=1(wjTxi−wyiTxi+Δ&gt;0)xi\\nabla_{w_j} L_i = \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta &gt; 0) x_i∇​w​j​​​​L​i​​=1(w​j​T​​x​i​​−w​y​i​​​T​​x​i​​+Δ&gt;0)x​i​​Once you derive the expression for the gradient it is straight-forward to implement the expressions and use them to perform the gradient update Gradient descentthe procedure of repeatedly evaluating the gradient and then performing a parameter update is called Gradient Descent123while True: weights_grad = evaluate_gradient(loss_fun, data, weights) weights += - step_size * weights_grad # perform parameter updateMini-batch gradient descent.对于输入数据量庞大的training set，为了更新w的某个索引下的值而对庞大的data进行操作有点浪费，常用的方法是compute the gradient over batches of the training data1234while True: data_batch = sample_training_data(data, 256) # sample 256 examples weights_grad = evaluate_gradient(loss_fun, data_batch, weights) weights += - step_size * weights_grad # perform parameter update为何在一小部分的数据中更新w也是可行的？The reason this works well is that the examples in the training data are correlatedThe extreme case of this is a setting where the mini-batch contains only a single example. This process is called Stochastic Gradient Descent (SGD) (or also sometimes on-line gradient descent)比较少见 Summary两种求gradient的方法:We discussed the tradeoffs between computing the numerical and analytic gradient. The numerical gradient is simple but it is approximate and expensive to compute. The analytic gradient is exact, fast to compute but more error-prone since it requires the derivation of the gradient with math. Hence, in practice we always use the analytic gradient and then perform a gradient check, in which its implementation is compared to the numerical gradient梯度下降方法:We introduced the Gradient Descent algorithm which iteratively computes the gradient and performs a parameter update in loop总的来说优化就是通过寻找loss function下降速度最快的方向(gradient的反方向)，对w进行不断的适当幅度(learning rate)的更新，使得loss不断减少。","categories":[{"name":"cs231n","slug":"cs231n","permalink":"http://aier02.com/categories/cs231n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"},{"name":"image classification","slug":"image-classification","permalink":"http://aier02.com/tags/image-classification/"}]},{"title":"linear classification","slug":"linear_classification","date":"2018-10-03T05:31:44.756Z","updated":"2018-12-16T12:01:20.756Z","comments":true,"path":"2018/10/03/linear_classification/","link":"","permalink":"http://aier02.com/2018/10/03/linear_classification/","excerpt":"","text":"Linear classification Multiclass SVM基本形式为y=wx+b，此时的x为列向量，一列为一个样本，w的每一行为一个class的template。loss function:Multiclass Support Vector Machine (SVM) loss,SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin Δ;Δ为超参，需要人为设定，它的存在说明多类svm关注的和普通的svm思想上是一致的，都是关注距离超平面一定范围内的误分类点，也就是间隔边界内的点，所以这里的损失函数和合页损失函数的设计是一样的；故第i张图像的损失函数为Li=∑j≠yimax(0,sj−syi+Δ)L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)L​i​​=​j≠y​i​​​∑​​max(0,s​j​​−s​y​i​​​​+Δ)注意这里的sj表示的是该图像在第j类的得分，而yi表示的是该图像的label，即只要计算其错误分类的所有分数和正确分类的分数的差值之和，当label类的得分没有大于某个非label类得分margin时，两者的差值会被算入loss中。引入regularizationR(W)=∑k∑lWk,l2R(W) = \\sum_k\\sum_l W_{k,l}^2R(W)=​k​∑​​​l​∑​​W​k,l​2​​完整的multicalss SVM loss:L = \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss}扩展形式为:L=1N∑i∑j≠yi[max(0,f(xi;W)j−f(xi;W)yi+Δ)]+λ∑k∑lWk,l2L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\lambda \\sum_k\\sum_l W_{k,l}^2L=​N​​1​​​i​∑​​​j≠y​i​​​∑​​[max(0,f(x​i​​;W)​j​​−f(x​i​​;W)​y​i​​​​+Δ)]+λ​k​∑​​​l​∑​​W​k,l​2​​λ\\lambdaλ为超参，常用cross validation确定，表示模型的一种偏好。L2范数作为正则化项的好处是:The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself,即在wx+b得分一样的情况下，L2范数的模型偏向于选择smaller and diffuse weights，使得没有哪个维度影响很大。setting delta:一般设置为1.0是安全的，在loss function中delta和lambda其实具有相同effect在tradeoff上，所以真正有意义的是对于lambda的控制与二分类的svm比较:Li=Cmax(0,1−yiwTxi)+R(W)L_i = C \\max(0, 1 - y_i w^Tx_i) + R(W)L​i​​=Cmax(0,1−y​i​​w​T​​x​i​​)+R(W)这里的yi∈{−1,1}y_i \\in \\{ -1,1 \\}y​i​​∈{−1,1} Softmax classifierf(xi;W)=Wxif(x_i; W) = W x_if(x​i​​;W)=Wx​i​​和svm保持一致，但经过softmax层用于指示概率的大小，损失函数由hinge loss变成cross-entropy lossL_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}fj(z)=ezj∑kezkf_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}f​j​​(z)=​∑​k​​e​z​k​​​​​​e​z​j​​​​​​称为softmax function,这里的z即为wx+b，整个softmax function estimated class probabilities，即unnormalized log probabilities交叉熵:p为true distibution，q为estimated distributionH(p,q)=−∑xp(x)logq(x)H(p,q) = - \\sum_x p(x) \\log q(x)H(p,q)=−​x​∑​​p(x)logq(x)Practical issues: Numeric stability:efyi∑jefj=CefyiC∑jefj=efyi+logC∑jefj+logC\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}} = \\frac{Ce^{f_{y_i}}}{C\\sum_j e^{f_j}} = \\frac{e^{f_{y_i} + \\log C}}{\\sum_j e^{f_j + \\log C}}​∑​j​​e​f​j​​​​​​e​f​y​i​​​​​​​​=​C∑​j​​e​f​j​​​​​​Ce​f​y​i​​​​​​​​=​∑​j​​e​f​j​​+logC​​​​e​f​y​i​​​​+logC​​​​常用的设置方法是logC=−maxjfj\\log C = -\\max_j f_jlogC=−max​j​​f​j​​The Softmax classifier gets its name from the softmax function, which is used to squash the raw class scores into normalized positive values that sum to one, so that the cross-entropy loss can be applied. SVM vs. Softmax主要区别在有loss function:两者的分数向量f都是一样的，不同的在于对f的解释，svm认为f是对应种类的得分，hinge loss鼓励正确的class得分比所有错误的class score都高出一个margin；而softmax认为f是在没有标准化之前表示的是属于某个种类的log概率，并且cross entropy鼓励正确的正确分类的概率大(-log§变小)Hence, the probabilities computed by the Softmax classifier are better thought of as confidences where, similar to the SVM Further ReadingDeep Learning using Linear Support Vector Machines from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.","categories":[{"name":"cs231n","slug":"cs231n","permalink":"http://aier02.com/categories/cs231n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"},{"name":"linear classification","slug":"linear-classification","permalink":"http://aier02.com/tags/linear-classification/"},{"name":"SVM","slug":"SVM","permalink":"http://aier02.com/tags/SVM/"}]},{"title":"pytorch cookbook U2&U3","slug":"pytorch_cookbook-U2&U3","date":"2018-10-02T08:27:32.398Z","updated":"2018-11-07T07:21:58.471Z","comments":true,"path":"2018/10/02/pytorch_cookbook-U2&U3/","link":"","permalink":"http://aier02.com/2018/10/02/pytorch_cookbook-U2&U3/","excerpt":"","text":"pytorch-cookbook 第二章函数名后带下划线会修改函数本身如y.add_(x)会直接修改ypytorch的tensor和numpy的对象共享内存，两者同时改变;对于tensor不支持的操作，可以先转为numpy进行操作在转为tensor（tensor支持gpu）1234a=t.ones(5)b = a.numpy() # Tensor -&gt; Numpya = np.ones(5)b = t.from_numpy(a) # Numpy-&gt;Tensortensor[idx]得到的为0-dim的tensor，scalar.item()获取tensor的单个元素对象t.Tensor(5,3)创建5行3列的tensor，t.tensor([3,4])创建包含3，4两个元素的tensort.tensor()会进行数据拷贝，新的tensor和旧的不共享内存，而torch.from_numpy（）或者tensor.detach()则相反使用gpu123device = t.device(&quot;cuda:0&quot; if t.cuda.is_available() else &quot;cpu&quot;)x = x.to(device)y = y.to(device)autograd: 自动微分;要想使得Tensor使用autograd功能，只需要设置tensor.requries_grad=True.如：x = t.ones(2, 2, requires_grad=True)注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以反向传播之前需把梯度清零。# 以下划线结束的函数是inplace操作，会修改自身的值，就像add__1x.grad.data.zero_()一起求导的过程示例123456x=t.ones(2,2,requires_grad=True)#生成tensory=x.sum()#定义表达式y.grad_fn#查看求导函数y.backward()#back propagationx.grad#查看y对x的导数x.grad.data_zero_()#清空导数缓存空间nerual network的定义主要是对torch.nn模块的使用,定义网络时，需要继承nn.Module，并实现它的forward方法，把网络中具有可学习参数的层放在构造函数__init__中。如果某一层(如ReLU)不具有可学习的参数，则既可以放在构造函数中，也可以不放，但建议不放在其中，而在forward中使用nn.functional代替,forwar的输入和输出都是tensor,input = t.randn(1, 1, 32, 32),需要注意的是，torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。但如果只想输入一个样本，则用 input.unsqueeze(0)将batch_size设为１,size形式为nSamples x nChannels x height x weight123456789101112131415161718192021222324252627282930import torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): # nn.Module子类的函数必须在构造函数中执行父类的构造函数 # 下式等价于nn.Module.__init__(self) super(Net, self).__init__() # 卷积层 &apos;1&apos;表示输入图片为单通道, &apos;6&apos;表示输出通道数，&apos;5&apos;表示卷积核为5*5 self.conv1 = nn.Conv2d(1, 6, 5) # 卷积层 self.conv2 = nn.Conv2d(6, 16, 5) # 仿射层/全连接层，y = Wx + b self.fc1 = nn.Linear(16*5*5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # 卷积 -&gt; 激活 -&gt; 池化 x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) x = F.max_pool2d(F.relu(self.conv2(x)), 2) # reshape，‘-1’表示自适应 x = x.view(x.size()[0], -1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return xnet = Net()print(net)conv layer主要特征是局部连接和权重共享局部连接：每个神经元仅与输入神经元的一块区域连接，这块局部区域称作感受野（receptive field）。在图像卷积操作中，即神经元在空间维度（spatial dimension，即上图示例H和W所在的平面）是局部连接，但在深度上是全部连接。对于二维图像本身而言，也是局部像素关联较强。这种局部连接保证了学习后的过滤器能够对于局部的输入特征有最强的响应。局部连接的思想，也是受启发于生物学里面的视觉系统结构，视觉皮层的神经元就是局部接受信息的。*权重共享：计算同一个深度切片的神经元时采用的滤波器是共享的。例上图中计算o[:,:,0]的每个每个神经元的滤波器均相同，都为W0，这样可以很大程度上减少参数。共享权重在一定程度上讲是有意义的，例如图片的底层边缘特征与特征在图中的具体位置无关。但是在一些场景中是无意的，比如输入的图片是人脸，眼睛和头发位于不同的位置，希望在不同的位置学到不同的特征 。请注意权重只是对于同一深度切片的神经元是共享的，在卷积层，通常采用多组卷积核提取不同特征，即对应不同深度切片的特征，不同深度切片的神经元权重是不共享。另外，偏重对同一深度切片的所有神经元都是共享的。池化是非线性下采样的一种形式，主要作用是通过减少网络的参数来减小计算量，并且能够在一定程度上控制过拟合。网络的可学习参数通过net.parameters()返回,net.named_parameters可同时返回可学习的参数及名称。nn.MSELoss()实现均方误差，nn.CrossEntropyLoss()实现交叉熵损失优化器更新参数1234567891011121314151617 import torch.optim as optim #新建一个优化器，指定要调整的参数和学习率optimizer = optim.SGD(net.parameters(), lr = 0.01) # 在训练过程中 # 先梯度清零(与net.zero_grad()效果一样)optimizer.zero_grad() # 计算损失output = net(input)loss = criterion(output, target) #反向传播loss.backward() #更新参数optimizer.step()数据加载和预处理：使用torchvision示例：下面我们来尝试实现对CIFAR-10数据集的分类，步骤如下:使用torchvision加载并预处理CIFAR-10数据集,得到dataset和dataloader定义网络,继承nn.Module,init中写入可学习的参数函数，forward定义好前向传播的过程定义损失函数和优化器，criterion和optimizer训练网络并更新网络参数，在每个ephco中加载数据，传入net，算loss，loss.backward，optimizer.step测试网络定义对数据的预处理:将两种转化合并一起；ToTensor()将shape为(H, W, C)的nump.ndarray或img转为shape为(C, H, W)的tensor，其将每一个数值归一化到[0,1]，其归一化方法比较简单，直接除以255即可，加入normalize则其作用就是先将输入归一化到(0,1)，再使用公式”(x-mean)/std”，将每个元素分布到(-1,1),函数normalize（std,mean）1234transform = transforms.Compose([ transforms.ToTensor(), # 转为Tensor transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # 归一化 ])Dataset对象是一个数据集，可以按下标访问，返回形如(data, label)的数据。Dataloader是一个可迭代的对象，它将dataset返回的每一条数据拼接成一个batch，并提供多线程加速优化和数据打乱等操作。当程序对dataset的所有数据遍历完一遍之后，相应的对Dataloader也完成了一次迭代，先定义好dataset，然后定义dataloader对指定的dataset进行操作123456789101112 # 训练集trainset = tv.datasets.CIFAR10( root=&apos;/home/cy/tmp/data/&apos;, train=True, download=True, transform=transform)trainloader = t.utils.data.DataLoader( trainset, batch_size=4, shuffle=True, num_workers=2)进行normaliza的必要性：每个样本图像减去数据集图像的均值后除以方差，保证了所有图像的分布相似，使得model训练的时候更快的收敛.训练网络的示例12345678910111213141516171819202122232425262728t.set_num_threads(8)for epoch in range(2): running_loss = 0.0 for i, data in enumerate(trainloader, 0): # 输入数据 inputs, labels = data # 梯度清零 optimizer.zero_grad() # forward + backward outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() # 更新参数 optimizer.step() # 打印log信息 # loss 是一个scalar,需要使用loss.item()来获取数值，不能使用loss[0] running_loss += loss.item() if i % 2000 == 1999: # 每2000个batch打印一下训练状态 print(&apos;[%d, %5d] loss: %.3f&apos; \\ % (epoch+1, i+1, running_loss / 2000)) running_loss = 0.0print(&apos;Finished Training&apos;) 第三章表3-1: 常见新建tensor的方法函数功能Tensor(*sizes)基础构造函数tensor(data,)类似np.array的构造函数ones(*sizes)全1Tensorzeros(*sizes)全0Tensoreye(*sizes)对角线为1，其他为0arange(s,e,step从s到e，步长为steplinspace(s,e,steps)从s到e，均匀切分成steps份rand/randn(*sizes)均匀/标准分布normal(mean,std)/uniform(from,to)正态分布/均匀分布randperm(m)随机排列除了tensor.size()，还可以利用tensor.shape直接查看tensor的形状，tensor.shape等价于tensor.size()tensor = t.Tensor(1,2)创建了一个size为【1，2】的张量vector = t.tensor([1, 2])创建了一个值为（1，2）的向量，size为2scalar = t.tensor(3.14159) 创建了一个值为3.14159的标量，size为【】,区别于size【0】，empty_tensor = t.tensor([])，size存在即为tensor,否则为scalar通过tensor.view方法可以调整tensor的形状，但必须保证调整前后元素总数一致。view不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。b = a.view(-1, 3)当某一维为-1的时候，会自动计算它的大小,torch.squeeze() 这个函数主要对数据的维度进行压缩，去掉维数为1的的维度，比如是一行或者一列这种，一个一行三列（1,3）的数去掉第一个维数为一的维度之后就变成（3）行。squeeze(a)就是将a中所有为1的维度删掉。不为1的维度没有影响。a.squeeze(N) 就是去掉a中指定的维数为一的维度。还有一种形式就是b=torch.squeeze(a，N) a中去掉指定的定的维数为一的维度。torch.unsqueeze()这个函数主要是对数据维度进行扩充。给指定位置加上维数为一的维度，比如原本有个三行的数据（3），在0的位置加了一维就变成一行三列（1,3）。a.unsqueeze(N) 就是在a中指定位置N加上一个维数为1的维度。还有一种形式就是b=torch.unsqueeze(a，N) a就是在a中指定位置N加上一个维数为1的维度resize是另一种可用来调整size的方法，但与view不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存，当再次扩展时其值为当时缩小保存的值tensor的索引操作和tensor共享内存，即更改其中一个，另一个也会更改。a[None]:None类似于np.newaxis, 为a新增了一个轴；等价于a.view(1, a.shape[0], a.shape[1])a &gt; 1 # 返回一个ByteTensor,即满足条件的位置值为1，否则为0对tensor的任何索引操作仍是一个tensor，想要获取标准的python对象数值，需要调用tensor.item(), 这个方法只对包含一个元素的tensor适用","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://aier02.com/categories/pytorch/"}],"tags":[{"name":"basic knowledge","slug":"basic-knowledge","permalink":"http://aier02.com/tags/basic-knowledge/"},{"name":"pytorch cookbook","slug":"pytorch-cookbook","permalink":"http://aier02.com/tags/pytorch-cookbook/"}]},{"title":"first time in Kaggle-summary","slug":"rsna_summary","date":"2018-10-01T16:08:39.573Z","updated":"2018-12-17T01:59:12.360Z","comments":true,"path":"2018/10/02/rsna_summary/","link":"","permalink":"http://aier02.com/2018/10/02/rsna_summary/","excerpt":"","text":"项目存在的问题和解决方案 RSNA 如何开始比赛完全是新手，很早以前就有学长介绍过kaggle，最近看完了cs231n，然后打算试试手，但是一开始并不知道应该做什么，于是就上知乎直接搜了如何打kaggle比赛，找到的很多的都是ml的，但是个人更喜欢cv，添加了关键词后，找到了一篇和我类似经历的blog，知道了kaggle比赛到底是什么，他的每个板块表示的是什么，常用的步骤是什么，有什么需要注意的（这个时候开始意识到我没有gpu，这点必定是后期的瓶颈） 选择平台没有gpu怎么办，没有资源只能去租用云平台跑model，开头抱有侥幸心理，有没有什么免费的平台呢，一开始找到的是google 的colab，听说是免费的，然后我就直接尝试使用，跑了一下mnist的基本模型发现经常自动断开连接，而且他给的免费的硬盘空间只有15g，当时我首先打算打的比赛其实是有关air ship的detection比赛。果然天底下没有免费的午餐，只能另找平台，后来在知乎上看到不少人推荐极客云，他自动帮你搭建好了深度学习的环境，（其实这是个坑，创建的dl环境是不能直接操作整个系统的，任何系统的指令都无法操作，这使得我后来无法进行端口的查询，visdom启动的时候总是提示端口被占用），运行的速度和价钱的确很吸引人 EDA什么是lung opacities，什么是pneumonia，他们两者有什么关系么？由于缺乏领域知识，得益于知乎，我直接奔向了discussion和kernel，果然找到了一篇名为what r lung opacities的kernel，这篇文章由一个具备radiologist领域知识的kaggler提供，他直接介绍了怎么看chest x-ray，黑色的为air，白色的为bone，grey为tissue或者fluid，这个kernel对于我整个项目影响最大，他补充了我xray的领域知识。做eda主要是看各个文件中的数量、样本中是否存在重复，是否有缺失，class_label.csv文件中有多少中不同的class，各自的数量如何，他与train数据集是否一一对应，open（）函数，创建一个reader，next跳过header之后就循环读取reader实现按行读取数据；也可用pd.read_csv实现，value_counts()统计重复的次数，groupby（keyword）可以根据keyword分组；主要用到了matplotlib.pyplot，numpy，pandas，pydicom，glob进行数据可视化 detection？还是segmentation？诚然我一开始看到官方的比赛介绍的时候，我主观上是认为要根据dicom图像提供的病人的个人信息以及图像中的信息进行classification，根据我个人对于部分pneumonia的xray的观察，以及提供的数据集中的bboxes的信息和之前提到的那篇kernel的引导，本次比赛更多的是做能指示pneumonia的lung opacities，而且明显是难以完全sgment出来的（opacities有一种是模糊了心脏和lung的边界。于是决定做detection 数据准备前期所做的数据准备，我的想法是直接把pneumonia的bbox的x,y,w,h装入内存，很明显这中做法有一定的风险，遇到内存不足恐怕直接爆了，后期我选择了使用npy文件先预先读取train_lable的信息，然后每次使用的时候再进行读取，这样的做法个人认为可以避免每次训练的时候进行pydicom数据的转换，也能减少内存的使用，后果用了更多的磁盘空间，而且每次读取npy文件都是存在时间成本的。而且有个严重的问题，就是后期如何做数据增强，是对image进行操作，pydicom.dcmread()和cv2.imread读都是hwc，而且是BGR上的0-255为了使后期进行读取方便，我把使用sitk.ReadImage（dcm）将所有的dcm图像转为png格式，sitk.GetArrayFromImage返回的是（hwc），同时存储患有肺炎的图像的numpy数组和bbox的位置信息（ymin,xmin,ymax,xmax），以npy文件的格式保存，实际上在这次操作中图片都是单通道的，而且都是1024*1024的。然后通过对patientid进行shuffle，创建train和validation，大概9:1 数据增强经过eda后，lung opacities，no lung opacities/not normal,normal,比例是1:1:1，把lung opacities作为positive，则dataset中存在样例不平衡，考虑进行图像的增强，旋转，平移，亮度改变之类的，由于是x-ray图像，考虑进行图像的仿射变换，但是旋转会涉及bbox的改变，因为之前在数字图像课程实验中学过仿射变换（旋转，缩放，平移）然后就直接实现了rotate_img_bbox(img, bboxes, degree=-45, scale=1.)，mat = cv2.getRotationMatrix2D(center,angle=degree,scale=scale) #affine matrix，仿射矩阵为（2，3）的矩阵，以水平进行划分，前（2，2）子矩阵为线性变换矩阵，（2，1）子矩阵为平移矩阵，用标量的形式来看就是ax+b；T=M【x,y,1】,对原来的bbox的四条边的四个中点进行相同的矩阵变化，然后合并为一个矩阵，表示一个仿射后的矩阵，矩形边框（Bounding Rectangle）是说，用一个最小的矩形，把找到的形状包起来。还有一个带旋转的矩形，面积会更小，即使用rx, ry, rw, rh = cv2.boundingRect(concat)得到摆正后的经过仿射变换的新的bbox，整个图像的变换out_image = cv2.warpAffine(img,mat,(width,height))做各种变换的时候由于数据集较大，而且操作多，经常出现等待时间较长的情况，缺乏可视化，不能确定是完成了操作还是仍然在等待，后来在循环中使用了tqdm进行进度条的显示，可视化了进程的进度。 准备dataset创建类generator(keras.utils.Sequence)，实现的函数分别有内置init()，初始化文件夹路径，文件名，bboxing box，batch_size=32,image_size=256(原图为1024)，shuffle，augment，predict为三者为真还是假内置len（），返回filenames中数据量的大小内置getitem（），以batchz_size为数据单位，根据index确定数据的位置，当predict为真时，返回对应文件名的图像和文件名；否则，返回对应文件名的img图像和msk，注意msks即为bboxes的列表内置load（），通过filename确定patientid，然后读取npy文件获取img_array和bboxes列表（每一项为对应图片的bbox的ymin,xmin,ymax,xmax），创建和img等大小的全0msk，根据npy中的‘bboxes’项将对应的标注框区域设置为1；然后resize图片，特别注意由于训练的时候进行的是batch_train,所以对于img和msk都必须添加一个dimension，作为训练的维度内置loadpredict（），基本操作和load函数一致，不同在与该函数不用获取bboxes信息 搭建神经网络create_downsample(channels, inputs)下采样函数，channels指示filter的大小，inputs指示images，依次使用的是keras.layers.BN(momentum=0.9)-&gt;leakyrelu-&gt;conv2d(padding=same)-&gt;maxpool2d。padding：补0策略，为“valid”, “same” 。“valid”代表只进行有效的卷积，即对边界数据不处理。“same”代表保留边界处的卷积结果，通常会导致输出shape与输入shape相同create_resblock(channels, inputs)resblock函数，一个resblock包含了BN-&gt;LEAKYRELU-&gt;CONV2D-&gt;BN-&gt;LEAKYRELU-&gt;CON2D-&gt;add([x, inputs])(channels用于conv层中filters的数目，即输出的维度，kernel_size指定filter的大小)create_network(input_size, channels, n_blocks=2, depth=4)搭建整个网络，inputs = keras.Input(shape=(input_size, input_size, 1))对输入进行规范化-&gt;conv2d-&gt;创建depth层结构，每个结构中包括了一个downsample和n_block个resblock-&gt;ouput layer依次为BN-&gt;LEAKYRELU-&gt;CONV2D-&gt;UpSampling2D(2**depth)(x)-&gt;将input和output包装为一个model即model = keras.Model(inputs=inputs, outputs=outputs) Resnet背景:随着网络的加深，出现了训练集准确率下降的现象，排除overfitting，针对该问题提出了resnet，以允许实现尽可能地加深网络resnet中提出了两种mapping，identity mapping 和 residual mapping，输出为y=F(x)+x,显然x为前者，F(x)为后者；理论上，对于“随着网络加深，准确率下降”的问题，Resnet提供了两种选择方式，也就是identity mapping和residual mapping，如果网络已经到达最优，继续加深网络，residual mapping将被push为0，只剩下identity mapping，这样理论上网络一直处于最优状态了，网络的性能也就不会随着深度增加而降低了。即真正学习的是残差，而形式上y保证了不会出现网络加深而经验误差增大的现象在resnet中加入1*1的conv layer就是bottleneck layer，目的是为了降维，降低计算量和参数的数目，最后又升维是为了保持和输入x的dimensions一致 Batch NormalizationBatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。(IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。然后提出了BatchNorm的基本思想：能不能让每个隐层节点的激活输入分布固定下来呢？这样就避免了“Internal Covariate Shift”问题了。启发式思考：对输入图像进行白化（Whiten）操作的话——所谓白化，就是对输入数据分布变换到0均值，单位方差的正态分布——那么神经网络会较快收敛；BN所做的可以理解为对深层神经网络每个隐层神经元的激活值做简化版本的白化操作简而言之，对于每个隐层神经元，把逐渐向非线性函数映射后向取值区间极限饱和区靠拢的输入分布强制拉回到均值为0方差为1的比较标准的正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。因为梯度一直都能保持比较大的状态，所以很明显对神经网络的参数调整效率比较高，就是变动大，就是说向损失函数最优值迈动的步子大，也就是说收敛加快。将输入x的分布强制转换到均值为0，方差为1的正态分布。经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)，每个神经元增加了两个参数scale和shift参数，这两个参数是通过训练学习到的，意思是通过scale和shift把这个值从标准正态分布左移或者右移一点并长胖一点或者变瘦一点①不仅仅极大提升了训练速度，收敛过程大大加快；②还能增加分类效果，一种解释是这是类似于Dropout的一种防止过拟合的正则化表达方式，所以不用Dropout也能达到相当的效果；③另外调参过程也简单多了，对于初始化要求没那么高，而且可以使用大的学习率等 定义loss function定义iou损失函数:iou_loss(y_true, y_pred),这里的label(即y)是mask，即bboxes为1的0-1图像，intersection = tf.reduce_sum(y_true * y_pred)求的共同区域的1的个数，score = (intersection + 1.) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)，注意+1.是为了保证score不为0，公式为inserction/union。reduce_sum返回该矩阵所有元素之和合并损失函数:0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * iou_loss(y_true, y_pred)，binary_crossentropy为对数损失函数损失函数是训练的关键，他表达了模型的目标，通过求原图像经过resnet后的feature map和图像的label，即bboxes的iou，使用adam进行优化，使得网络参数朝iou减少的方向进行更新。 训练参数的设置定义tf.metric算子，即评估指标算子，用于计算accuracymodel.compile(optimizer=‘adam’,loss=iou_bce_loss,metrics=[‘accuracy’, mean_iou])指定优化器，损失函数和accuracy初始化lr=0.01，epoch=25，lr的更新策略:lrx(np.cos(np.pi*x/epochs)+1.)/2,x为动态变化的epochkeras.callbacks.LearningRateScheduler(schedule)该回调函数是用于动态设置学习率，其中schedule函数以epoch号为参数（从0算起的整数），返回一个新学习率（浮点数） 创建train和validation generator指定图片文件夹:folder = ‘./input/stage_1_train_png’训练生成器:train_gen = generator(folder, train_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=True, augment=True, predict=False)测试生成器:valid_gen = generator(folder, valid_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=False, predict=False) 训练模型history = model.fit_generator(train_gen, validation_data=valid_gen, callbacks=[learning_rate], epochs=25, workers=4, use_multiprocessing=True)用于指定callback内容，即learning rate，训练集，验证集，epoch数量，线程数量 测试模型创建测试数据生成器:test_gen = generator(folder, test_filenames, None, batch_size=25, image_size=256, shuffle=False, predict=True)threshold predicted mask,pred中大于0.5才为maskcomp = pred[:, :, 0] &gt; 0.5apply connected componentscomp = measure.label(comp),measure.label作用是给comp标记连通区域，用于确定不同的bbox，measure.regionprops(comp)获取comp的不同连通区域，返回的region列表，每一个分别为ymin,xmin,ymax,xmax计算置信度proxy for confidence score:conf = np.mean(pred[y:y+height, x:x+width]) 提交结果保存结果到csv文件中:以字典的形式保存到csv文件中save dictionary as csv filesub = pd.DataFrame.from_dict(submission_dict,orient=‘index’)指定索引名称:sub.index.names = [‘patientId’]指定列名:sub.columns = [‘PredictionString’]写入指定目录文件:sub.to_csv(’/input/submission.csv’)","categories":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://aier02.com/categories/Kaggle/"}],"tags":[{"name":"segmentation","slug":"segmentation","permalink":"http://aier02.com/tags/segmentation/"},{"name":"keras","slug":"keras","permalink":"http://aier02.com/tags/keras/"},{"name":"ResNet","slug":"ResNet","permalink":"http://aier02.com/tags/ResNet/"},{"name":"jikecloud","slug":"jikecloud","permalink":"http://aier02.com/tags/jikecloud/"}]},{"title":"python rubbish collection","slug":"python_rubbish_collection","date":"2018-09-28T07:32:02.260Z","updated":"2018-10-03T08:08:16.164Z","comments":true,"path":"2018/09/28/python_rubbish_collection/","link":"","permalink":"http://aier02.com/2018/09/28/python_rubbish_collection/","excerpt":"","text":"python垃圾回收机制 引用计数每个对象维护一个ob_ref字段，每次被别的对象引用的ob_ref加1，若引用失效，则减1，当ob_ref为0则该对象被回收，占用的内存空间被释放。但是该方法不能解决循环引用问题，即两个对象相互引用，当他们的外部引用都失效时，ob_ref仍为1，非零，但是他们实质是要被回收的，而python却不能将其回收 标记清除对活动对象进行标记，将非活动对象进行回收。对象之间通过引用（指针）连在一起，构成一个有向图，对象构成这个有向图的节点，而引用关系构成这个有向图的边。从根对象（root object）出发，沿着有向边遍历对象，可达的（reachable）对象标记为活动对象，不可达的对象就是要被清除的非活动对象。根对象就是全局变量、调用栈、寄存器。 分代回收根据对象存活时间的不同对内存进行了划分，时间由短到长分别划分为年轻代，中年代，老年代，新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发，把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，依此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。","categories":[{"name":"python","slug":"python","permalink":"http://aier02.com/categories/python/"}],"tags":[{"name":"basic knowledge","slug":"basic-knowledge","permalink":"http://aier02.com/tags/basic-knowledge/"}]},{"title":"faster RCNN","slug":"faster_RCNN","date":"2018-09-18T10:55:07.479Z","updated":"2018-10-03T08:08:10.513Z","comments":true,"path":"2018/09/18/faster_RCNN/","link":"","permalink":"http://aier02.com/2018/09/18/faster_RCNN/","excerpt":"","text":"faster RCNN感受野：在卷积神经网络CNN中，决定某一层输出结果中一个元素所对应的输入层的区域大小，被称作感受野receptive fieldfatal error: numpy/arrayobject.h: No such file or directory”​这个错误的出现可以如下解决，将setup.py内容加入一条include_dirs=[numpy.get_include()]​就可以了。示例setup.py文件如下：from distutils.core import setupfrom distutils.extension import Extensionfrom Cython.Distutils import build_extimport numpy as npext_modules=[Extension(“test03”,[“test03.pyx”])]setup(name=‘gravity_cy’,cmdclass={‘build_ext’:build_ext},include_dirs = [np.get_include()],ext_modules=ext_modules) 主要问题是要指定numpy的路径过程记录：1）下载faster-rcnn的预训练模型并在demo中进行调试2）运行python misc/convert_caffe_pretrain.py把预训练的vgg16下载并保存到checkpoint中，作为cnn提取器3）","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://aier02.com/categories/pytorch/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://aier02.com/tags/CNN/"}]},{"title":"analysis kernel demo","slug":"analysis_kernel_demo","date":"2018-09-18T10:55:03.519Z","updated":"2018-10-30T02:57:12.890Z","comments":true,"path":"2018/09/18/analysis_kernel_demo/","link":"","permalink":"http://aier02.com/2018/09/18/analysis_kernel_demo/","excerpt":"","text":"analysis kernel demoBatchNormalization层：该层在每个batch上将前一层的激活值重新规范化，即使得其输出数据的均值接近0，其标准差接近1BNleakyRelu层,大于0为x，小于0为ax(a常为0.01)pytorch自定义dataset和dataloadertorch.utils.data.Dataset是表示数据集的抽象类。您自定义的数据集应该继承Dataset并重写以下方法：len使用len(dataset)将返回数据集的大小。getitem 支持索引，dataset[i]可以获取第i个样本让我们为我们的人脸标记点数据集创建一个数据集类。我们将在init中读取csv，而将在getitem存放读取图片的任务。因为所有的图像不是一次性存储在内存中，而是根据需要进行读取，这样可以高效的使用内存。123456789101112131415class CustomDataset(data.Dataset):#需要继承data.Dataset def __init__(self): # TODO # 1. Initialize file path or list of file names. pass def __getitem__(self, index): # TODO # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open). # 2. Preprocess the data (e.g. torchvision.Transform). # 3. Return a data pair (e.g. image and label). #这里需要注意的是，第一步：read one data，是一个data pass def __len__(self): # You should change 0 to the total size of your dataset. return 0将numpy ndarry转为tensor：123456789101112class ToTensor(object): \"\"\"Convert ndarrays in sample to Tensors.\"\"\" def __call__(self, sample): image, landmarks = sample['image'], sample['landmarks'] # swap color axis because # numpy image: H x W x C # torch image: C X H X W image = image.transpose((2, 0, 1)) return &#123;'image': torch.from_numpy(image), 'landmarks': torch.from_numpy(landmarks)&#125;","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://aier02.com/categories/pytorch/"}],"tags":[{"name":"demo","slug":"demo","permalink":"http://aier02.com/tags/demo/"}]},{"title":"first time in Kaggle-preparation","slug":"rsna_pneumonia_detection","date":"2018-09-06T09:08:00.932Z","updated":"2018-10-03T08:15:40.619Z","comments":true,"path":"2018/09/06/rsna_pneumonia_detection/","link":"","permalink":"http://aier02.com/2018/09/06/rsna_pneumonia_detection/","excerpt":"","text":"Question descriptionIn this competition, you’re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs.(CXR)胸部X线片上肺部阴影的定位以跟踪可能的肺炎病况They see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review.But the title “Pneumonia Detection” for the competition is misleading because you actually have to do “Lung Opacities Detection”, and lung opacities are not the same as pneumonia. Lung opacities are vague, fuzzy clouds of white in the darkness of the lungs, which makes detecting them a real challenge.-from a kernal如何确认x照片中的阴影？ Basic informationin CXR:black-air;white-bones;grey-fluid or tissueUsually the lungs are full of air. When someone has pneumonia, the air in the lungs is replaced by other material - fluids, bacteria, immune system cells, etc. That’s why areas of opacities are areas that are grey but should be more black.(找出原来是黑但是变灰了的区域As you can see, lung opacities are not homogenoues and they do not have a clear center or clear boundaries. I don’t think you can properly segment opacities out of the entire picture because there are no clear boundries.-直接找阴影貌似很难，可以先segment lungsin fact, there was only a moderate level of agreement between radiologists about the presence of infiltrates, which are opacities by definition预测任务为病人的dcm图像找到lung opacities,以bbox的形式给出，在submission_file中，一张图片中只能有一条bbox信息，每4个信息项为一个bbox，即(x,y,weight,height) Difficultiesthere is a mass of tissue surrounding the lungs and between the lungs. These areas contain skin, muscles, fat, bones, and also the heart and big blood vessels. That translates into a lot of information on the chest radiograph that is not useful for this competition.阴影有多种，怎样的阴影才和肺炎相关The main difference in the types of opacities between these two patients is the borders and the shape of the opacity, Patient 3 has multiple round and clearly defined opacities. Patient 2 has this poorly defined haziness which obscures the margins of the lungs and heart. This haziness is termed consolidation.Exclude: obvious mass(es), nodule(s), lobar collapse, linear atelectasis要找的阴影是模糊的，难懂的，不明显的In the cases labeled Not Normal/No Lung Opacity, no lung opacity refers to no opacity suspicious for pneumonia. 在数据集中，不正常/没有肺部阴影，是指没有与肺炎相关的阴影，但会有与其他病症相关的阴影，故是不正常。两种肺炎阴影：Ground-Glass Opacities；Consolidations前者We can see that the lungs are “whiter” than they should be, but we can see most of the borders of the lungs and hear后者There are fuzzy areas in the lungs and the borders of the lungs and heart cannot be seen.玻璃粉状的阴影不会影响心脏和肺部的边界，而合并类的阴影会模糊两者的边界预测pneumoni和opacities应该独立预测？lung opacities并不是是pneumonia的充要条件实际上该比赛为Lung Opacities Detectio，而不是Pneumonia Detection exprolatory data analysisstage_1_detailed_class_info里面包括了3种class，共有28989条记录，每条信息为patientid：class，标注了病人id对应的正常、不正常/不是肺炎，肺炎3种情况stage_1_train_labels记录了bbox的位置，同样有28989条记录，每条记录分别是patientid：x,y,width,height,target;x,y表示bbox的左上角的坐标，width是宽度，即x的范围，height是高度，即y的范围，target=0表示没有肺炎，=1表示有肺炎，对应class_info，normal则bbox信息empty，target=0；No Lung Opacity / Not Normal则bbox信息empty，target=0；因此lung opacities in data is associated with pneumonia；lung opacities 则包含了bbox的信息同时target=1注意数据集存在一名病人对应多条记录的情况，并不是有28989名病人，经过eda可知存在25684名病人（patientid）(training data 中有25684张dcm)实际上因为bbox信息表是一条记录只能表示一个bbox，故存在一个病人的cxr中有多个bbox，而class表和label表相一致，所以保持对应关系class存在重复数据训练集中dcm文件不只是image，还有meta information，如sex，age等，是否需要添加此类属性，从而考虑相关的内容进行判断？patientId - A patientId. Each patientId corresponds to a unique image.x_ - the upper-left x coordinate of the bounding box.y_ - the upper-left y coordinate of the bounding box.width_ - the width of the bounding box.height_ - the height of the bounding box.Target_ - the binary Target, indicating whether this sample has evidence of pneumonia.No Lung Opacity / Not Normal and Normal have together the same percent (69.077%) as the percent of missing values for target window in class details information.显然存在正样本偏少的情况，需要做数据增强，过采样？角度偏转？ models ChexNetThe CheXNet algorithm is a 121-layer deep 2D Convolutional Neural Network; a Densenet after Huang &amp; Liu. The Densenet’s multiple residual connections reduce parameters and training time, allowing a deeper, more powerful model. The model accepts a vectorized two-dimensional image of size 224 pixels by 224 pixels.CheXNet is a 121-layer Dense Convolutional Network (DenseNet) (Huang et al., 2016) trained on the ChestX-ray 14 dataset. DenseNets improve flow of information and gradients through the network, making the optimization of very deep networks tractable. We replace the final fully connected layer with one that has a single output, after which we apply a sigmoid nonlinearity. The weights of the network are initialized with weights from a model pretrained on ImageNet (Deng et al., 2009). The network is trained end-to-end using Adam with standard parameters (ß1 = 0.9 and ß2 = 0.999) (Kingma &amp; Ba, 2014). We train the model using minibatches of size 16. We use an initial learning rate of 0.001 that is decayed by a factor of 10 each time the validation loss plateaus after an epoch, and pick the model with the lowest validation loss.","categories":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://aier02.com/categories/Kaggle/"}],"tags":[{"name":"segmentation","slug":"segmentation","permalink":"http://aier02.com/tags/segmentation/"},{"name":"chest X-ray","slug":"chest-X-ray","permalink":"http://aier02.com/tags/chest-X-ray/"},{"name":"ChexNet","slug":"ChexNet","permalink":"http://aier02.com/tags/ChexNet/"},{"name":"EDA","slug":"EDA","permalink":"http://aier02.com/tags/EDA/"}]},{"title":"experience from a Kaggler","slug":"kaggle_expericence","date":"2018-09-05T02:19:10.278Z","updated":"2018-10-03T08:08:35.464Z","comments":true,"path":"2018/09/05/kaggle_expericence/","link":"","permalink":"http://aier02.com/2018/09/05/kaggle_expericence/","excerpt":"","text":"kaggle比赛步骤-经验1.仔细阅读比赛介绍和数据描述；2.查找相似的Kaggle比赛。作为一个接触不久的Kaggler，我已经完成对所有Kaggle比赛基本分析的收集；3.研究相似比赛的解决方案；4.阅读有关论文，以确保不错过该领域的最新进展；5.分析数据，并构建可靠的交叉验证结果；6.数据预处理、特征工程和模型训练。7.结果分析，包括如预测分布、错误分析和困难样本等；8.根据分析来改进模型或设计新模型；9.基于数据分析和结果分析来设计模型以增加多样性或解决困难样本；10.模型集成；11.必要时返回到前面的某个步骤。Q：你觉得，赢得比赛的关键是什么？可靠的验证方式，借鉴其他比赛并阅读相关论文，以及良好的自制力和心理素质。Q：你认为你最具竞争力的比赛技巧或方法是什么？我认为应该是在比赛开始时准备解决方案的文档。我会强迫自己写出一份清单，包括面临的挑战、应该阅读的解决方案和论文、可能的风险、可用的验证方式、可能的数据增强方法以及增加模型多样性的方式。而且，我不断更新这个文档。幸运地，这些文档为我后面在很多比赛中取得不错成绩提供了支持","categories":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://aier02.com/categories/Kaggle/"}],"tags":[{"name":"experience","slug":"experience","permalink":"http://aier02.com/tags/experience/"}]},{"title":"a failed experience in Kaggle","slug":"airbus_ship_detection","date":"2018-09-05T02:18:30.433Z","updated":"2018-10-03T08:08:00.121Z","comments":true,"path":"2018/09/05/airbus_ship_detection/","link":"","permalink":"http://aier02.com/2018/09/05/airbus_ship_detection/","excerpt":"","text":"airbus-ship-detection challenge 问题描述：从卫星图像中找到ships，并用bbox分割出来 难点所在并不是所有图片都有ships在图片中ships的大小不一致图像分割不允许存在部分重叠，但数据集中的ships当两者直接相邻时，存在轻微的overlap，重叠部分将会被作为背景而移除部分带有人工标注的图片中的bbox可能存在边界像素的丢失train-ship-segmentations.csv提供了人工标注的bbox作为训练的数据图片，其中bbox以run-length encoding 表示。数据集很大，需要用到gpu，训练模型可能要几天时间run-length-encoding评价方式为IoU,即人工标注的bbox和预测的bbox的相交部分与两者合并的部分的占比。 EDA(exploratory data analysis)The images (at least many of them) are slices of the same image, not separate frames taken at different times.data leak:The images in test are just shifted versions of images in train. The problem also happens in train, it looks like airbus had bigger images and then the 768x768 are random crops of the bigger images; but it looks they didn’t check whether there were any overlaps.How to find:Run nearest neighbors on all imagesFor each image take N closest neighbors and find where it overlapsCutting all the images 256x256 they can be found easier because they match almost exactly (except for compression artifacts). up to nowdue to data leak,large data set,few computation source and time limit,I decided to pause the competition.","categories":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://aier02.com/categories/Kaggle/"}],"tags":[{"name":"segmentation","slug":"segmentation","permalink":"http://aier02.com/tags/segmentation/"}]},{"title":"Introduction to statistical learning method U1","slug":"statistic_1","date":"2018-08-27T16:58:59.803Z","updated":"2019-01-07T03:44:15.658Z","comments":true,"path":"2018/08/28/statistic_1/","link":"","permalink":"http://aier02.com/2018/08/28/statistic_1/","excerpt":"","text":"李航老师的《统计学习方法》基本已经过一遍了，感觉只是略懂皮毛，现为了加强知识点的认识和部分课后题的实现，有必要进行个人总结。 统计学习 定义statistical learning 是关于计算机基于数据构建构建统计模型并运用模型对数据进行预测和分析的一门学科，是概率论、统计学、信息论、计算理论、最优化理论和计算机科学等学科的交叉学科。 学习对象和目的从数据出发，提取数据的特征，抽象出数据的模型（概率统计模型），并对数据进行预测和分析 实现统计学习方法的一般步骤得到一个有限的训练数据集合确定包含所有可能的模型的假设空间，即学习的模型的集合确定模型选择的准则，即学习的策略实现求解最优模型的算法，即学习的算法通过学习方法选择最优的模型利用最优模型对新的数据进行预测或则分析简而言之：数据-&gt;假设空间-&gt;策略-&gt;算法-&gt;最优模型-&gt;预测分析 监督学习统计学习包括了监督学习，非监督学习，半监督学习和强化学习。 基本概念输入所有可能取值的集合称为输入空间，同理输出所有可能的取值集合称为输出空间，通常output space 远小于 input space；而特征空间则是所有特征向量所在的空间，特征向量用于表示一个输入实例，特征空间的每个维度对应一个特征，输入空间可以和特征空间相同，也可把输入空间映射到空间，模型实质是定义在特征空间上的（对特征向量进行学习）。X表示输入变量，Y表示输出变量，输入变量X中的第i个表示为xi=(xi(1),xi(2),...,xi(n))Tx_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^Tx​i​​=(x​i​(1)​​,x​i​(2)​​,...,x​i​(n)​​)​T​​N个数据的训练数据集(labled)表示为T={(x1,y1),(x2,y2),...,(xN,yN)}T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}T={(x​1​​,y​1​​),(x​2​​,y​2​​),...,(x​N​​,y​N​​)}输入到输出的映射关系由模型进行表示，所有可能的由输入空间（特征空间）到输出空间的映射的集合组成了假设空间，学习的范围局限在假设空间中。 统计学习的三要素 模型统计学习首先考虑是学习什么样的模型，在监督学习中就是要学习的条件概率分布或者决策函数，由所有可能的模型构成的集合组成了假设空间F,通常是一个由参数决定的函数族.决策函数模型的集合表示为$$F={f|Y=f_\\theta(X),\\theta\\in R^n}$$条件概率模型的集合表示为$$F={f|P=P_\\theta(Y|X),\\theta\\in R^n}$$其中θ\\thetaθ取值于n维的欧氏空间，称为参数空间 策略有了假设空间，接着考虑按照什么样的准则学习最优的模型，称为策略。 损失函数和风险函数损失函数度量模型预测的好坏（预测的结果和标记的差距），风险函数度量平均意义下的模型预测的好坏（在具体某次预测可能出错的期望） 损失函数记作L(f(X),Y)&gt;=0,常见类型:0-1损失函数（0-1 loss fuction ）：L(f(X),Y)={1,Y≠f(X)0,Y=f(X)L(f(X),Y) = \\begin{cases} 1, &amp; Y \\neq f(X)\\\\ 0, &amp; Y = f(X) \\end{cases}L(f(X),Y)={​1,​0,​​​Y≠f(X)​Y=f(X)​​平方损失函数(quadratic loss function):L(f(X),Y)=(Y−f(X))2L(f(X),Y) =(Y-f(X))^2L(f(X),Y)=(Y−f(X))​2​​绝对损失函数(absolute loss fuction):L(f(X),Y)=∣(Y−f(X))∣L(f(X),Y) =|(Y-f(X))|L(f(X),Y)=∣(Y−f(X))∣ 经验风险和结构风险模型f(x)关于训练数据集的平均损失称为经验风险：Remp(f)=1N∑i=1NL(yi,f(xi))R_{emp}(f) =\\frac 1N\\sum_{i=1}^NL(y_i,f(x_i))R​emp​​(f)=​N​​1​​​i=1​∑​N​​L(y​i​​,f(x​i​​))结构风险简单而言就是经验风险加入了正则化项，正则化项用于表示模型复杂度，因此结构风险最小化是为了防止过拟合而提出的策略。Rsrm(f)=1N∑i=1NL(yi,f(xi))+λJ(x)R_{srm}(f) =\\frac 1N\\sum_{i=1}^NL(y_i,f(x_i))+\\lambda J(x)R​srm​​(f)=​N​​1​​​i=1​∑​N​​L(y​i​​,f(x​i​​))+λJ(x)其中$\\lambda $&gt;=0,用以权衡经验风险和模型复杂度 算法算法是指学习模型的具体方法，一般而言就是求解模型f中的参数以达到模型的最优化，故很多时候统计学习的算法到最后基本都是最优化问题。 模型评估和模型选择 训练误差训练误差是模型关于训练数据集的平均损失，即前面所说的经验风险；R_{exp}(f) =\\frac 1N\\sum_{i=1}^NL(y_i,f(x_i))​ 测试误差测试误差跟训练误差相似，只是前者是在测试数据集上的平均损失 过拟合和模型选择一味最求模型对于训练数据的预测能力，所求得的模型往往会比真实模型更加复杂，这种现象叫做过拟合（模型参数过多，训练误差很小，但测试误差较大或者泛化能力很差），而模型的选择就是为了避免过拟合并提高模型对于未知数据的预测能力。 正则化模型选择的典型方法是正则化，通过结构风险最小化实现，即在经验风险后加上一个正则化项，一般是模型复杂度的单调递增函数（比如L_2范数）正则化的作用是选择经验风险和模型复杂度同时较小的模型；问题是为什么需要简单的模型？一方面是过拟合的存在使得模型有可能过度关注训练数据集的“个性”，导致模型的泛化能力下降；另一个解释是根据奥卡姆剃刀原理，“如无必要，勿增实体”，即在所有可能选择的模型中，能够很好地解释数据并且十分简单的模型才是最优的（尽管这个要求有点自相矛盾，此时λ\\lambdaλ往往起到折中的作用）；从贝叶斯估计的角度而言，正则化项对应于模型的先验概率，复杂的模型先验概率小，简单的模型先验概率大。 交叉验证基本思想是重复地使用数据，对给定的数据进行划分，然后组合成训练集和测试集，反复进行训练、测试和模型的选择。 简单交叉验证随机把数据集划分为训练集和测试集两部分（训练集更大），改变不同的条件使得在相同的训练集上也能得到不同的模型，然后在测试集进行模型的测试和选择。 S折交叉验证随机地把数据集划分为S个互不相交的子数据集，利用S-1个子数据集作为训练数据集，剩下的一个做为测试集；将这一过程对可能的S中选择重复进行，从得到的S个模型中选择测试误差最小的模型。 留一交叉验证S=N时，用于数据缺乏的情况下，即每次只拿一个数据样本作为测试集，进行N次相同操作，从N个模型中选择最优者（平均测试误差最小）。 泛化能力generalization alibity指由学习方法得到的模型对于未知数据的预测能力，常用测试误差评价学习方法的泛化能力，仅记测试数据集是十分宝贵的，在学习模型的过程中不能使用，要在训练完成后才能用于测试，保证对于学得的模型而言，测试数据是“未知的”。对于泛化能力的分析往往通过研究泛化误差的概率上界进行，常具有以下性质：样本容量增加，泛化上界趋于0；假设空间容量越大，模型就越难学，泛化误差上界就越大。存在定理，对于二类分类问题，当假设空间是有限个函数的集合时F={f1,f2,f3…,fd},对任意一个函数f∈\\in∈F,至少以概率1-δ\\deltaδ,以下不等式成立：R(f)&lt;=R^(f)+ϵ(d,N,δ)R(f)&lt;=\\hat R(f)+\\epsilon (d,N,\\delta)R(f)&lt;=​R​^​​(f)+ϵ(d,N,δ)即泛化误差&lt;=训练误差+N的单调递减函数(右端即为泛化误差的上界)ϵ(d,N,δ)=12N(logd+log1δ)\\epsilon (d,N,\\delta)=\\sqrt {\\frac 1{2N}(logd+log\\frac 1\\delta)}ϵ(d,N,δ)=√​​2N​​1​​(logd+log​δ​​1​​)​​​可见泛化误差上界和训练误差、样本容量均正相关，和模型复杂度d负相关。 生成模型和判别模型 生成模型由生成方法学到的模型称为生成模型，生成方法由数据学习联合概率分布P(X,Y),然后求出条件概率分布P(Y|X)作为预测的模型，即：P(Y∣X)=P(X,Y)P(X)P(Y|X)=\\frac {P(X,Y)}{P(X)}P(Y∣X)=​P(X)​​P(X,Y)​​生成模型表示了给定输入X产生输出Y的生成关系，典型的生成模型由朴素贝叶斯法和隐马尔可夫模型。 判别模型由判别方法学到的模型称为判别模型，判别方法由数据直接学习决策函数f(X)或者条件概率P(Y|X)作为预测的模型。判别模型表示给定输入X应该预测什么样的输出Y。典型的判别模型有k紧邻法，感知机，决策树，逻辑斯谛回归模型，最大熵模型，支持向量机，提升方法，条件随机场等。存在隐变量时不能使用判别方法，但可以用生成方法；判别方法直接面对预测，准确率更高。","categories":[{"name":"statistical learning method","slug":"statistical-learning-method","permalink":"http://aier02.com/categories/statistical-learning-method/"}],"tags":[{"name":"basic knowledge","slug":"basic-knowledge","permalink":"http://aier02.com/tags/basic-knowledge/"},{"name":"introduction","slug":"introduction","permalink":"http://aier02.com/tags/introduction/"}]},{"title":"Build personal blog","slug":"blog_tutorial","date":"2018-08-07T11:24:35.583Z","updated":"2018-10-03T08:30:19.069Z","comments":true,"path":"2018/08/07/blog_tutorial/","link":"","permalink":"http://aier02.com/2018/08/07/blog_tutorial/","excerpt":"","text":"最近在加强ml和cv基础知识的学习，为了加深理解，同时记录自己的学习过程，尝试着写blog；向好友请教后得知建立个人blog的方式，我的选择是Hexo(一种静态博客网页框架)+Github page（免费托管博客项目代码），当然还有租云服务器和自己写后台和前端的方式，前者更加方便和易于上手。 前期准备 安装Git毕竟所有项目代码都是托管到Github上面，必须保证系统中已经安装了git，terminal中输入git不出现commant not found即可 安装Node.js因为Hexo的使用基于Node.js，所以要先安装Node.js和他的包安装器npm,建议到官网下载pkg并选择稳定版本(LTS) Github创建项目在Github上面创建一个名为：yourname.github.io 的空项目，yourname是指你创建的github账户名（github上面每个用户的用户名是唯一的标识），该Repository就是之后你的blog所有代码和文件存放的地方，blog的地址在不添加设置的情况下：https://yourname.github.io 安装Hexo在终端输入npm命令下载静态网页生成器Hexo1npm install -g hexo若出现permiss denied等情况，则加入sudo1sudo npm install -g hexo 创建Hexo文件夹在本地文件系统中创建Hexo文件夹，cd进入该文件夹，然后进行初始化1Hexo init没有错误，则以后所有操作都必须在该文件目录下进行，特别注意文件夹Hexo/node_models,所有通过npm install XXX --save操作下载的依赖都会存放在该文件夹中，易于包的管理(–save参数的意义就是存放在node_models)顺利完成上述步骤后可以登陆https://yourname.github.io 查看效果 绑定个人域名Ps:不想花钱的同学可以跳过 购买国外个人域名对于没有个人域名的同学来说，国内备案实在是太漫长了(我前段时间就在腾讯云备案过一次，周期太长==)，直接购买国外的域名方便而且预算基本一样，推荐namesilo，60一年，后缀多选择而且有免费的private protection；注册的时候我写的假的信息(除了邮箱）；一开始先检索你想注册的域名，然后在根据英文指导取购买就ok了(默认设置),网站支持alipay，非常方便了。 更改DNS统一使用一个dns的话对于以后多域名管会更加方便，推荐使用国内的厂商dnspod，登陆后添加你更注册的域名进行管理，主要添加连个主机记录，分别是@和www,譬如我的设置为图中A记录的记录值为github page中项目的ip，可以通过在终端中输入ping https://yourname.github.io 找到，也可在Github help中找到。登陆namesilo，在右侧找到domain manager,进入后选择更改nameserver,处理过程还是挺长的，所以不用急着登陆你的域名 更改blog主题在 Hexo 中有两份主要的配置文件，其名称都是 _config.yml。 其中，一份位于站点根目录下，主要包含 Hexo 本身的配置；另一份位于主题目录下，这份配置由主题作者提供，主要用于配置主题相关的选项。为了描述方便，在以下说明中，将前者称为站点配置文件， 后者称为主题配置文件 选择主题进入官网选择适合的主题[hexo](https://hexo.io/themes/)，进入相应的Github地址，git clone命令把整个文件夹都下载下来，存放在Hexo/themes文件夹中。 更改配置文件一般而言，下载下来的主题文件中都有新手引导，注意readme就ok了。 部署项目代码在Hexo/_config.yml和themes里面的config文件中添加以下说明1234deploy: type: git repo: https://github.com/aier02/aier02.github.io.git branch: master注意修改repo的地址为你的github上面的地址(只要修改你的用户名)在Hexo目录下输入1hexo clean &amp;&amp; hexo g &amp;&amp; hexo d即可将本地下的Hexo/public文件夹所有内容上传到github项目中(可能需要生成公私钥，请自行百度）hexo clean实质是删除Hexo/public，而hexo g则是根据配置和Hexo/source生成Hexo/public,新写的blog存放在Hexo/source/_posts即可，hexo d命令则是将Hexo/public上传到git服务器中,使用个人域名的同学还得在Github上面的yourname.github.io仓库根目录下新建一个CNAME文件，写入你的个人域名,不需要http和www,比如我的就是在第一行为：aier02.com 个人寄语blog关键在于内容，所以我基本很多操作都是为了方便，倘若遇到了什么问题可在下方评论提出(主要是看我是否也遇到了可以提供解决方案，没有踩过的坑我也不会=。=）,还有可以选择的theme很多，奉劝各位以实用为主，再好看的blog没有内容也是没价值的。","categories":[{"name":"blog","slug":"blog","permalink":"http://aier02.com/categories/blog/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://aier02.com/tags/Hexo/"},{"name":"Github page","slug":"Github-page","permalink":"http://aier02.com/tags/Github-page/"},{"name":"namesilo","slug":"namesilo","permalink":"http://aier02.com/tags/namesilo/"}]}]}