{"meta":{"title":"Aier02","subtitle":null,"description":null,"author":"æ˜“å®‰æ˜","url":"http://aier02.com"},"pages":[{"title":"404 Not Foundï¼šè¯¥é¡µæ— æ³•æ˜¾ç¤º","date":"2018-08-06T10:41:28.220Z","updated":"2018-08-04T16:46:32.021Z","comments":false,"path":"/404.html","permalink":"http://aier02.com//404.html","excerpt":"","text":""},{"title":"å…³äº","date":"2018-08-06T10:42:02.162Z","updated":"2018-08-04T16:46:32.022Z","comments":false,"path":"about/index.html","permalink":"http://aier02.com/about/index.html","excerpt":"","text":"ä¸ªäººè¯¦ç»†ä»‹ç»"},{"title":"ä¹¦å•","date":"2018-08-05T02:38:10.217Z","updated":"2018-08-04T16:46:32.022Z","comments":false,"path":"books/index.html","permalink":"http://aier02.com/books/index.html","excerpt":"","text":""},{"title":"åˆ†ç±»","date":"2018-08-05T02:38:10.227Z","updated":"2018-08-04T16:46:32.022Z","comments":false,"path":"categories/index.html","permalink":"http://aier02.com/categories/index.html","excerpt":"","text":""},{"title":"å‹æƒ…é“¾æ¥","date":"2018-08-05T02:19:59.559Z","updated":"2018-08-04T16:46:32.023Z","comments":true,"path":"links/index.html","permalink":"http://aier02.com/links/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2018-08-05T02:19:10.292Z","updated":"2018-08-04T16:46:32.023Z","comments":false,"path":"repository/index.html","permalink":"http://aier02.com/repository/index.html","excerpt":"","text":""},{"title":"æ ‡ç­¾","date":"2018-08-05T02:38:10.236Z","updated":"2018-08-04T16:46:32.023Z","comments":false,"path":"tags/index.html","permalink":"http://aier02.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"optimization","slug":"optimization","date":"2018-10-05T12:44:14.537Z","updated":"2018-10-05T15:46:13.106Z","comments":true,"path":"2018/10/05/optimization/","link":"","permalink":"http://aier02.com/2018/10/05/optimization/","excerpt":"","text":"OptimizationOptimization is the process of finding the set of parameters W that minimize the loss function,å¦‚ä½•æ”¹å˜wä½¿å¾—æŸå¤±å‡½æ•°ä¸æ–­å‡å° Strategy #1 random searchæ ¸å¿ƒæ€æƒ³:ä¸€æ¬¡æ€§æ‰¾åˆ°ä½¿å¾—æŸå¤±å‡½æ•°æœ€å°çš„wå–å€¼è²Œä¼¼å¾ˆéš¾ï¼Ÿ(random search,a bad ideaï¼‰ï¼Œä½†æ˜¯iterative refinementç›´è§‚ä¸Šæ˜¯å¯è¡Œçš„ï¼Œæ•…å¯ä»¥ç»™äºˆwä¸€ä¸ªrandom å€¼ï¼Œç„¶åè¿­ä»£çš„æ”¹è¿›ï¼Œä½¿å¾—lossæ¯æ¬¡éƒ½æ¯”ä¸Šæ¬¡è¦å°ã€‚Our strategy will be to start with random weights and iteratively refine them over time to get lower lossç±»ä¼¼äºä¸€ä¸ªæˆ´çœ¼ç½©çš„hikerï¼Œåœ¨å¤šä¸ªè§’åº¦ä¸‹å°è¯•å¾€ä¸‹èµ°ï¼Œç›´åˆ°å±±åº•ã€‚å¦‚åœ¨CIFAR-10ä¸­ï¼Œhills are 30730 dimensional. Strategy #2 random local searchç­–ç•¥ä¸€ç›¸å½“äºéšæ„é€‰å®šæŸæ¡è·¯å¾„å‰è¿›ï¼Œè‹¥æ ¹æ®è¯¥è·¯å¾„ç›´æ¥èµ°åˆ°çš„åœ°æ–¹æ›´ä½ï¼Œåˆ™é€‰æ‹©è¯¥æ–¹å‘æœ€ä¼˜ï¼›è€Œç­–ç•¥äºŒç›¸å½“äºå…ˆéšæ„é€‰å®šæŸæ¡è·¯å¾„ï¼Œç„¶åå¾ªç¯åœ°åœ¨è¿™ä¸ªè·¯å¾„ä¸Šè¿›è¡Œéšæ„çš„æ–¹å‘ä¿®æ”¹ï¼Œè‹¥èµ°åˆ°çš„åœ°æ–¹æ›´ä½ï¼Œåˆ™è¿›è¡Œè·¯å¾„çš„ä¿®æ”¹ï¼Œç›´åˆ°å¾ªç¯ç»“æŸã€‚ Strategy #3 follow gradientä¸ç”¨åˆ»æ„è§„åˆ’è·¯çº¿ï¼Œè€Œæ˜¯è¦æ‰¾åˆ°æ¯ä¸€æ­¥çš„æœ€ä¼˜æ–¹å‘ï¼Œå³lossä¸‹é™çš„æ–¹å‘gradinetï¼Œä¹Ÿå°±æ˜¯å½“æ—¶è„šä¸‹çš„hillsçš„æ–œå¡æ–¹å‘æ‰€è°“gradientå°±æ˜¯a generalization of slope for functions that donâ€™t take a single number but a vector of numbersï¼Œç”±åœ¨æ‰€æ±‚å‡½æ•°å¯¹æ•´ä¸ªè¾“å…¥ç©ºé—´ä¸­çš„å„ä¸ªç»´åº¦çš„derivativeï¼Œå³åå¯¼æ•°ç»„æˆã€‚ä¸€ç»´å¯¼æ•°çš„å®šä¹‰:df(x)dx=limhâ†’0f(x+h)âˆ’f(x)h\\frac{df(x)}{dx} = \\lim_{h\\ \\to 0} \\frac{f(x + h) - f(x)}{h}â€‹dxâ€‹â€‹df(x)â€‹â€‹=â€‹h â†’0â€‹limâ€‹â€‹â€‹hâ€‹â€‹f(x+h)âˆ’f(x)â€‹â€‹When the functions of interest take a vector of numbers instead of a single number,(å³è‡ªå˜é‡ä¸æ­¢ä¸€ä¸ª,å¯¹ä¸åŒçš„å˜é‡è¿›è¡Œæ±‚å¯¼) we call the derivatives partial derivatives, and the gradient is simply the vector of partial derivatives in each dimension.gradientæ˜¯å‡½æ•°å¢é•¿é€Ÿç‡æœ€å¿«çš„æ–¹å‘,The gradient tells us the slope of the loss function along every dimension, which we can use to make an updateã€‚ Computing gradientä¸¤ç§æ–¹å¼:æ•°å€¼è§£å’Œåˆ†æè§£ Numerical gradientå®è´¨æ˜¯æ ¹æ®ä¸€ç»´å¯¼æ•°çš„å…¬å¼è¿›è¡Œè®¡ç®—ï¼Œæ¯æ¬¡åœ¨è¾“å…¥xçš„ä¸€ä¸ªç»´åº¦ä¸Šè¿›è¡Œæ•°å€¼æ±‚è§£ï¼Œå³è®©è¯¥ç»´åº¦ä¸Šæ—§çš„å€¼åŠ ä¸ŠæŒ‡å®šçš„h(å°½å¯èƒ½å°ï¼Œå…¬å¼ä¸­æ˜¯goes toward zero)ï¼Œç®—å¾—f(x+h)åå†ç®—(f(x+h)-f(x))/hï¼Œä»¥æ±‚å¾—åœ¨è¯¥ç»´åº¦ä¸Šçš„åå¯¼æ•°çš„è¿‘ä¼¼å€¼,è®¡ç®—å®ŒæŸä¸ªç»´åº¦åæ³¨æ„è¦è®¾ç½®å›åŸæ¥çš„è¾“å…¥å†è¿›è¡Œä¸‹ä¸ªç»´åº¦çš„è®¡ç®—.it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])åˆ›å»ºäº†ä¸€ä¸ªnumpyæ•°ç»„çš„è¿­ä»£å™¨ï¼Œè¿™é‡Œçš„flagsè¡¨ç¤ºå¯¹æ•°ç»„è¿›è¡Œå¤šé‡ç´¢å¼•ï¼Œop_flagsè¡¨ç¤ºè¿­ä»£å™¨å¯¹æ•°ç»„xå¯æ‰§è¡Œè¯»å†™æ“ä½œã€‚print it.multi_indexå¯ä»¥å¾—åˆ°æ•°ç»„å…ƒç´ æ‰€æœ‰çš„ç´¢å¼•(ä»¥å…ƒç»„å½¢å¼è¿”å›)ï¼Œå¦‚(0,0),(0,1)åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè®¡ç®—æ¢¯åº¦çš„æ—¶å€™å¸¸ç”¨centered difference formulaf(x+h)âˆ’f(xâˆ’h)2h\\frac{f(x+h)-f(x-h)}{2h}â€‹2hâ€‹â€‹f(x+h)âˆ’f(xâˆ’h)â€‹â€‹æ ¹æ®gradientï¼Œåœ¨æ¯ä¸ªç»´åº¦ä¸Šæœç€loss funcitnå¢é•¿é€Ÿç‡æœ€å¿«çš„æ–¹å‘çš„è´Ÿæ–¹å‘è¿›è¡Œstep_sizeçš„æ›´æ–°ã€‚Update in negative gradient direction. In the code above, notice that to compute W_new we are making an update in the negative direction of the gradient df since we wish our loss function to decrease, not increase.step_sizeçš„ä½œç”¨:the gradient tells us the direction in which the function has the steepest rate of increase, but it does not tell us how far along this direction we should stepæ•ˆç‡é—®é¢˜:æ˜¾ç„¶å¯¹äºæ¯ä¸ªç»´åº¦(æ•°ç»„ç´¢å¼•)è¿›è¡Œæ•°å€¼æ±‚è§£ï¼Œåˆ™å¯¹äºgradientçš„æ±‚è§£æ˜¯O(n)ï¼Œnä¸ºwçš„ç»´åº¦ï¼Œæˆ–è€…wijw_{ij}wâ€‹ijâ€‹â€‹çš„ä¸ªæ•°ï¼›å¯¹äºç¥ç»ç½‘ç»œè€Œè¨€ï¼Œå‚æ•°ä¸ªæ•°å¤ªå¤šï¼Œæ•°å€¼è§£æ‰©å±•æ€§å¤ªå·®ã€‚ Analytic gradienté€šè¿‡å¾®ç§¯åˆ†è¿›è¡Œå¯¼æ•°çš„æ¨å¯¼,å¾—åˆ°å¯¼æ•°ç¡®åˆ‡çš„å€¼ï¼Œè€Œä¸æ˜¯è¿‘ä¼¼è§£ã€‚æ±‚å¯¼æ•°å®¹æ˜“å‡ºé”™ï¼Œæ‰€ä»¥ç»å¸¸è®²åˆ†æè§£å’Œæ•°å€¼è§£è¿›è¡Œæ¯”è¾ƒï¼Œç§°ä¸ºgradient checkã€‚ä¸¾ä¾‹SVM loss functionï¼Œå¯¹äºå•ä¸ªç¤ºä¾‹xix_ixâ€‹iâ€‹â€‹:Li=âˆ‘jâ‰ yi[max(0,wjTxiâˆ’wyiTxi+Î”)]L_i = \\sum_{j\\neq y_i} \\left[ \\max(0, w_j^Tx_i - w_{y_i}^Tx_i + \\Delta) \\right]Lâ€‹iâ€‹â€‹=â€‹jâ‰ yâ€‹iâ€‹â€‹â€‹âˆ‘â€‹â€‹[max(0,wâ€‹jâ€‹Tâ€‹â€‹xâ€‹iâ€‹â€‹âˆ’wâ€‹yâ€‹iâ€‹â€‹â€‹Tâ€‹â€‹xâ€‹iâ€‹â€‹+Î”)]å¯¹äºwyiw_{y_i}wâ€‹yâ€‹iâ€‹â€‹â€‹â€‹,âˆ‡wyiLi=âˆ’(âˆ‘jâ‰ yi1(wjTxiâˆ’wyiTxi+Î”&gt;0))xi\\nabla_{w_{y_i}} L_i = - \\left( \\sum_{j\\neq y_i} \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta &gt; 0) \\right) x_iâˆ‡â€‹wâ€‹yâ€‹iâ€‹â€‹â€‹â€‹â€‹â€‹Lâ€‹iâ€‹â€‹=âˆ’â€‹ââ€‹â›â€‹â€‹â€‹jâ‰ yâ€‹iâ€‹â€‹â€‹âˆ‘â€‹â€‹1(wâ€‹jâ€‹Tâ€‹â€‹xâ€‹iâ€‹â€‹âˆ’wâ€‹yâ€‹iâ€‹â€‹â€‹Tâ€‹â€‹xâ€‹iâ€‹â€‹+Î”&gt;0)â€‹â â€‹ââ€‹â€‹xâ€‹iâ€‹â€‹where ğŸ™ is the indicator function that is one if the condition inside is true or zero otherwiseå¯¹äºwjw_jwâ€‹jâ€‹â€‹,âˆ‡wjLi=1(wjTxiâˆ’wyiTxi+Î”&gt;0)xi\\nabla_{w_j} L_i = \\mathbb{1}(w_j^Tx_i - w_{y_i}^Tx_i + \\Delta &gt; 0) x_iâˆ‡â€‹wâ€‹jâ€‹â€‹â€‹â€‹Lâ€‹iâ€‹â€‹=1(wâ€‹jâ€‹Tâ€‹â€‹xâ€‹iâ€‹â€‹âˆ’wâ€‹yâ€‹iâ€‹â€‹â€‹Tâ€‹â€‹xâ€‹iâ€‹â€‹+Î”&gt;0)xâ€‹iâ€‹â€‹Once you derive the expression for the gradient it is straight-forward to implement the expressions and use them to perform the gradient update Gradient descentthe procedure of repeatedly evaluating the gradient and then performing a parameter update is called Gradient Descent123while True: weights_grad = evaluate_gradient(loss_fun, data, weights) weights += - step_size * weights_grad # perform parameter updateMini-batch gradient descent.å¯¹äºè¾“å…¥æ•°æ®é‡åºå¤§çš„training setï¼Œä¸ºäº†æ›´æ–°wçš„æŸä¸ªç´¢å¼•ä¸‹çš„å€¼è€Œå¯¹åºå¤§çš„dataè¿›è¡Œæ“ä½œæœ‰ç‚¹æµªè´¹ï¼Œå¸¸ç”¨çš„æ–¹æ³•æ˜¯compute the gradient over batches of the training data1234while True: data_batch = sample_training_data(data, 256) # sample 256 examples weights_grad = evaluate_gradient(loss_fun, data_batch, weights) weights += - step_size * weights_grad # perform parameter updateä¸ºä½•åœ¨ä¸€å°éƒ¨åˆ†çš„æ•°æ®ä¸­æ›´æ–°wä¹Ÿæ˜¯å¯è¡Œçš„ï¼ŸThe reason this works well is that the examples in the training data are correlatedThe extreme case of this is a setting where the mini-batch contains only a single example. This process is called Stochastic Gradient Descent (SGD) (or also sometimes on-line gradient descent)æ¯”è¾ƒå°‘è§ Summaryä¸¤ç§æ±‚gradientçš„æ–¹æ³•:We discussed the tradeoffs between computing the numerical and analytic gradient. The numerical gradient is simple but it is approximate and expensive to compute. The analytic gradient is exact, fast to compute but more error-prone since it requires the derivation of the gradient with math. Hence, in practice we always use the analytic gradient and then perform a gradient check, in which its implementation is compared to the numerical gradientæ¢¯åº¦ä¸‹é™æ–¹æ³•:We introduced the Gradient Descent algorithm which iteratively computes the gradient and performs a parameter update in loopæ€»çš„æ¥è¯´ä¼˜åŒ–å°±æ˜¯é€šè¿‡å¯»æ‰¾loss functionä¸‹é™é€Ÿåº¦æœ€å¿«çš„æ–¹å‘(gradientçš„åæ–¹å‘)ï¼Œå¯¹wè¿›è¡Œä¸æ–­çš„é€‚å½“å¹…åº¦(learning rate)çš„æ›´æ–°ï¼Œä½¿å¾—lossä¸æ–­å‡å°‘ã€‚","categories":[{"name":"cs231n","slug":"cs231n","permalink":"http://aier02.com/categories/cs231n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"},{"name":"image classification","slug":"image-classification","permalink":"http://aier02.com/tags/image-classification/"}]},{"title":"Linear classification","slug":"linear_classification","date":"2018-10-03T05:31:44.756Z","updated":"2018-10-05T16:06:24.522Z","comments":true,"path":"2018/10/03/linear_classification/","link":"","permalink":"http://aier02.com/2018/10/03/linear_classification/","excerpt":"","text":"Linear classification Multiclass SVMåŸºæœ¬å½¢å¼ä¸ºy=wx+bï¼Œæ­¤æ—¶çš„xä¸ºåˆ—å‘é‡ï¼Œä¸€åˆ—ä¸ºä¸€ä¸ªæ ·æœ¬ï¼Œwçš„æ¯ä¸€è¡Œä¸ºä¸€ä¸ªclassçš„templateã€‚loss function:Multiclass Support Vector Machine (SVM) loss,SVM â€œwantsâ€ the correct class for each image to a have a score higher than the incorrect classes by some fixed margin Î”;Î”ä¸ºè¶…å‚ï¼Œéœ€è¦äººä¸ºè®¾å®šï¼Œå®ƒçš„å­˜åœ¨è¯´æ˜å¤šç±»svmå…³æ³¨çš„å’Œæ™®é€šçš„svmæ€æƒ³ä¸Šæ˜¯ä¸€è‡´çš„ï¼Œéƒ½æ˜¯å…³æ³¨è·ç¦»è¶…å¹³é¢ä¸€å®šèŒƒå›´å†…çš„è¯¯åˆ†ç±»ç‚¹ï¼Œä¹Ÿå°±æ˜¯é—´éš”è¾¹ç•Œå†…çš„ç‚¹ï¼Œæ‰€ä»¥è¿™é‡Œçš„æŸå¤±å‡½æ•°å’Œåˆé¡µæŸå¤±å‡½æ•°çš„è®¾è®¡æ˜¯ä¸€æ ·çš„ï¼›æ•…ç¬¬iå¼ å›¾åƒçš„æŸå¤±å‡½æ•°ä¸ºLi=âˆ‘jâ‰ yimax(0,sjâˆ’syi+Î”)L_i = \\sum_{j\\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)Lâ€‹iâ€‹â€‹=â€‹jâ‰ yâ€‹iâ€‹â€‹â€‹âˆ‘â€‹â€‹max(0,sâ€‹jâ€‹â€‹âˆ’sâ€‹yâ€‹iâ€‹â€‹â€‹â€‹+Î”)æ³¨æ„è¿™é‡Œçš„sjè¡¨ç¤ºçš„æ˜¯è¯¥å›¾åƒåœ¨ç¬¬jç±»çš„å¾—åˆ†ï¼Œè€Œyiè¡¨ç¤ºçš„æ˜¯è¯¥å›¾åƒçš„labelï¼Œå³åªè¦è®¡ç®—å…¶é”™è¯¯åˆ†ç±»çš„æ‰€æœ‰åˆ†æ•°å’Œæ­£ç¡®åˆ†ç±»çš„åˆ†æ•°çš„å·®å€¼ä¹‹å’Œï¼Œå½“labelç±»çš„å¾—åˆ†æ²¡æœ‰å¤§äºæŸä¸ªélabelç±»å¾—åˆ†marginæ—¶ï¼Œä¸¤è€…çš„å·®å€¼ä¼šè¢«ç®—å…¥lossä¸­ã€‚å¼•å…¥regularizationR(W)=âˆ‘kâˆ‘lWk,l2R(W) = \\sum_k\\sum_l W_{k,l}^2R(W)=â€‹kâ€‹âˆ‘â€‹â€‹â€‹lâ€‹âˆ‘â€‹â€‹Wâ€‹k,lâ€‹2â€‹â€‹å®Œæ•´çš„multicalss SVM loss:L = \\underbrace{ \\frac{1}{N} \\sum_i L_i }_\\text{data loss} + \\underbrace{ \\lambda R(W) }_\\text{regularization loss}æ‰©å±•å½¢å¼ä¸º:L=1Nâˆ‘iâˆ‘jâ‰ yi[max(0,f(xi;W)jâˆ’f(xi;W)yi+Î”)]+Î»âˆ‘kâˆ‘lWk,l2L = \\frac{1}{N} \\sum_i \\sum_{j\\neq y_i} \\left[ \\max(0, f(x_i; W)_j - f(x_i; W)_{y_i} + \\Delta) \\right] + \\lambda \\sum_k\\sum_l W_{k,l}^2L=â€‹Nâ€‹â€‹1â€‹â€‹â€‹iâ€‹âˆ‘â€‹â€‹â€‹jâ‰ yâ€‹iâ€‹â€‹â€‹âˆ‘â€‹â€‹[max(0,f(xâ€‹iâ€‹â€‹;W)â€‹jâ€‹â€‹âˆ’f(xâ€‹iâ€‹â€‹;W)â€‹yâ€‹iâ€‹â€‹â€‹â€‹+Î”)]+Î»â€‹kâ€‹âˆ‘â€‹â€‹â€‹lâ€‹âˆ‘â€‹â€‹Wâ€‹k,lâ€‹2â€‹â€‹Î»\\lambdaÎ»ä¸ºè¶…å‚ï¼Œå¸¸ç”¨cross validationç¡®å®šï¼Œè¡¨ç¤ºæ¨¡å‹çš„ä¸€ç§åå¥½ã€‚L2èŒƒæ•°ä½œä¸ºæ­£åˆ™åŒ–é¡¹çš„å¥½å¤„æ˜¯:The most appealing property is that penalizing large weights tends to improve generalization, because it means that no input dimension can have a very large influence on the scores all by itself,å³åœ¨wx+bå¾—åˆ†ä¸€æ ·çš„æƒ…å†µä¸‹ï¼ŒL2èŒƒæ•°çš„æ¨¡å‹åå‘äºé€‰æ‹©smaller and diffuse weightsï¼Œä½¿å¾—æ²¡æœ‰å“ªä¸ªç»´åº¦å½±å“å¾ˆå¤§ã€‚setting delta:ä¸€èˆ¬è®¾ç½®ä¸º1.0æ˜¯å®‰å…¨çš„ï¼Œåœ¨loss functionä¸­deltaå’Œlambdaå…¶å®å…·æœ‰ç›¸åŒeffectåœ¨tradeoffä¸Šï¼Œæ‰€ä»¥çœŸæ­£æœ‰æ„ä¹‰çš„æ˜¯å¯¹äºlambdaçš„æ§åˆ¶ä¸äºŒåˆ†ç±»çš„svmæ¯”è¾ƒ:Li=Cmax(0,1âˆ’yiwTxi)+R(W)L_i = C \\max(0, 1 - y_i w^Tx_i) + R(W)Lâ€‹iâ€‹â€‹=Cmax(0,1âˆ’yâ€‹iâ€‹â€‹wâ€‹Tâ€‹â€‹xâ€‹iâ€‹â€‹)+R(W)è¿™é‡Œçš„yiâˆˆ{âˆ’1,1}y_i \\in \\{ -1,1 \\}yâ€‹iâ€‹â€‹âˆˆ{âˆ’1,1} Softmax classifierf(xi;W)=Wxif(x_i; W) = W x_if(xâ€‹iâ€‹â€‹;W)=Wxâ€‹iâ€‹â€‹å’Œsvmä¿æŒä¸€è‡´ï¼Œä½†ç»è¿‡softmaxå±‚ç”¨äºæŒ‡ç¤ºæ¦‚ç‡çš„å¤§å°ï¼ŒæŸå¤±å‡½æ•°ç”±hinge losså˜æˆcross-entropy lossL_i = -\\log\\left(\\frac{e^{f_{y_i}}}{ \\sum_j e^{f_j} }\\right) \\hspace{0.5in} \\text{or equivalently} \\hspace{0.5in} L_i = -f_{y_i} + \\log\\sum_j e^{f_j}fj(z)=ezjâˆ‘kezkf_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}fâ€‹jâ€‹â€‹(z)=â€‹âˆ‘â€‹kâ€‹â€‹eâ€‹zâ€‹kâ€‹â€‹â€‹â€‹â€‹â€‹eâ€‹zâ€‹jâ€‹â€‹â€‹â€‹â€‹â€‹ç§°ä¸ºsoftmax function,è¿™é‡Œçš„zå³ä¸ºwx+bï¼Œæ•´ä¸ªsoftmax function estimated class probabilitiesï¼Œå³unnormalized log probabilitiesäº¤å‰ç†µ:pä¸ºtrue distibutionï¼Œqä¸ºestimated distributionH(p,q)=âˆ’âˆ‘xp(x)logq(x)H(p,q) = - \\sum_x p(x) \\log q(x)H(p,q)=âˆ’â€‹xâ€‹âˆ‘â€‹â€‹p(x)logq(x)Practical issues: Numeric stability:efyiâˆ‘jefj=CefyiCâˆ‘jefj=efyi+logCâˆ‘jefj+logC\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}} = \\frac{Ce^{f_{y_i}}}{C\\sum_j e^{f_j}} = \\frac{e^{f_{y_i} + \\log C}}{\\sum_j e^{f_j + \\log C}}â€‹âˆ‘â€‹jâ€‹â€‹eâ€‹fâ€‹jâ€‹â€‹â€‹â€‹â€‹â€‹eâ€‹fâ€‹yâ€‹iâ€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹=â€‹Câˆ‘â€‹jâ€‹â€‹eâ€‹fâ€‹jâ€‹â€‹â€‹â€‹â€‹â€‹Ceâ€‹fâ€‹yâ€‹iâ€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹=â€‹âˆ‘â€‹jâ€‹â€‹eâ€‹fâ€‹jâ€‹â€‹+logCâ€‹â€‹â€‹â€‹eâ€‹fâ€‹yâ€‹iâ€‹â€‹â€‹â€‹+logCâ€‹â€‹â€‹â€‹å¸¸ç”¨çš„è®¾ç½®æ–¹æ³•æ˜¯logC=âˆ’maxjfj\\log C = -\\max_j f_jlogC=âˆ’maxâ€‹jâ€‹â€‹fâ€‹jâ€‹â€‹The Softmax classifier gets its name from the softmax function, which is used to squash the raw class scores into normalized positive values that sum to one, so that the cross-entropy loss can be applied. SVM vs. Softmaxä¸»è¦åŒºåˆ«åœ¨æœ‰loss function:ä¸¤è€…çš„åˆ†æ•°å‘é‡féƒ½æ˜¯ä¸€æ ·çš„ï¼Œä¸åŒçš„åœ¨äºå¯¹fçš„è§£é‡Šï¼Œsvmè®¤ä¸ºfæ˜¯å¯¹åº”ç§ç±»çš„å¾—åˆ†ï¼Œhinge lossé¼“åŠ±æ­£ç¡®çš„classå¾—åˆ†æ¯”æ‰€æœ‰é”™è¯¯çš„class scoreéƒ½é«˜å‡ºä¸€ä¸ªmarginï¼›è€Œsoftmaxè®¤ä¸ºfæ˜¯åœ¨æ²¡æœ‰æ ‡å‡†åŒ–ä¹‹å‰è¡¨ç¤ºçš„æ˜¯å±äºæŸä¸ªç§ç±»çš„logæ¦‚ç‡ï¼Œå¹¶ä¸”cross entropyé¼“åŠ±æ­£ç¡®çš„æ­£ç¡®åˆ†ç±»çš„æ¦‚ç‡å¤§(-logÂ§å˜å°)Hence, the probabilities computed by the Softmax classifier are better thought of as confidences where, similar to the SVM Further ReadingDeep Learning using Linear Support Vector Machines from Charlie Tang 2013 presents some results claiming that the L2SVM outperforms Softmax.","categories":[{"name":"cs231n","slug":"cs231n","permalink":"http://aier02.com/categories/cs231n/"}],"tags":[{"name":"notebook","slug":"notebook","permalink":"http://aier02.com/tags/notebook/"},{"name":"linear classification","slug":"linear-classification","permalink":"http://aier02.com/tags/linear-classification/"},{"name":"SVM","slug":"SVM","permalink":"http://aier02.com/tags/SVM/"}]},{"title":"pytorch cookbook U2&U3","slug":"pytorch_cookbook-U2&U3","date":"2018-10-02T08:27:32.398Z","updated":"2018-10-05T15:50:37.362Z","comments":true,"path":"2018/10/02/pytorch_cookbook-U2&U3/","link":"","permalink":"http://aier02.com/2018/10/02/pytorch_cookbook-U2&U3/","excerpt":"","text":"pytorch-cookbook ç¬¬äºŒç« å‡½æ•°ååå¸¦ä¸‹åˆ’çº¿ä¼šä¿®æ”¹å‡½æ•°æœ¬èº«å¦‚y.add_(x)ä¼šç›´æ¥ä¿®æ”¹ypytorchçš„tensorå’Œnumpyçš„å¯¹è±¡å…±äº«å†…å­˜ï¼Œä¸¤è€…åŒæ—¶æ”¹å˜;å¯¹äºtensorä¸æ”¯æŒçš„æ“ä½œï¼Œå¯ä»¥å…ˆè½¬ä¸ºnumpyè¿›è¡Œæ“ä½œåœ¨è½¬ä¸ºtensorï¼ˆtensoræ”¯æŒgpuï¼‰1234a=t.ones(5)b = a.numpy() # Tensor -&gt; Numpya = np.ones(5)b = t.from_numpy(a) # Numpy-&gt;Tensortensor[idx]å¾—åˆ°çš„ä¸º0-dimçš„tensorï¼Œscalar.item()è·å–tensorçš„å•ä¸ªå…ƒç´ å¯¹è±¡t.Tensor(5,3)åˆ›å»º5è¡Œ3åˆ—çš„tensorï¼Œt.tensor([3,4])åˆ›å»ºåŒ…å«3ï¼Œ4ä¸¤ä¸ªå…ƒç´ çš„tensort.tensor()ä¼šè¿›è¡Œæ•°æ®æ‹·è´ï¼Œæ–°çš„tensorå’Œæ—§çš„ä¸å…±äº«å†…å­˜ï¼Œè€Œtorch.from_numpyï¼ˆï¼‰æˆ–è€…tensor.detach()åˆ™ç›¸åä½¿ç”¨gpu123device = t.device(&quot;cuda:0&quot; if t.cuda.is_available() else &quot;cpu&quot;)x = x.to(device)y = y.to(device)autograd: è‡ªåŠ¨å¾®åˆ†;è¦æƒ³ä½¿å¾—Tensorä½¿ç”¨autogradåŠŸèƒ½ï¼Œåªéœ€è¦è®¾ç½®tensor.requries_grad=True.å¦‚ï¼šx = t.ones(2, 2, requires_grad=True)æ³¨æ„ï¼šgradåœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ˜¯ç´¯åŠ çš„(accumulated)ï¼Œè¿™æ„å‘³ç€æ¯ä¸€æ¬¡è¿è¡Œåå‘ä¼ æ’­ï¼Œæ¢¯åº¦éƒ½ä¼šç´¯åŠ ä¹‹å‰çš„æ¢¯åº¦ï¼Œæ‰€ä»¥åå‘ä¼ æ’­ä¹‹å‰éœ€æŠŠæ¢¯åº¦æ¸…é›¶ã€‚# ä»¥ä¸‹åˆ’çº¿ç»“æŸçš„å‡½æ•°æ˜¯inplaceæ“ä½œï¼Œä¼šä¿®æ”¹è‡ªèº«çš„å€¼ï¼Œå°±åƒadd__1x.grad.data.zero_()ä¸€èµ·æ±‚å¯¼çš„è¿‡ç¨‹ç¤ºä¾‹123456x=t.ones(2,2,requires_grad=True)#ç”Ÿæˆtensory=x.sum()#å®šä¹‰è¡¨è¾¾å¼y.grad_fn#æŸ¥çœ‹æ±‚å¯¼å‡½æ•°y.backward()#back propagationx.grad#æŸ¥çœ‹yå¯¹xçš„å¯¼æ•°x.grad.data_zero_()#æ¸…ç©ºå¯¼æ•°ç¼“å­˜ç©ºé—´nerual networkçš„å®šä¹‰ä¸»è¦æ˜¯å¯¹torch.nnæ¨¡å—çš„ä½¿ç”¨,å®šä¹‰ç½‘ç»œæ—¶ï¼Œéœ€è¦ç»§æ‰¿nn.Moduleï¼Œå¹¶å®ç°å®ƒçš„forwardæ–¹æ³•ï¼ŒæŠŠç½‘ç»œä¸­å…·æœ‰å¯å­¦ä¹ å‚æ•°çš„å±‚æ”¾åœ¨æ„é€ å‡½æ•°__init__ä¸­ã€‚å¦‚æœæŸä¸€å±‚(å¦‚ReLU)ä¸å…·æœ‰å¯å­¦ä¹ çš„å‚æ•°ï¼Œåˆ™æ—¢å¯ä»¥æ”¾åœ¨æ„é€ å‡½æ•°ä¸­ï¼Œä¹Ÿå¯ä»¥ä¸æ”¾ï¼Œä½†å»ºè®®ä¸æ”¾åœ¨å…¶ä¸­ï¼Œè€Œåœ¨forwardä¸­ä½¿ç”¨nn.functionalä»£æ›¿,forwarçš„è¾“å…¥å’Œè¾“å‡ºéƒ½æ˜¯tensor,input = t.randn(1, 1, 32, 32),éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œtorch.nnåªæ”¯æŒmini-batchesï¼Œä¸æ”¯æŒä¸€æ¬¡åªè¾“å…¥ä¸€ä¸ªæ ·æœ¬ï¼Œå³ä¸€æ¬¡å¿…é¡»æ˜¯ä¸€ä¸ªbatchã€‚ä½†å¦‚æœåªæƒ³è¾“å…¥ä¸€ä¸ªæ ·æœ¬ï¼Œåˆ™ç”¨ input.unsqueeze(0)å°†batch_sizeè®¾ä¸ºï¼‘,sizeå½¢å¼ä¸ºnSamples x nChannels x height x weight123456789101112131415161718192021222324252627282930import torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): # nn.Moduleå­ç±»çš„å‡½æ•°å¿…é¡»åœ¨æ„é€ å‡½æ•°ä¸­æ‰§è¡Œçˆ¶ç±»çš„æ„é€ å‡½æ•° # ä¸‹å¼ç­‰ä»·äºnn.Module.__init__(self) super(Net, self).__init__() # å·ç§¯å±‚ &apos;1&apos;è¡¨ç¤ºè¾“å…¥å›¾ç‰‡ä¸ºå•é€šé“, &apos;6&apos;è¡¨ç¤ºè¾“å‡ºé€šé“æ•°ï¼Œ&apos;5&apos;è¡¨ç¤ºå·ç§¯æ ¸ä¸º5*5 self.conv1 = nn.Conv2d(1, 6, 5) # å·ç§¯å±‚ self.conv2 = nn.Conv2d(6, 16, 5) # ä»¿å°„å±‚/å…¨è¿æ¥å±‚ï¼Œy = Wx + b self.fc1 = nn.Linear(16*5*5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # å·ç§¯ -&gt; æ¿€æ´» -&gt; æ± åŒ– x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) x = F.max_pool2d(F.relu(self.conv2(x)), 2) # reshapeï¼Œâ€˜-1â€™è¡¨ç¤ºè‡ªé€‚åº” x = x.view(x.size()[0], -1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return xnet = Net()print(net)conv layerä¸»è¦ç‰¹å¾æ˜¯å±€éƒ¨è¿æ¥å’Œæƒé‡å…±äº«å±€éƒ¨è¿æ¥ï¼šæ¯ä¸ªç¥ç»å…ƒä»…ä¸è¾“å…¥ç¥ç»å…ƒçš„ä¸€å—åŒºåŸŸè¿æ¥ï¼Œè¿™å—å±€éƒ¨åŒºåŸŸç§°ä½œæ„Ÿå—é‡ï¼ˆreceptive fieldï¼‰ã€‚åœ¨å›¾åƒå·ç§¯æ“ä½œä¸­ï¼Œå³ç¥ç»å…ƒåœ¨ç©ºé—´ç»´åº¦ï¼ˆspatial dimensionï¼Œå³ä¸Šå›¾ç¤ºä¾‹Hå’ŒWæ‰€åœ¨çš„å¹³é¢ï¼‰æ˜¯å±€éƒ¨è¿æ¥ï¼Œä½†åœ¨æ·±åº¦ä¸Šæ˜¯å…¨éƒ¨è¿æ¥ã€‚å¯¹äºäºŒç»´å›¾åƒæœ¬èº«è€Œè¨€ï¼Œä¹Ÿæ˜¯å±€éƒ¨åƒç´ å…³è”è¾ƒå¼ºã€‚è¿™ç§å±€éƒ¨è¿æ¥ä¿è¯äº†å­¦ä¹ åçš„è¿‡æ»¤å™¨èƒ½å¤Ÿå¯¹äºå±€éƒ¨çš„è¾“å…¥ç‰¹å¾æœ‰æœ€å¼ºçš„å“åº”ã€‚å±€éƒ¨è¿æ¥çš„æ€æƒ³ï¼Œä¹Ÿæ˜¯å—å¯å‘äºç”Ÿç‰©å­¦é‡Œé¢çš„è§†è§‰ç³»ç»Ÿç»“æ„ï¼Œè§†è§‰çš®å±‚çš„ç¥ç»å…ƒå°±æ˜¯å±€éƒ¨æ¥å—ä¿¡æ¯çš„ã€‚*æƒé‡å…±äº«ï¼šè®¡ç®—åŒä¸€ä¸ªæ·±åº¦åˆ‡ç‰‡çš„ç¥ç»å…ƒæ—¶é‡‡ç”¨çš„æ»¤æ³¢å™¨æ˜¯å…±äº«çš„ã€‚ä¾‹ä¸Šå›¾ä¸­è®¡ç®—o[:,:,0]çš„æ¯ä¸ªæ¯ä¸ªç¥ç»å…ƒçš„æ»¤æ³¢å™¨å‡ç›¸åŒï¼Œéƒ½ä¸ºW0ï¼Œè¿™æ ·å¯ä»¥å¾ˆå¤§ç¨‹åº¦ä¸Šå‡å°‘å‚æ•°ã€‚å…±äº«æƒé‡åœ¨ä¸€å®šç¨‹åº¦ä¸Šè®²æ˜¯æœ‰æ„ä¹‰çš„ï¼Œä¾‹å¦‚å›¾ç‰‡çš„åº•å±‚è¾¹ç¼˜ç‰¹å¾ä¸ç‰¹å¾åœ¨å›¾ä¸­çš„å…·ä½“ä½ç½®æ— å…³ã€‚ä½†æ˜¯åœ¨ä¸€äº›åœºæ™¯ä¸­æ˜¯æ— æ„çš„ï¼Œæ¯”å¦‚è¾“å…¥çš„å›¾ç‰‡æ˜¯äººè„¸ï¼Œçœ¼ç›å’Œå¤´å‘ä½äºä¸åŒçš„ä½ç½®ï¼Œå¸Œæœ›åœ¨ä¸åŒçš„ä½ç½®å­¦åˆ°ä¸åŒçš„ç‰¹å¾ ã€‚è¯·æ³¨æ„æƒé‡åªæ˜¯å¯¹äºåŒä¸€æ·±åº¦åˆ‡ç‰‡çš„ç¥ç»å…ƒæ˜¯å…±äº«çš„ï¼Œåœ¨å·ç§¯å±‚ï¼Œé€šå¸¸é‡‡ç”¨å¤šç»„å·ç§¯æ ¸æå–ä¸åŒç‰¹å¾ï¼Œå³å¯¹åº”ä¸åŒæ·±åº¦åˆ‡ç‰‡çš„ç‰¹å¾ï¼Œä¸åŒæ·±åº¦åˆ‡ç‰‡çš„ç¥ç»å…ƒæƒé‡æ˜¯ä¸å…±äº«ã€‚å¦å¤–ï¼Œåé‡å¯¹åŒä¸€æ·±åº¦åˆ‡ç‰‡çš„æ‰€æœ‰ç¥ç»å…ƒéƒ½æ˜¯å…±äº«çš„ã€‚æ± åŒ–æ˜¯éçº¿æ€§ä¸‹é‡‡æ ·çš„ä¸€ç§å½¢å¼ï¼Œä¸»è¦ä½œç”¨æ˜¯é€šè¿‡å‡å°‘ç½‘ç»œçš„å‚æ•°æ¥å‡å°è®¡ç®—é‡ï¼Œå¹¶ä¸”èƒ½å¤Ÿåœ¨ä¸€å®šç¨‹åº¦ä¸Šæ§åˆ¶è¿‡æ‹Ÿåˆã€‚ç½‘ç»œçš„å¯å­¦ä¹ å‚æ•°é€šè¿‡net.parameters()è¿”å›,net.named_parameterså¯åŒæ—¶è¿”å›å¯å­¦ä¹ çš„å‚æ•°åŠåç§°ã€‚nn.MSELoss()å®ç°å‡æ–¹è¯¯å·®ï¼Œnn.CrossEntropyLoss()å®ç°äº¤å‰ç†µæŸå¤±ä¼˜åŒ–å™¨æ›´æ–°å‚æ•°1234567891011121314151617 import torch.optim as optim #æ–°å»ºä¸€ä¸ªä¼˜åŒ–å™¨ï¼ŒæŒ‡å®šè¦è°ƒæ•´çš„å‚æ•°å’Œå­¦ä¹ ç‡optimizer = optim.SGD(net.parameters(), lr = 0.01) # åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ # å…ˆæ¢¯åº¦æ¸…é›¶(ä¸net.zero_grad()æ•ˆæœä¸€æ ·)optimizer.zero_grad() # è®¡ç®—æŸå¤±output = net(input)loss = criterion(output, target) #åå‘ä¼ æ’­loss.backward() #æ›´æ–°å‚æ•°optimizer.step()æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼šä½¿ç”¨torchvisionç¤ºä¾‹ï¼šä¸‹é¢æˆ‘ä»¬æ¥å°è¯•å®ç°å¯¹CIFAR-10æ•°æ®é›†çš„åˆ†ç±»ï¼Œæ­¥éª¤å¦‚ä¸‹:ä½¿ç”¨torchvisionåŠ è½½å¹¶é¢„å¤„ç†CIFAR-10æ•°æ®é›†,å¾—åˆ°datasetå’Œdataloaderå®šä¹‰ç½‘ç»œ,ç»§æ‰¿nn.Module,initä¸­å†™å…¥å¯å­¦ä¹ çš„å‚æ•°å‡½æ•°ï¼Œforwardå®šä¹‰å¥½å‰å‘ä¼ æ’­çš„è¿‡ç¨‹å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼Œcriterionå’Œoptimizerè®­ç»ƒç½‘ç»œå¹¶æ›´æ–°ç½‘ç»œå‚æ•°ï¼Œåœ¨æ¯ä¸ªephcoä¸­åŠ è½½æ•°æ®ï¼Œä¼ å…¥netï¼Œç®—lossï¼Œloss.backwardï¼Œoptimizer.stepæµ‹è¯•ç½‘ç»œå®šä¹‰å¯¹æ•°æ®çš„é¢„å¤„ç†:å°†ä¸¤ç§è½¬åŒ–åˆå¹¶ä¸€èµ·ï¼›ToTensor()å°†shapeä¸º(H, W, C)çš„nump.ndarrayæˆ–imgè½¬ä¸ºshapeä¸º(C, H, W)çš„tensorï¼Œå…¶å°†æ¯ä¸€ä¸ªæ•°å€¼å½’ä¸€åŒ–åˆ°[0,1]ï¼Œå…¶å½’ä¸€åŒ–æ–¹æ³•æ¯”è¾ƒç®€å•ï¼Œç›´æ¥é™¤ä»¥255å³å¯ï¼ŒåŠ å…¥normalizeåˆ™å…¶ä½œç”¨å°±æ˜¯å…ˆå°†è¾“å…¥å½’ä¸€åŒ–åˆ°(0,1)ï¼Œå†ä½¿ç”¨å…¬å¼â€(x-mean)/stdâ€ï¼Œå°†æ¯ä¸ªå…ƒç´ åˆ†å¸ƒåˆ°(-1,1),å‡½æ•°normalizeï¼ˆstd,meanï¼‰1234transform = transforms.Compose([ transforms.ToTensor(), # è½¬ä¸ºTensor transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # å½’ä¸€åŒ– ])Datasetå¯¹è±¡æ˜¯ä¸€ä¸ªæ•°æ®é›†ï¼Œå¯ä»¥æŒ‰ä¸‹æ ‡è®¿é—®ï¼Œè¿”å›å½¢å¦‚(data, label)çš„æ•°æ®ã€‚Dataloaderæ˜¯ä¸€ä¸ªå¯è¿­ä»£çš„å¯¹è±¡ï¼Œå®ƒå°†datasetè¿”å›çš„æ¯ä¸€æ¡æ•°æ®æ‹¼æ¥æˆä¸€ä¸ªbatchï¼Œå¹¶æä¾›å¤šçº¿ç¨‹åŠ é€Ÿä¼˜åŒ–å’Œæ•°æ®æ‰“ä¹±ç­‰æ“ä½œã€‚å½“ç¨‹åºå¯¹datasetçš„æ‰€æœ‰æ•°æ®éå†å®Œä¸€éä¹‹åï¼Œç›¸åº”çš„å¯¹Dataloaderä¹Ÿå®Œæˆäº†ä¸€æ¬¡è¿­ä»£ï¼Œå…ˆå®šä¹‰å¥½datasetï¼Œç„¶åå®šä¹‰dataloaderå¯¹æŒ‡å®šçš„datasetè¿›è¡Œæ“ä½œ123456789101112 # è®­ç»ƒé›†trainset = tv.datasets.CIFAR10( root=&apos;/home/cy/tmp/data/&apos;, train=True, download=True, transform=transform)trainloader = t.utils.data.DataLoader( trainset, batch_size=4, shuffle=True, num_workers=2)è¿›è¡Œnormalizaçš„å¿…è¦æ€§ï¼šæ¯ä¸ªæ ·æœ¬å›¾åƒå‡å»æ•°æ®é›†å›¾åƒçš„å‡å€¼åé™¤ä»¥æ–¹å·®ï¼Œä¿è¯äº†æ‰€æœ‰å›¾åƒçš„åˆ†å¸ƒç›¸ä¼¼ï¼Œä½¿å¾—modelè®­ç»ƒçš„æ—¶å€™æ›´å¿«çš„æ”¶æ•›.è®­ç»ƒç½‘ç»œçš„ç¤ºä¾‹12345678910111213141516171819202122232425262728t.set_num_threads(8)for epoch in range(2): running_loss = 0.0 for i, data in enumerate(trainloader, 0): # è¾“å…¥æ•°æ® inputs, labels = data # æ¢¯åº¦æ¸…é›¶ optimizer.zero_grad() # forward + backward outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() # æ›´æ–°å‚æ•° optimizer.step() # æ‰“å°logä¿¡æ¯ # loss æ˜¯ä¸€ä¸ªscalar,éœ€è¦ä½¿ç”¨loss.item()æ¥è·å–æ•°å€¼ï¼Œä¸èƒ½ä½¿ç”¨loss[0] running_loss += loss.item() if i % 2000 == 1999: # æ¯2000ä¸ªbatchæ‰“å°ä¸€ä¸‹è®­ç»ƒçŠ¶æ€ print(&apos;[%d, %5d] loss: %.3f&apos; \\ % (epoch+1, i+1, running_loss / 2000)) running_loss = 0.0print(&apos;Finished Training&apos;) ç¬¬ä¸‰ç« è¡¨3-1: å¸¸è§æ–°å»ºtensorçš„æ–¹æ³•å‡½æ•°åŠŸèƒ½Tensor(*sizes)åŸºç¡€æ„é€ å‡½æ•°tensor(data,)ç±»ä¼¼np.arrayçš„æ„é€ å‡½æ•°ones(*sizes)å…¨1Tensorzeros(*sizes)å…¨0Tensoreye(*sizes)å¯¹è§’çº¿ä¸º1ï¼Œå…¶ä»–ä¸º0arange(s,e,stepä»såˆ°eï¼Œæ­¥é•¿ä¸ºsteplinspace(s,e,steps)ä»såˆ°eï¼Œå‡åŒ€åˆ‡åˆ†æˆstepsä»½rand/randn(*sizes)å‡åŒ€/æ ‡å‡†åˆ†å¸ƒnormal(mean,std)/uniform(from,to)æ­£æ€åˆ†å¸ƒ/å‡åŒ€åˆ†å¸ƒrandperm(m)éšæœºæ’åˆ—é™¤äº†tensor.size()ï¼Œè¿˜å¯ä»¥åˆ©ç”¨tensor.shapeç›´æ¥æŸ¥çœ‹tensorçš„å½¢çŠ¶ï¼Œtensor.shapeç­‰ä»·äºtensor.size()tensor = t.Tensor(1,2)åˆ›å»ºäº†ä¸€ä¸ªsizeä¸ºã€1ï¼Œ2ã€‘çš„å¼ é‡vector = t.tensor([1, 2])åˆ›å»ºäº†ä¸€ä¸ªå€¼ä¸ºï¼ˆ1ï¼Œ2ï¼‰çš„å‘é‡ï¼Œsizeä¸º2scalar = t.tensor(3.14159) åˆ›å»ºäº†ä¸€ä¸ªå€¼ä¸º3.14159çš„æ ‡é‡ï¼Œsizeä¸ºã€ã€‘,åŒºåˆ«äºsizeã€0ã€‘ï¼Œempty_tensor = t.tensor([])ï¼Œsizeå­˜åœ¨å³ä¸ºtensor,å¦åˆ™ä¸ºscalaré€šè¿‡tensor.viewæ–¹æ³•å¯ä»¥è°ƒæ•´tensorçš„å½¢çŠ¶ï¼Œä½†å¿…é¡»ä¿è¯è°ƒæ•´å‰åå…ƒç´ æ€»æ•°ä¸€è‡´ã€‚viewä¸ä¼šä¿®æ”¹è‡ªèº«çš„æ•°æ®ï¼Œè¿”å›çš„æ–°tensorä¸æºtensorå…±äº«å†…å­˜ï¼Œä¹Ÿå³æ›´æ”¹å…¶ä¸­çš„ä¸€ä¸ªï¼Œå¦å¤–ä¸€ä¸ªä¹Ÿä¼šè·Ÿç€æ”¹å˜ã€‚b = a.view(-1, 3)å½“æŸä¸€ç»´ä¸º-1çš„æ—¶å€™ï¼Œä¼šè‡ªåŠ¨è®¡ç®—å®ƒçš„å¤§å°,torch.squeeze() è¿™ä¸ªå‡½æ•°ä¸»è¦å¯¹æ•°æ®çš„ç»´åº¦è¿›è¡Œå‹ç¼©ï¼Œå»æ‰ç»´æ•°ä¸º1çš„çš„ç»´åº¦ï¼Œæ¯”å¦‚æ˜¯ä¸€è¡Œæˆ–è€…ä¸€åˆ—è¿™ç§ï¼Œä¸€ä¸ªä¸€è¡Œä¸‰åˆ—ï¼ˆ1,3ï¼‰çš„æ•°å»æ‰ç¬¬ä¸€ä¸ªç»´æ•°ä¸ºä¸€çš„ç»´åº¦ä¹‹åå°±å˜æˆï¼ˆ3ï¼‰è¡Œã€‚squeeze(a)å°±æ˜¯å°†aä¸­æ‰€æœ‰ä¸º1çš„ç»´åº¦åˆ æ‰ã€‚ä¸ä¸º1çš„ç»´åº¦æ²¡æœ‰å½±å“ã€‚a.squeeze(N) å°±æ˜¯å»æ‰aä¸­æŒ‡å®šçš„ç»´æ•°ä¸ºä¸€çš„ç»´åº¦ã€‚è¿˜æœ‰ä¸€ç§å½¢å¼å°±æ˜¯b=torch.squeeze(aï¼ŒN) aä¸­å»æ‰æŒ‡å®šçš„å®šçš„ç»´æ•°ä¸ºä¸€çš„ç»´åº¦ã€‚torch.unsqueeze()è¿™ä¸ªå‡½æ•°ä¸»è¦æ˜¯å¯¹æ•°æ®ç»´åº¦è¿›è¡Œæ‰©å……ã€‚ç»™æŒ‡å®šä½ç½®åŠ ä¸Šç»´æ•°ä¸ºä¸€çš„ç»´åº¦ï¼Œæ¯”å¦‚åŸæœ¬æœ‰ä¸ªä¸‰è¡Œçš„æ•°æ®ï¼ˆ3ï¼‰ï¼Œåœ¨0çš„ä½ç½®åŠ äº†ä¸€ç»´å°±å˜æˆä¸€è¡Œä¸‰åˆ—ï¼ˆ1,3ï¼‰ã€‚a.unsqueeze(N) å°±æ˜¯åœ¨aä¸­æŒ‡å®šä½ç½®NåŠ ä¸Šä¸€ä¸ªç»´æ•°ä¸º1çš„ç»´åº¦ã€‚è¿˜æœ‰ä¸€ç§å½¢å¼å°±æ˜¯b=torch.unsqueeze(aï¼ŒN) aå°±æ˜¯åœ¨aä¸­æŒ‡å®šä½ç½®NåŠ ä¸Šä¸€ä¸ªç»´æ•°ä¸º1çš„ç»´åº¦resizeæ˜¯å¦ä¸€ç§å¯ç”¨æ¥è°ƒæ•´sizeçš„æ–¹æ³•ï¼Œä½†ä¸viewä¸åŒï¼Œå®ƒå¯ä»¥ä¿®æ”¹tensorçš„å¤§å°ã€‚å¦‚æœæ–°å¤§å°è¶…è¿‡äº†åŸå¤§å°ï¼Œä¼šè‡ªåŠ¨åˆ†é…æ–°çš„å†…å­˜ç©ºé—´ï¼Œè€Œå¦‚æœæ–°å¤§å°å°äºåŸå¤§å°ï¼Œåˆ™ä¹‹å‰çš„æ•°æ®ä¾æ—§ä¼šè¢«ä¿å­˜ï¼Œå½“å†æ¬¡æ‰©å±•æ—¶å…¶å€¼ä¸ºå½“æ—¶ç¼©å°ä¿å­˜çš„å€¼tensorçš„ç´¢å¼•æ“ä½œå’Œtensorå…±äº«å†…å­˜ï¼Œå³æ›´æ”¹å…¶ä¸­ä¸€ä¸ªï¼Œå¦ä¸€ä¸ªä¹Ÿä¼šæ›´æ”¹ã€‚a[None]:Noneç±»ä¼¼äºnp.newaxis, ä¸ºaæ–°å¢äº†ä¸€ä¸ªè½´ï¼›ç­‰ä»·äºa.view(1, a.shape[0], a.shape[1])a &gt; 1 # è¿”å›ä¸€ä¸ªByteTensor,å³æ»¡è¶³æ¡ä»¶çš„ä½ç½®å€¼ä¸º1ï¼Œå¦åˆ™ä¸º0","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://aier02.com/categories/pytorch/"}],"tags":[{"name":"basic knowledge","slug":"basic-knowledge","permalink":"http://aier02.com/tags/basic-knowledge/"},{"name":"pytorch cookbook","slug":"pytorch-cookbook","permalink":"http://aier02.com/tags/pytorch-cookbook/"}]},{"title":"first time in Kaggle-summary","slug":"rsna_summary","date":"2018-10-01T16:08:39.573Z","updated":"2018-10-04T14:10:53.908Z","comments":true,"path":"2018/10/02/rsna_summary/","link":"","permalink":"http://aier02.com/2018/10/02/rsna_summary/","excerpt":"","text":"é¡¹ç›®å­˜åœ¨çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ RSNA å¦‚ä½•å¼€å§‹æ¯”èµ›å®Œå…¨æ˜¯æ–°æ‰‹ï¼Œå¾ˆæ—©ä»¥å‰å°±æœ‰å­¦é•¿ä»‹ç»è¿‡kaggleï¼Œæœ€è¿‘çœ‹å®Œäº†cs231nï¼Œç„¶åæ‰“ç®—è¯•è¯•æ‰‹ï¼Œä½†æ˜¯ä¸€å¼€å§‹å¹¶ä¸çŸ¥é“åº”è¯¥åšä»€ä¹ˆï¼Œäºæ˜¯å°±ä¸ŠçŸ¥ä¹ç›´æ¥æœäº†å¦‚ä½•æ‰“kaggleæ¯”èµ›ï¼Œæ‰¾åˆ°çš„å¾ˆå¤šçš„éƒ½æ˜¯mlçš„ï¼Œä½†æ˜¯ä¸ªäººæ›´å–œæ¬¢cvï¼Œæ·»åŠ äº†å…³é”®è¯åï¼Œæ‰¾åˆ°äº†ä¸€ç¯‡å’Œæˆ‘ç±»ä¼¼ç»å†çš„blogï¼ŒçŸ¥é“äº†kaggleæ¯”èµ›åˆ°åº•æ˜¯ä»€ä¹ˆï¼Œä»–çš„æ¯ä¸ªæ¿å—è¡¨ç¤ºçš„æ˜¯ä»€ä¹ˆï¼Œå¸¸ç”¨çš„æ­¥éª¤æ˜¯ä»€ä¹ˆï¼Œæœ‰ä»€ä¹ˆéœ€è¦æ³¨æ„çš„ï¼ˆè¿™ä¸ªæ—¶å€™å¼€å§‹æ„è¯†åˆ°æˆ‘æ²¡æœ‰gpuï¼Œè¿™ç‚¹å¿…å®šæ˜¯åæœŸçš„ç“¶é¢ˆï¼‰ é€‰æ‹©å¹³å°æ²¡æœ‰gpuæ€ä¹ˆåŠï¼Œæ²¡æœ‰èµ„æºåªèƒ½å»ç§Ÿç”¨äº‘å¹³å°è·‘modelï¼Œå¼€å¤´æŠ±æœ‰ä¾¥å¹¸å¿ƒç†ï¼Œæœ‰æ²¡æœ‰ä»€ä¹ˆå…è´¹çš„å¹³å°å‘¢ï¼Œä¸€å¼€å§‹æ‰¾åˆ°çš„æ˜¯google çš„colabï¼Œå¬è¯´æ˜¯å…è´¹çš„ï¼Œç„¶åæˆ‘å°±ç›´æ¥å°è¯•ä½¿ç”¨ï¼Œè·‘äº†ä¸€ä¸‹mnistçš„åŸºæœ¬æ¨¡å‹å‘ç°ç»å¸¸è‡ªåŠ¨æ–­å¼€è¿æ¥ï¼Œè€Œä¸”ä»–ç»™çš„å…è´¹çš„ç¡¬ç›˜ç©ºé—´åªæœ‰15gï¼Œå½“æ—¶æˆ‘é¦–å…ˆæ‰“ç®—æ‰“çš„æ¯”èµ›å…¶å®æ˜¯æœ‰å…³air shipçš„detectionæ¯”èµ›ã€‚æœç„¶å¤©åº•ä¸‹æ²¡æœ‰å…è´¹çš„åˆé¤ï¼Œåªèƒ½å¦æ‰¾å¹³å°ï¼Œåæ¥åœ¨çŸ¥ä¹ä¸Šçœ‹åˆ°ä¸å°‘äººæ¨èæå®¢äº‘ï¼Œä»–è‡ªåŠ¨å¸®ä½ æ­å»ºå¥½äº†æ·±åº¦å­¦ä¹ çš„ç¯å¢ƒï¼Œï¼ˆå…¶å®è¿™æ˜¯ä¸ªå‘ï¼Œåˆ›å»ºçš„dlç¯å¢ƒæ˜¯ä¸èƒ½ç›´æ¥æ“ä½œæ•´ä¸ªç³»ç»Ÿçš„ï¼Œä»»ä½•ç³»ç»Ÿçš„æŒ‡ä»¤éƒ½æ— æ³•æ“ä½œï¼Œè¿™ä½¿å¾—æˆ‘åæ¥æ— æ³•è¿›è¡Œç«¯å£çš„æŸ¥è¯¢ï¼Œvisdomå¯åŠ¨çš„æ—¶å€™æ€»æ˜¯æç¤ºç«¯å£è¢«å ç”¨ï¼‰ï¼Œè¿è¡Œçš„é€Ÿåº¦å’Œä»·é’±çš„ç¡®å¾ˆå¸å¼•äºº EDAä»€ä¹ˆæ˜¯lung opacitiesï¼Œä»€ä¹ˆæ˜¯pneumoniaï¼Œä»–ä»¬ä¸¤è€…æœ‰ä»€ä¹ˆå…³ç³»ä¹ˆï¼Ÿç”±äºç¼ºä¹é¢†åŸŸçŸ¥è¯†ï¼Œå¾—ç›ŠäºçŸ¥ä¹ï¼Œæˆ‘ç›´æ¥å¥”å‘äº†discussionå’Œkernelï¼Œæœç„¶æ‰¾åˆ°äº†ä¸€ç¯‡åä¸ºwhat r lung opacitiesçš„kernelï¼Œè¿™ç¯‡æ–‡ç« ç”±ä¸€ä¸ªå…·å¤‡radiologisté¢†åŸŸçŸ¥è¯†çš„kaggleræä¾›ï¼Œä»–ç›´æ¥ä»‹ç»äº†æ€ä¹ˆçœ‹chest x-rayï¼Œé»‘è‰²çš„ä¸ºairï¼Œç™½è‰²çš„ä¸ºboneï¼Œgreyä¸ºtissueæˆ–è€…fluidï¼Œè¿™ä¸ªkernelå¯¹äºæˆ‘æ•´ä¸ªé¡¹ç›®å½±å“æœ€å¤§ï¼Œä»–è¡¥å……äº†æˆ‘xrayçš„é¢†åŸŸçŸ¥è¯†ã€‚åšedaä¸»è¦æ˜¯çœ‹å„ä¸ªæ–‡ä»¶ä¸­çš„æ•°é‡ã€æ ·æœ¬ä¸­æ˜¯å¦å­˜åœ¨é‡å¤ï¼Œæ˜¯å¦æœ‰ç¼ºå¤±ï¼Œclass_label.csvæ–‡ä»¶ä¸­æœ‰å¤šå°‘ä¸­ä¸åŒçš„classï¼Œå„è‡ªçš„æ•°é‡å¦‚ä½•ï¼Œä»–ä¸trainæ•°æ®é›†æ˜¯å¦ä¸€ä¸€å¯¹åº”ï¼Œopenï¼ˆï¼‰å‡½æ•°ï¼Œåˆ›å»ºä¸€ä¸ªreaderï¼Œnextè·³è¿‡headerä¹‹åå°±å¾ªç¯è¯»å–readerå®ç°æŒ‰è¡Œè¯»å–æ•°æ®ï¼›ä¹Ÿå¯ç”¨pd.read_csvå®ç°ï¼Œvalue_counts()ç»Ÿè®¡é‡å¤çš„æ¬¡æ•°ï¼Œgroupbyï¼ˆkeywordï¼‰å¯ä»¥æ ¹æ®keywordåˆ†ç»„ï¼›ä¸»è¦ç”¨åˆ°äº†matplotlib.pyplotï¼Œnumpyï¼Œpandasï¼Œpydicomï¼Œglobè¿›è¡Œæ•°æ®å¯è§†åŒ– detectionï¼Ÿè¿˜æ˜¯segmentationï¼Ÿè¯šç„¶æˆ‘ä¸€å¼€å§‹çœ‹åˆ°å®˜æ–¹çš„æ¯”èµ›ä»‹ç»çš„æ—¶å€™ï¼Œæˆ‘ä¸»è§‚ä¸Šæ˜¯è®¤ä¸ºè¦æ ¹æ®dicomå›¾åƒæä¾›çš„ç—…äººçš„ä¸ªäººä¿¡æ¯ä»¥åŠå›¾åƒä¸­çš„ä¿¡æ¯è¿›è¡Œclassificationï¼Œæ ¹æ®æˆ‘ä¸ªäººå¯¹äºéƒ¨åˆ†pneumoniaçš„xrayçš„è§‚å¯Ÿï¼Œä»¥åŠæä¾›çš„æ•°æ®é›†ä¸­çš„bboxesçš„ä¿¡æ¯å’Œä¹‹å‰æåˆ°çš„é‚£ç¯‡kernelçš„å¼•å¯¼ï¼Œæœ¬æ¬¡æ¯”èµ›æ›´å¤šçš„æ˜¯åšèƒ½æŒ‡ç¤ºpneumoniaçš„lung opacitiesï¼Œè€Œä¸”æ˜æ˜¾æ˜¯éš¾ä»¥å®Œå…¨sgmentå‡ºæ¥çš„ï¼ˆopacitiesæœ‰ä¸€ç§æ˜¯æ¨¡ç³Šäº†å¿ƒè„å’Œlungçš„è¾¹ç•Œã€‚äºæ˜¯å†³å®šåšdetection æ•°æ®å‡†å¤‡å‰æœŸæ‰€åšçš„æ•°æ®å‡†å¤‡ï¼Œæˆ‘çš„æƒ³æ³•æ˜¯ç›´æ¥æŠŠpneumoniaçš„bboxçš„x,y,w,hè£…å…¥å†…å­˜ï¼Œå¾ˆæ˜æ˜¾è¿™ä¸­åšæ³•æœ‰ä¸€å®šçš„é£é™©ï¼Œé‡åˆ°å†…å­˜ä¸è¶³ææ€•ç›´æ¥çˆ†äº†ï¼ŒåæœŸæˆ‘é€‰æ‹©äº†ä½¿ç”¨npyæ–‡ä»¶å…ˆé¢„å…ˆè¯»å–train_lableçš„ä¿¡æ¯ï¼Œç„¶åæ¯æ¬¡ä½¿ç”¨çš„æ—¶å€™å†è¿›è¡Œè¯»å–ï¼Œè¿™æ ·çš„åšæ³•ä¸ªäººè®¤ä¸ºå¯ä»¥é¿å…æ¯æ¬¡è®­ç»ƒçš„æ—¶å€™è¿›è¡Œpydicomæ•°æ®çš„è½¬æ¢ï¼Œä¹Ÿèƒ½å‡å°‘å†…å­˜çš„ä½¿ç”¨ï¼Œåæœç”¨äº†æ›´å¤šçš„ç£ç›˜ç©ºé—´ï¼Œè€Œä¸”æ¯æ¬¡è¯»å–npyæ–‡ä»¶éƒ½æ˜¯å­˜åœ¨æ—¶é—´æˆæœ¬çš„ã€‚è€Œä¸”æœ‰ä¸ªä¸¥é‡çš„é—®é¢˜ï¼Œå°±æ˜¯åæœŸå¦‚ä½•åšæ•°æ®å¢å¼ºï¼Œæ˜¯å¯¹imageè¿›è¡Œæ“ä½œï¼Œpydicom.dcmread()å’Œcv2.imreadè¯»éƒ½æ˜¯hwcï¼Œè€Œä¸”æ˜¯BGRä¸Šçš„0-255ä¸ºäº†ä½¿åæœŸè¿›è¡Œè¯»å–æ–¹ä¾¿ï¼Œæˆ‘æŠŠä½¿ç”¨sitk.ReadImageï¼ˆdcmï¼‰å°†æ‰€æœ‰çš„dcmå›¾åƒè½¬ä¸ºpngæ ¼å¼ï¼Œsitk.GetArrayFromImageè¿”å›çš„æ˜¯ï¼ˆhwcï¼‰ï¼ŒåŒæ—¶å­˜å‚¨æ‚£æœ‰è‚ºç‚çš„å›¾åƒçš„numpyæ•°ç»„å’Œbboxçš„ä½ç½®ä¿¡æ¯ï¼ˆymin,xmin,ymax,xmaxï¼‰ï¼Œä»¥npyæ–‡ä»¶çš„æ ¼å¼ä¿å­˜ï¼Œå®é™…ä¸Šåœ¨è¿™æ¬¡æ“ä½œä¸­å›¾ç‰‡éƒ½æ˜¯å•é€šé“çš„ï¼Œè€Œä¸”éƒ½æ˜¯1024*1024çš„ã€‚ç„¶åé€šè¿‡å¯¹patientidè¿›è¡Œå²è¿›shuffleï¼Œåˆ›å»ºtrainå’Œvaldationï¼Œå¤§æ¦‚9:1 æ•°æ®å¢å¼ºç»è¿‡edaåï¼Œlung opacitiesï¼Œno lung opacities/not normal,normal,æ¯”ä¾‹æ˜¯1:1:1ï¼ŒæŠŠlung opacitiesä½œä¸ºpositiveï¼Œåˆ™datasetä¸­å­˜åœ¨æ ·ä¾‹ä¸å¹³è¡¡ï¼Œè€ƒè™‘è¿›è¡Œå›¾åƒçš„å¢å¼ºï¼Œæ—‹è½¬ï¼Œå¹³ç§»ï¼Œäº®åº¦æ”¹å˜ä¹‹ç±»çš„ï¼Œç”±äºæ˜¯x-rayå›¾åƒï¼Œè€ƒè™‘è¿›è¡Œå›¾åƒçš„ä»¿å°„å˜æ¢ï¼Œä½†æ˜¯æ—‹è½¬ä¼šæ¶‰åŠbboxçš„æ”¹å˜ï¼Œå› ä¸ºä¹‹å‰åœ¨æ•°å­—å›¾åƒè¯¾ç¨‹å®éªŒä¸­å­¦è¿‡ä»¿å°„å˜æ¢ï¼ˆæ—‹è½¬ï¼Œç¼©æ”¾ï¼Œå¹³ç§»ï¼‰ç„¶åå°±ç›´æ¥å®ç°äº†rotate_img_bbox(img, bboxes, degree=-45, scale=1.)ï¼Œmat = cv2.getRotationMatrix2D(center,angle=degree,scale=scale) #affine matrixï¼Œä»¿å°„çŸ©é˜µä¸ºï¼ˆ2ï¼Œ3ï¼‰çš„çŸ©é˜µï¼Œä»¥æ°´å¹³è¿›è¡Œåˆ’åˆ†ï¼Œå‰ï¼ˆ2ï¼Œ2ï¼‰å­çŸ©é˜µä¸ºçº¿æ€§å˜æ¢çŸ©é˜µï¼Œï¼ˆ2ï¼Œ1ï¼‰å­çŸ©é˜µä¸ºå¹³ç§»çŸ©é˜µï¼Œç”¨æ ‡é‡çš„å½¢å¼æ¥çœ‹å°±æ˜¯ax+bï¼›T=Mã€x,y,1ã€‘,å¯¹åŸæ¥çš„bboxçš„å››æ¡è¾¹çš„å››ä¸ªä¸­ç‚¹è¿›è¡Œç›¸åŒçš„çŸ©é˜µå˜åŒ–ï¼Œç„¶ååˆå¹¶ä¸ºä¸€ä¸ªçŸ©é˜µï¼Œè¡¨ç¤ºä¸€ä¸ªä»¿å°„åçš„çŸ©é˜µï¼ŒçŸ©å½¢è¾¹æ¡†ï¼ˆBounding Rectangleï¼‰æ˜¯è¯´ï¼Œç”¨ä¸€ä¸ªæœ€å°çš„çŸ©å½¢ï¼ŒæŠŠæ‰¾åˆ°çš„å½¢çŠ¶åŒ…èµ·æ¥ã€‚è¿˜æœ‰ä¸€ä¸ªå¸¦æ—‹è½¬çš„çŸ©å½¢ï¼Œé¢ç§¯ä¼šæ›´å°ï¼Œå³ä½¿ç”¨rx, ry, rw, rh = cv2.boundingRect(concat)å¾—åˆ°æ‘†æ­£åçš„ç»è¿‡ä»¿å°„å˜æ¢çš„æ–°çš„bboxï¼Œæ•´ä¸ªå›¾åƒçš„å˜æ¢out_image = cv2.warpAffine(img,mat,(width,height))åšå„ç§å˜æ¢çš„æ—¶å€™ç”±äºæ•°æ®é›†è¾ƒå¤§ï¼Œè€Œä¸”æ“ä½œå¤šï¼Œç»å¸¸å‡ºç°ç­‰å¾…æ—¶é—´è¾ƒé•¿çš„æƒ…å†µï¼Œç¼ºä¹å¯è§†åŒ–ï¼Œä¸èƒ½ç¡®å®šæ˜¯å®Œæˆäº†æ“ä½œè¿˜æ˜¯ä»ç„¶åœ¨ç­‰å¾…ï¼Œåæ¥åœ¨å¾ªç¯ä¸­ä½¿ç”¨äº†tqdmè¿›è¡Œè¿›åº¦æ¡çš„æ˜¾ç¤ºï¼Œå¯è§†åŒ–äº†è¿›ç¨‹çš„è¿›åº¦ã€‚ å‡†å¤‡datasetåˆ›å»ºç±»generator(keras.utils.Sequence)ï¼Œå®ç°çš„å‡½æ•°åˆ†åˆ«æœ‰å†…ç½®init()ï¼Œåˆå§‹åŒ–æ–‡ä»¶å¤¹è·¯å¾„ï¼Œæ–‡ä»¶åï¼Œbboxing boxï¼Œbatch_size=32,image_size=256(åŸå›¾ä¸º1024)ï¼Œshuffleï¼Œaugmentï¼Œpredictä¸ºä¸‰è€…ä¸ºçœŸè¿˜æ˜¯å‡å†…ç½®lenï¼ˆï¼‰ï¼Œè¿”å›filenamesä¸­æ•°æ®é‡çš„å¤§å°å†…ç½®getitemï¼ˆï¼‰ï¼Œä»¥batchz_sizeä¸ºæ•°æ®å•ä½ï¼Œæ ¹æ®indexç¡®å®šæ•°æ®çš„ä½ç½®ï¼Œå½“predictä¸ºçœŸæ—¶ï¼Œè¿”å›å¯¹åº”æ–‡ä»¶åçš„å›¾åƒå’Œæ–‡ä»¶åï¼›å¦åˆ™ï¼Œè¿”å›å¯¹åº”æ–‡ä»¶åçš„imgå›¾åƒå’Œmskï¼Œæ³¨æ„mskså³ä¸ºbboxesçš„åˆ—è¡¨å†…ç½®loadï¼ˆï¼‰ï¼Œé€šè¿‡filenameç¡®å®špatientidï¼Œç„¶åè¯»å–npyæ–‡ä»¶è·å–img_arrayå’Œbboxesåˆ—è¡¨ï¼ˆæ¯ä¸€é¡¹ä¸ºå¯¹åº”å›¾ç‰‡çš„bboxçš„ymin,xmin,ymax,xmaxï¼‰ï¼Œåˆ›å»ºå’Œimgç­‰å¤§å°çš„å…¨0mskï¼Œæ ¹æ®npyä¸­çš„â€˜bboxesâ€™é¡¹å°†å¯¹åº”çš„æ ‡æ³¨æ¡†åŒºåŸŸè®¾ç½®ä¸º1ï¼›ç„¶åresizeå›¾ç‰‡ï¼Œç‰¹åˆ«æ³¨æ„ç”±äºè®­ç»ƒçš„æ—¶å€™è¿›è¡Œçš„æ˜¯batch_train,æ‰€ä»¥å¯¹äºimgå’Œmskéƒ½å¿…é¡»æ·»åŠ ä¸€ä¸ªdimensionï¼Œä½œä¸ºè®­ç»ƒçš„ç»´åº¦å†…ç½®loadpredictï¼ˆï¼‰ï¼ŒåŸºæœ¬æ“ä½œå’Œloadå‡½æ•°ä¸€è‡´ï¼Œä¸åŒåœ¨ä¸è¯¥å‡½æ•°ä¸ç”¨è·å–bboxesä¿¡æ¯ æ­å»ºç¥ç»ç½‘ç»œcreate_downsample(channels, inputs)ä¸‹é‡‡æ ·å‡½æ•°ï¼ŒchannelsæŒ‡ç¤ºfilterçš„å¤§å°ï¼ŒinputsæŒ‡ç¤ºimagesï¼Œä¾æ¬¡ä½¿ç”¨çš„æ˜¯keras.layers.BN(momentum=0.9)-&gt;leakyrelu-&gt;conv2d(padding=same)-&gt;maxpool2dã€‚paddingï¼šè¡¥0ç­–ç•¥ï¼Œä¸ºâ€œvalidâ€, â€œsameâ€ ã€‚â€œvalidâ€ä»£è¡¨åªè¿›è¡Œæœ‰æ•ˆçš„å·ç§¯ï¼Œå³å¯¹è¾¹ç•Œæ•°æ®ä¸å¤„ç†ã€‚â€œsameâ€ä»£è¡¨ä¿ç•™è¾¹ç•Œå¤„çš„å·ç§¯ç»“æœï¼Œé€šå¸¸ä¼šå¯¼è‡´è¾“å‡ºshapeä¸è¾“å…¥shapeç›¸åŒcreate_resblock(channels, inputs)resblockå‡½æ•°ï¼Œä¸€ä¸ªresblockåŒ…å«äº†BN-&gt;LEAKYRELU-&gt;CONV2D-&gt;BN-&gt;LEAKYRELU-&gt;CON2D-&gt;add([x, inputs])(channelsç”¨äºconvå±‚ä¸­filtersçš„æ•°ç›®ï¼Œå³è¾“å‡ºçš„ç»´åº¦ï¼Œkernel_sizeæŒ‡å®šfilterçš„å¤§å°)create_network(input_size, channels, n_blocks=2, depth=4)æ­å»ºæ•´ä¸ªç½‘ç»œï¼Œinputs = keras.Input(shape=(input_size, input_size, 1))å¯¹è¾“å…¥è¿›è¡Œè§„èŒƒåŒ–-&gt;conv2d-&gt;åˆ›å»ºdepthå±‚ç»“æ„ï¼Œæ¯ä¸ªç»“æ„ä¸­åŒ…æ‹¬äº†ä¸€ä¸ªdownsampleå’Œn_blockä¸ªresblock-&gt;ouput layerä¾æ¬¡ä¸ºBN-&gt;LEAKYRELU-&gt;CONV2D-&gt;UpSampling2D(2**depth)(x)-&gt;å°†inputå’ŒoutputåŒ…è£…ä¸ºä¸€ä¸ªmodelå³model = keras.Model(inputs=inputs, outputs=outputs) ResnetèƒŒæ™¯:éšç€ç½‘ç»œçš„åŠ æ·±ï¼Œå‡ºç°äº†è®­ç»ƒé›†å‡†ç¡®ç‡ä¸‹é™çš„ç°è±¡ï¼Œæ’é™¤overfittingï¼Œé’ˆå¯¹è¯¥é—®é¢˜æå‡ºäº†resnetï¼Œä»¥å…è®¸å®ç°å°½å¯èƒ½åœ°åŠ æ·±ç½‘ç»œresnetä¸­æå‡ºäº†ä¸¤ç§mappingï¼Œidentity mapping å’Œ residual mappingï¼Œè¾“å‡ºä¸ºy=F(x)+x,æ˜¾ç„¶xä¸ºå‰è€…ï¼ŒF(x)ä¸ºåè€…ï¼›ç†è®ºä¸Šï¼Œå¯¹äºâ€œéšç€ç½‘ç»œåŠ æ·±ï¼Œå‡†ç¡®ç‡ä¸‹é™â€çš„é—®é¢˜ï¼ŒResnetæä¾›äº†ä¸¤ç§é€‰æ‹©æ–¹å¼ï¼Œä¹Ÿå°±æ˜¯identity mappingå’Œresidual mappingï¼Œå¦‚æœç½‘ç»œå·²ç»åˆ°è¾¾æœ€ä¼˜ï¼Œç»§ç»­åŠ æ·±ç½‘ç»œï¼Œresidual mappingå°†è¢«pushä¸º0ï¼Œåªå‰©ä¸‹identity mappingï¼Œè¿™æ ·ç†è®ºä¸Šç½‘ç»œä¸€ç›´å¤„äºæœ€ä¼˜çŠ¶æ€äº†ï¼Œç½‘ç»œçš„æ€§èƒ½ä¹Ÿå°±ä¸ä¼šéšç€æ·±åº¦å¢åŠ è€Œé™ä½äº†ã€‚å³çœŸæ­£å­¦ä¹ çš„æ˜¯æ®‹å·®ï¼Œè€Œå½¢å¼ä¸Šyä¿è¯äº†ä¸ä¼šå‡ºç°ç½‘ç»œåŠ æ·±è€Œç»éªŒè¯¯å·®å¢å¤§çš„ç°è±¡åœ¨resnetä¸­åŠ å…¥1*1çš„conv layerå°±æ˜¯bottleneck layerï¼Œç›®çš„æ˜¯ä¸ºäº†é™ç»´ï¼Œé™ä½è®¡ç®—é‡å’Œå‚æ•°çš„æ•°ç›®ï¼Œæœ€ååˆå‡ç»´æ˜¯ä¸ºäº†ä¿æŒå’Œè¾“å…¥xçš„dimensionsä¸€è‡´ Batch NormalizationBatchNormå°±æ˜¯åœ¨æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ä¸­ä½¿å¾—æ¯ä¸€å±‚ç¥ç»ç½‘ç»œçš„è¾“å…¥ä¿æŒç›¸åŒåˆ†å¸ƒçš„ã€‚(IIDç‹¬ç«‹åŒåˆ†å¸ƒå‡è®¾ï¼Œå°±æ˜¯å‡è®¾è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®æ˜¯æ»¡è¶³ç›¸åŒåˆ†å¸ƒçš„ï¼Œè¿™æ˜¯é€šè¿‡è®­ç»ƒæ•°æ®è·å¾—çš„æ¨¡å‹èƒ½å¤Ÿåœ¨æµ‹è¯•é›†è·å¾—å¥½çš„æ•ˆæœçš„ä¸€ä¸ªåŸºæœ¬ä¿éšœåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œéšå±‚çš„è¾“å…¥åˆ†å¸ƒè€æ˜¯å˜æ¥å˜å»ï¼Œè¿™å°±æ˜¯æ‰€è°“çš„â€œInternal Covariate Shiftâ€ï¼ŒInternalæŒ‡çš„æ˜¯æ·±å±‚ç½‘ç»œçš„éšå±‚ï¼Œæ˜¯å‘ç”Ÿåœ¨ç½‘ç»œå†…éƒ¨çš„äº‹æƒ…ï¼Œè€Œä¸æ˜¯covariate shifté—®é¢˜åªå‘ç”Ÿåœ¨è¾“å…¥å±‚ã€‚ç„¶åæå‡ºäº†BatchNormçš„åŸºæœ¬æ€æƒ³ï¼šèƒ½ä¸èƒ½è®©æ¯ä¸ªéšå±‚èŠ‚ç‚¹çš„æ¿€æ´»è¾“å…¥åˆ†å¸ƒå›ºå®šä¸‹æ¥å‘¢ï¼Ÿè¿™æ ·å°±é¿å…äº†â€œInternal Covariate Shiftâ€é—®é¢˜äº†ã€‚å¯å‘å¼æ€è€ƒï¼šå¯¹è¾“å…¥å›¾åƒè¿›è¡Œç™½åŒ–ï¼ˆWhitenï¼‰æ“ä½œçš„è¯â€”â€”æ‰€è°“ç™½åŒ–ï¼Œå°±æ˜¯å¯¹è¾“å…¥æ•°æ®åˆ†å¸ƒå˜æ¢åˆ°0å‡å€¼ï¼Œå•ä½æ–¹å·®çš„æ­£æ€åˆ†å¸ƒâ€”â€”é‚£ä¹ˆç¥ç»ç½‘ç»œä¼šè¾ƒå¿«æ”¶æ•›ï¼›BNæ‰€åšçš„å¯ä»¥ç†è§£ä¸ºå¯¹æ·±å±‚ç¥ç»ç½‘ç»œæ¯ä¸ªéšå±‚ç¥ç»å…ƒçš„æ¿€æ´»å€¼åšç®€åŒ–ç‰ˆæœ¬çš„ç™½åŒ–æ“ä½œç®€è€Œè¨€ä¹‹ï¼Œå¯¹äºæ¯ä¸ªéšå±‚ç¥ç»å…ƒï¼ŒæŠŠé€æ¸å‘éçº¿æ€§å‡½æ•°æ˜ å°„åå‘å–å€¼åŒºé—´æé™é¥±å’ŒåŒºé æ‹¢çš„è¾“å…¥åˆ†å¸ƒå¼ºåˆ¶æ‹‰å›åˆ°å‡å€¼ä¸º0æ–¹å·®ä¸º1çš„æ¯”è¾ƒæ ‡å‡†çš„æ­£æ€åˆ†å¸ƒï¼Œä½¿å¾—éçº¿æ€§å˜æ¢å‡½æ•°çš„è¾“å…¥å€¼è½å…¥å¯¹è¾“å…¥æ¯”è¾ƒæ•æ„Ÿçš„åŒºåŸŸï¼Œä»¥æ­¤é¿å…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚å› ä¸ºæ¢¯åº¦ä¸€ç›´éƒ½èƒ½ä¿æŒæ¯”è¾ƒå¤§çš„çŠ¶æ€ï¼Œæ‰€ä»¥å¾ˆæ˜æ˜¾å¯¹ç¥ç»ç½‘ç»œçš„å‚æ•°è°ƒæ•´æ•ˆç‡æ¯”è¾ƒé«˜ï¼Œå°±æ˜¯å˜åŠ¨å¤§ï¼Œå°±æ˜¯è¯´å‘æŸå¤±å‡½æ•°æœ€ä¼˜å€¼è¿ˆåŠ¨çš„æ­¥å­å¤§ï¼Œä¹Ÿå°±æ˜¯è¯´æ”¶æ•›åœ°å¿«ã€‚å°†è¾“å…¥xçš„åˆ†å¸ƒå¼ºåˆ¶è½¬æ¢åˆ°å‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1çš„æ­£æ€åˆ†å¸ƒã€‚ç»è¿‡BNåï¼Œç›®å‰å¤§éƒ¨åˆ†Activationçš„å€¼è½å…¥éçº¿æ€§å‡½æ•°çš„çº¿æ€§åŒºå†…ï¼Œå…¶å¯¹åº”çš„å¯¼æ•°è¿œç¦»å¯¼æ•°é¥±å’ŒåŒºï¼Œè¿™æ ·æ¥åŠ é€Ÿè®­ç»ƒæ”¶æ•›è¿‡ç¨‹ã€‚BNä¸ºäº†ä¿è¯éçº¿æ€§çš„è·å¾—ï¼Œå¯¹å˜æ¢åçš„æ»¡è¶³å‡å€¼ä¸º0æ–¹å·®ä¸º1çš„xåˆè¿›è¡Œäº†scaleåŠ ä¸Šshiftæ“ä½œ(y=scale*x+shift)ï¼Œæ¯ä¸ªç¥ç»å…ƒå¢åŠ äº†ä¸¤ä¸ªå‚æ•°scaleå’Œshiftå‚æ•°ï¼Œè¿™ä¸¤ä¸ªå‚æ•°æ˜¯é€šè¿‡è®­ç»ƒå­¦ä¹ åˆ°çš„ï¼Œæ„æ€æ˜¯é€šè¿‡scaleå’ŒshiftæŠŠè¿™ä¸ªå€¼ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒå·¦ç§»æˆ–è€…å³ç§»ä¸€ç‚¹å¹¶é•¿èƒ–ä¸€ç‚¹æˆ–è€…å˜ç˜¦ä¸€ç‚¹â‘ ä¸ä»…ä»…æå¤§æå‡äº†è®­ç»ƒé€Ÿåº¦ï¼Œæ”¶æ•›è¿‡ç¨‹å¤§å¤§åŠ å¿«ï¼›â‘¡è¿˜èƒ½å¢åŠ åˆ†ç±»æ•ˆæœï¼Œä¸€ç§è§£é‡Šæ˜¯è¿™æ˜¯ç±»ä¼¼äºDropoutçš„ä¸€ç§é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ­£åˆ™åŒ–è¡¨è¾¾æ–¹å¼ï¼Œæ‰€ä»¥ä¸ç”¨Dropoutä¹Ÿèƒ½è¾¾åˆ°ç›¸å½“çš„æ•ˆæœï¼›â‘¢å¦å¤–è°ƒå‚è¿‡ç¨‹ä¹Ÿç®€å•å¤šäº†ï¼Œå¯¹äºåˆå§‹åŒ–è¦æ±‚æ²¡é‚£ä¹ˆé«˜ï¼Œè€Œä¸”å¯ä»¥ä½¿ç”¨å¤§çš„å­¦ä¹ ç‡ç­‰ å®šä¹‰loss functionå®šä¹‰iouæŸå¤±å‡½æ•°:iou_loss(y_true, y_pred),è¿™é‡Œçš„label(å³y)æ˜¯maskï¼Œå³bboxesä¸º1çš„0-1å›¾åƒï¼Œintersection = tf.reduce_sum(y_true * y_pred)æ±‚çš„å…±åŒåŒºåŸŸçš„1çš„ä¸ªæ•°ï¼Œscore = (intersection + 1.) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)ï¼Œæ³¨æ„+1.æ˜¯ä¸ºäº†ä¿è¯scoreä¸ä¸º0ï¼Œå…¬å¼ä¸ºinserction/unionã€‚reduce_sumè¿”å›è¯¥çŸ©é˜µæ‰€æœ‰å…ƒç´ ä¹‹å’Œåˆå¹¶æŸå¤±å‡½æ•°:0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * iou_loss(y_true, y_pred)ï¼Œbinary_crossentropyä¸ºå¯¹æ•°æŸå¤±å‡½æ•°æŸå¤±å‡½æ•°æ˜¯è®­ç»ƒçš„å…³é”®ï¼Œä»–è¡¨è¾¾äº†æ¨¡å‹çš„ç›®æ ‡ï¼Œé€šè¿‡æ±‚åŸå›¾åƒç»è¿‡resnetåçš„feature mapå’Œå›¾åƒçš„labelï¼Œå³bboxesçš„iouï¼Œä½¿ç”¨adamè¿›è¡Œä¼˜åŒ–ï¼Œä½¿å¾—ç½‘ç»œå‚æ•°æœiouå‡å°‘çš„æ–¹å‘è¿›è¡Œæ›´æ–°ã€‚ è®­ç»ƒå‚æ•°çš„è®¾ç½®å®šä¹‰tf.metricç®—å­ï¼Œå³è¯„ä¼°æŒ‡æ ‡ç®—å­ï¼Œç”¨äºè®¡ç®—accuracymodel.compile(optimizer=â€˜adamâ€™,loss=iou_bce_loss,metrics=[â€˜accuracyâ€™, mean_iou])æŒ‡å®šä¼˜åŒ–å™¨ï¼ŒæŸå¤±å‡½æ•°å’Œaccuracyåˆå§‹åŒ–lr=0.01ï¼Œepoch=25ï¼Œlrçš„æ›´æ–°ç­–ç•¥:lrx(np.cos(np.pi*x/epochs)+1.)/2,xä¸ºåŠ¨æ€å˜åŒ–çš„epochkeras.callbacks.LearningRateScheduler(schedule)è¯¥å›è°ƒå‡½æ•°æ˜¯ç”¨äºåŠ¨æ€è®¾ç½®å­¦ä¹ ç‡ï¼Œå…¶ä¸­scheduleå‡½æ•°ä»¥epochå·ä¸ºå‚æ•°ï¼ˆä»0ç®—èµ·çš„æ•´æ•°ï¼‰ï¼Œè¿”å›ä¸€ä¸ªæ–°å­¦ä¹ ç‡ï¼ˆæµ®ç‚¹æ•°ï¼‰ åˆ›å»ºtrainå’Œvalidation generatoræŒ‡å®šå›¾ç‰‡æ–‡ä»¶å¤¹:folder = â€˜./input/stage_1_train_pngâ€™è®­ç»ƒç”Ÿæˆå™¨:train_gen = generator(folder, train_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=True, augment=True, predict=False)æµ‹è¯•ç”Ÿæˆå™¨:valid_gen = generator(folder, valid_filenames, pneumonia_locations, batch_size=32, image_size=256, shuffle=False, predict=False) è®­ç»ƒæ¨¡å‹history = model.fit_generator(train_gen, validation_data=valid_gen, callbacks=[learning_rate], epochs=25, workers=4, use_multiprocessing=True)ç”¨äºæŒ‡å®šcallbackå†…å®¹ï¼Œå³learning rateï¼Œè®­ç»ƒé›†ï¼ŒéªŒè¯é›†ï¼Œepochæ•°é‡ï¼Œçº¿ç¨‹æ•°é‡ æµ‹è¯•æ¨¡å‹åˆ›å»ºæµ‹è¯•æ•°æ®ç”Ÿæˆå™¨:test_gen = generator(folder, test_filenames, None, batch_size=25, image_size=256, shuffle=False, predict=True)threshold predicted mask,predä¸­å¤§äº0.5æ‰ä¸ºmaskcomp = pred[:, :, 0] &gt; 0.5apply connected componentscomp = measure.label(comp),measure.labelä½œç”¨æ˜¯ç»™compæ ‡è®°è¿é€šåŒºåŸŸï¼Œç”¨äºç¡®å®šä¸åŒçš„bboxï¼Œmeasure.regionprops(comp)è·å–compçš„ä¸åŒè¿é€šåŒºåŸŸï¼Œè¿”å›çš„regionåˆ—è¡¨ï¼Œæ¯ä¸€ä¸ªåˆ†åˆ«ä¸ºymin,xmin,ymax,xmaxè®¡ç®—ç½®ä¿¡åº¦proxy for confidence score:conf = np.mean(pred[y:y+height, x:x+width]) æäº¤ç»“æœä¿å­˜ç»“æœåˆ°csvæ–‡ä»¶ä¸­:ä»¥å­—å…¸çš„å½¢å¼ä¿å­˜åˆ°csvæ–‡ä»¶ä¸­save dictionary as csv filesub = pd.DataFrame.from_dict(submission_dict,orient=â€˜indexâ€™)æŒ‡å®šç´¢å¼•åç§°:sub.index.names = [â€˜patientIdâ€™]æŒ‡å®šåˆ—å:sub.columns = [â€˜PredictionStringâ€™]å†™å…¥æŒ‡å®šç›®å½•æ–‡ä»¶:sub.to_csv(â€™/input/submission.csvâ€™)","categories":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://aier02.com/categories/Kaggle/"}],"tags":[{"name":"segmentation","slug":"segmentation","permalink":"http://aier02.com/tags/segmentation/"},{"name":"keras","slug":"keras","permalink":"http://aier02.com/tags/keras/"},{"name":"ResNet","slug":"ResNet","permalink":"http://aier02.com/tags/ResNet/"},{"name":"jikecloud","slug":"jikecloud","permalink":"http://aier02.com/tags/jikecloud/"}]},{"title":"python rubbish collection","slug":"python_rubbish_collection","date":"2018-09-28T07:32:02.260Z","updated":"2018-10-03T08:08:16.164Z","comments":true,"path":"2018/09/28/python_rubbish_collection/","link":"","permalink":"http://aier02.com/2018/09/28/python_rubbish_collection/","excerpt":"","text":"pythonåƒåœ¾å›æ”¶æœºåˆ¶ å¼•ç”¨è®¡æ•°æ¯ä¸ªå¯¹è±¡ç»´æŠ¤ä¸€ä¸ªob_refå­—æ®µï¼Œæ¯æ¬¡è¢«åˆ«çš„å¯¹è±¡å¼•ç”¨çš„ob_refåŠ 1ï¼Œè‹¥å¼•ç”¨å¤±æ•ˆï¼Œåˆ™å‡1ï¼Œå½“ob_refä¸º0åˆ™è¯¥å¯¹è±¡è¢«å›æ”¶ï¼Œå ç”¨çš„å†…å­˜ç©ºé—´è¢«é‡Šæ”¾ã€‚ä½†æ˜¯è¯¥æ–¹æ³•ä¸èƒ½è§£å†³å¾ªç¯å¼•ç”¨é—®é¢˜ï¼Œå³ä¸¤ä¸ªå¯¹è±¡ç›¸äº’å¼•ç”¨ï¼Œå½“ä»–ä»¬çš„å¤–éƒ¨å¼•ç”¨éƒ½å¤±æ•ˆæ—¶ï¼Œob_refä»ä¸º1ï¼Œéé›¶ï¼Œä½†æ˜¯ä»–ä»¬å®è´¨æ˜¯è¦è¢«å›æ”¶çš„ï¼Œè€Œpythonå´ä¸èƒ½å°†å…¶å›æ”¶ æ ‡è®°æ¸…é™¤å¯¹æ´»åŠ¨å¯¹è±¡è¿›è¡Œæ ‡è®°ï¼Œå°†éæ´»åŠ¨å¯¹è±¡è¿›è¡Œå›æ”¶ã€‚å¯¹è±¡ä¹‹é—´é€šè¿‡å¼•ç”¨ï¼ˆæŒ‡é’ˆï¼‰è¿åœ¨ä¸€èµ·ï¼Œæ„æˆä¸€ä¸ªæœ‰å‘å›¾ï¼Œå¯¹è±¡æ„æˆè¿™ä¸ªæœ‰å‘å›¾çš„èŠ‚ç‚¹ï¼Œè€Œå¼•ç”¨å…³ç³»æ„æˆè¿™ä¸ªæœ‰å‘å›¾çš„è¾¹ã€‚ä»æ ¹å¯¹è±¡ï¼ˆroot objectï¼‰å‡ºå‘ï¼Œæ²¿ç€æœ‰å‘è¾¹éå†å¯¹è±¡ï¼Œå¯è¾¾çš„ï¼ˆreachableï¼‰å¯¹è±¡æ ‡è®°ä¸ºæ´»åŠ¨å¯¹è±¡ï¼Œä¸å¯è¾¾çš„å¯¹è±¡å°±æ˜¯è¦è¢«æ¸…é™¤çš„éæ´»åŠ¨å¯¹è±¡ã€‚æ ¹å¯¹è±¡å°±æ˜¯å…¨å±€å˜é‡ã€è°ƒç”¨æ ˆã€å¯„å­˜å™¨ã€‚ åˆ†ä»£å›æ”¶æ ¹æ®å¯¹è±¡å­˜æ´»æ—¶é—´çš„ä¸åŒå¯¹å†…å­˜è¿›è¡Œäº†åˆ’åˆ†ï¼Œæ—¶é—´ç”±çŸ­åˆ°é•¿åˆ†åˆ«åˆ’åˆ†ä¸ºå¹´è½»ä»£ï¼Œä¸­å¹´ä»£ï¼Œè€å¹´ä»£ï¼Œæ–°åˆ›å»ºçš„å¯¹è±¡éƒ½ä¼šåˆ†é…åœ¨å¹´è½»ä»£ï¼Œå¹´è½»ä»£é“¾è¡¨çš„æ€»æ•°è¾¾åˆ°ä¸Šé™æ—¶ï¼ŒPythonåƒåœ¾æ”¶é›†æœºåˆ¶å°±ä¼šè¢«è§¦å‘ï¼ŒæŠŠé‚£äº›å¯ä»¥è¢«å›æ”¶çš„å¯¹è±¡å›æ”¶æ‰ï¼Œè€Œé‚£äº›ä¸ä¼šå›æ”¶çš„å¯¹è±¡å°±ä¼šè¢«ç§»åˆ°ä¸­å¹´ä»£å»ï¼Œä¾æ­¤ç±»æ¨ï¼Œè€å¹´ä»£ä¸­çš„å¯¹è±¡æ˜¯å­˜æ´»æ—¶é—´æœ€ä¹…çš„å¯¹è±¡ï¼Œç”šè‡³æ˜¯å­˜æ´»äºæ•´ä¸ªç³»ç»Ÿçš„ç”Ÿå‘½å‘¨æœŸå†…ã€‚","categories":[{"name":"python","slug":"python","permalink":"http://aier02.com/categories/python/"}],"tags":[{"name":"basic knowledge","slug":"basic-knowledge","permalink":"http://aier02.com/tags/basic-knowledge/"}]},{"title":"faster RCNN","slug":"faster_RCNN","date":"2018-09-18T10:55:07.479Z","updated":"2018-10-03T08:08:10.513Z","comments":true,"path":"2018/09/18/faster_RCNN/","link":"","permalink":"http://aier02.com/2018/09/18/faster_RCNN/","excerpt":"","text":"faster RCNNæ„Ÿå—é‡ï¼šåœ¨å·ç§¯ç¥ç»ç½‘ç»œCNNä¸­ï¼Œå†³å®šæŸä¸€å±‚è¾“å‡ºç»“æœä¸­ä¸€ä¸ªå…ƒç´ æ‰€å¯¹åº”çš„è¾“å…¥å±‚çš„åŒºåŸŸå¤§å°ï¼Œè¢«ç§°ä½œæ„Ÿå—é‡receptive fieldfatal error: numpy/arrayobject.h: No such file or directoryâ€â€‹è¿™ä¸ªé”™è¯¯çš„å‡ºç°å¯ä»¥å¦‚ä¸‹è§£å†³ï¼Œå°†setup.pyå†…å®¹åŠ å…¥ä¸€æ¡include_dirs=[numpy.get_include()]â€‹å°±å¯ä»¥äº†ã€‚ç¤ºä¾‹setup.pyæ–‡ä»¶å¦‚ä¸‹ï¼šfrom distutils.core import setupfrom distutils.extension import Extensionfrom Cython.Distutils import build_extimport numpy as npext_modules=[Extension(â€œtest03â€,[â€œtest03.pyxâ€])]setup(name=â€˜gravity_cyâ€™,cmdclass={â€˜build_extâ€™:build_ext},include_dirs = [np.get_include()],ext_modules=ext_modules) ä¸»è¦é—®é¢˜æ˜¯è¦æŒ‡å®šnumpyçš„è·¯å¾„è¿‡ç¨‹è®°å½•ï¼š1ï¼‰ä¸‹è½½faster-rcnnçš„é¢„è®­ç»ƒæ¨¡å‹å¹¶åœ¨demoä¸­è¿›è¡Œè°ƒè¯•2ï¼‰è¿è¡Œpython misc/convert_caffe_pretrain.pyæŠŠé¢„è®­ç»ƒçš„vgg16ä¸‹è½½å¹¶ä¿å­˜åˆ°checkpointä¸­ï¼Œä½œä¸ºcnnæå–å™¨3ï¼‰","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://aier02.com/categories/pytorch/"}],"tags":[{"name":"CNN","slug":"CNN","permalink":"http://aier02.com/tags/CNN/"}]},{"title":"analysis kernel demo","slug":"analysis_kernel_demo","date":"2018-09-18T10:55:03.519Z","updated":"2018-10-03T08:20:26.193Z","comments":true,"path":"2018/09/18/analysis_kernel_demo/","link":"","permalink":"http://aier02.com/2018/09/18/analysis_kernel_demo/","excerpt":"","text":"analysis kernel demoBatchNormalizationå±‚ï¼šè¯¥å±‚åœ¨æ¯ä¸ªbatchä¸Šå°†å‰ä¸€å±‚çš„æ¿€æ´»å€¼é‡æ–°è§„èŒƒåŒ–ï¼Œå³ä½¿å¾—å…¶è¾“å‡ºæ•°æ®çš„å‡å€¼æ¥è¿‘0ï¼Œå…¶æ ‡å‡†å·®æ¥è¿‘1[]!https://arxiv.org/pdf/1502.03167v3.pdfleakyReluå±‚ï¼š$$LeakyReluï¼ˆxï¼‰ =\\begin{cases}x, &amp; x &gt; 0\\ax, &amp; x &lt;= 0\\end{cases}- pytorchè‡ªå®šä¹‰datasetå’Œdataloader - torch.utils.data.Datasetæ˜¯è¡¨ç¤ºæ•°æ®é›†çš„æŠ½è±¡ç±»ã€‚æ‚¨è‡ªå®šä¹‰çš„æ•°æ®é›†åº”è¯¥ç»§æ‰¿Datasetå¹¶é‡å†™ä»¥ä¸‹æ–¹æ³•ï¼š __len__ ä½¿ç”¨len(dataset)å°†è¿”å›æ•°æ®é›†çš„å¤§å°ã€‚ __getitem__ æ”¯æŒç´¢å¼•ï¼Œdataset[i]å¯ä»¥è·å–ç¬¬iä¸ªæ ·æœ¬ è®©æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„äººè„¸æ ‡è®°ç‚¹æ•°æ®é›†åˆ›å»ºä¸€ä¸ªæ•°æ®é›†ç±»ã€‚æˆ‘ä»¬å°†åœ¨__init__ä¸­è¯»å–csvï¼Œè€Œå°†åœ¨__getitem__å­˜æ”¾è¯»å–å›¾ç‰‡çš„ä»»åŠ¡ã€‚å› ä¸ºæ‰€æœ‰çš„å›¾åƒä¸æ˜¯ä¸€æ¬¡æ€§å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œè€Œæ˜¯æ ¹æ®éœ€è¦è¿›è¡Œè¯»å–ï¼Œè¿™æ ·å¯ä»¥é«˜æ•ˆçš„ä½¿ç”¨å†…å­˜ã€‚123456789101112131415class CustomDataset(data.Dataset):#éœ€è¦ç»§æ‰¿data.Dataset def __init__(self): # TODO # 1. Initialize file path or list of file names. pass def __getitem__(self, index): # TODO # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open). # 2. Preprocess the data (e.g. torchvision.Transform). # 3. Return a data pair (e.g. image and label). #è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œç¬¬ä¸€æ­¥ï¼šread one dataï¼Œæ˜¯ä¸€ä¸ªdata pass def __len__(self): # You should change 0 to the total size of your dataset. return 0å°†numpy ndarryè½¬ä¸ºtensorï¼š123456789101112class ToTensor(object): &quot;&quot;&quot;Convert ndarrays in sample to Tensors.&quot;&quot;&quot; def __call__(self, sample): image, landmarks = sample[&apos;image&apos;], sample[&apos;landmarks&apos;] # swap color axis because # numpy image: H x W x C # torch image: C X H X W image = image.transpose((2, 0, 1)) return &#123;&apos;image&apos;: torch.from_numpy(image), &apos;landmarks&apos;: torch.from_numpy(landmarks)&#125;","categories":[{"name":"pytorch","slug":"pytorch","permalink":"http://aier02.com/categories/pytorch/"}],"tags":[{"name":"demo","slug":"demo","permalink":"http://aier02.com/tags/demo/"}]},{"title":"first time in Kaggle-preparation","slug":"rsna_pneumonia_detection","date":"2018-09-06T09:08:00.932Z","updated":"2018-10-03T08:15:40.619Z","comments":true,"path":"2018/09/06/rsna_pneumonia_detection/","link":"","permalink":"http://aier02.com/2018/09/06/rsna_pneumonia_detection/","excerpt":"","text":"Question descriptionIn this competition, youâ€™re challenged to build an algorithm to detect a visual signal for pneumonia in medical images. Specifically, your algorithm needs to automatically locate lung opacities on chest radiographs.(CXR)èƒ¸éƒ¨Xçº¿ç‰‡ä¸Šè‚ºéƒ¨é˜´å½±çš„å®šä½ä»¥è·Ÿè¸ªå¯èƒ½çš„è‚ºç‚ç—…å†µThey see the potential for ML to automate initial detection (imaging screening) of potential pneumonia cases in order to prioritize and expedite their review.But the title â€œPneumonia Detectionâ€ for the competition is misleading because you actually have to do â€œLung Opacities Detectionâ€, and lung opacities are not the same as pneumonia. Lung opacities are vague, fuzzy clouds of white in the darkness of the lungs, which makes detecting them a real challenge.-from a kernalå¦‚ä½•ç¡®è®¤xç…§ç‰‡ä¸­çš„é˜´å½±ï¼Ÿ Basic informationin CXR:black-air;white-bones;grey-fluid or tissueUsually the lungs are full of air. When someone has pneumonia, the air in the lungs is replaced by other material - fluids, bacteria, immune system cells, etc. Thatâ€™s why areas of opacities are areas that are grey but should be more black.(æ‰¾å‡ºåŸæ¥æ˜¯é»‘ä½†æ˜¯å˜ç°äº†çš„åŒºåŸŸAs you can see, lung opacities are not homogenoues and they do not have a clear center or clear boundaries. I donâ€™t think you can properly segment opacities out of the entire picture because there are no clear boundries.-ç›´æ¥æ‰¾é˜´å½±è²Œä¼¼å¾ˆéš¾ï¼Œå¯ä»¥å…ˆsegment lungsin fact, there was only a moderate level of agreement between radiologists about the presence of infiltrates, which are opacities by definitioné¢„æµ‹ä»»åŠ¡ä¸ºç—…äººçš„dcmå›¾åƒæ‰¾åˆ°lung opacities,ä»¥bboxçš„å½¢å¼ç»™å‡ºï¼Œåœ¨submission_fileä¸­ï¼Œä¸€å¼ å›¾ç‰‡ä¸­åªèƒ½æœ‰ä¸€æ¡bboxä¿¡æ¯ï¼Œæ¯4ä¸ªä¿¡æ¯é¡¹ä¸ºä¸€ä¸ªbboxï¼Œå³(x,y,weight,height) Difficultiesthere is a mass of tissue surrounding the lungs and between the lungs. These areas contain skin, muscles, fat, bones, and also the heart and big blood vessels. That translates into a lot of information on the chest radiograph that is not useful for this competition.é˜´å½±æœ‰å¤šç§ï¼Œæ€æ ·çš„é˜´å½±æ‰å’Œè‚ºç‚ç›¸å…³The main difference in the types of opacities between these two patients is the borders and the shape of the opacity, Patient 3 has multiple round and clearly defined opacities. Patient 2 has this poorly defined haziness which obscures the margins of the lungs and heart. This haziness is termed consolidation.Exclude: obvious mass(es), nodule(s), lobar collapse, linear atelectasisè¦æ‰¾çš„é˜´å½±æ˜¯æ¨¡ç³Šçš„ï¼Œéš¾æ‡‚çš„ï¼Œä¸æ˜æ˜¾çš„In the cases labeled Not Normal/No Lung Opacity, no lung opacity refers to no opacity suspicious for pneumonia. åœ¨æ•°æ®é›†ä¸­ï¼Œä¸æ­£å¸¸/æ²¡æœ‰è‚ºéƒ¨é˜´å½±ï¼Œæ˜¯æŒ‡æ²¡æœ‰ä¸è‚ºç‚ç›¸å…³çš„é˜´å½±ï¼Œä½†ä¼šæœ‰ä¸å…¶ä»–ç—…ç—‡ç›¸å…³çš„é˜´å½±ï¼Œæ•…æ˜¯ä¸æ­£å¸¸ã€‚ä¸¤ç§è‚ºç‚é˜´å½±ï¼šGround-Glass Opacitiesï¼›Consolidationså‰è€…We can see that the lungs are â€œwhiterâ€ than they should be, but we can see most of the borders of the lungs and hearåè€…There are fuzzy areas in the lungs and the borders of the lungs and heart cannot be seen.ç»ç’ƒç²‰çŠ¶çš„é˜´å½±ä¸ä¼šå½±å“å¿ƒè„å’Œè‚ºéƒ¨çš„è¾¹ç•Œï¼Œè€Œåˆå¹¶ç±»çš„é˜´å½±ä¼šæ¨¡ç³Šä¸¤è€…çš„è¾¹ç•Œé¢„æµ‹pneumoniå’Œopacitiesåº”è¯¥ç‹¬ç«‹é¢„æµ‹ï¼Ÿlung opacitieså¹¶ä¸æ˜¯æ˜¯pneumoniaçš„å……è¦æ¡ä»¶å®é™…ä¸Šè¯¥æ¯”èµ›ä¸ºLung Opacities Detectioï¼Œè€Œä¸æ˜¯Pneumonia Detection exprolatory data analysisstage_1_detailed_class_infoé‡Œé¢åŒ…æ‹¬äº†3ç§classï¼Œå…±æœ‰28989æ¡è®°å½•ï¼Œæ¯æ¡ä¿¡æ¯ä¸ºpatientidï¼šclassï¼Œæ ‡æ³¨äº†ç—…äººidå¯¹åº”çš„æ­£å¸¸ã€ä¸æ­£å¸¸/ä¸æ˜¯è‚ºç‚ï¼Œè‚ºç‚3ç§æƒ…å†µstage_1_train_labelsè®°å½•äº†bboxçš„ä½ç½®ï¼ŒåŒæ ·æœ‰28989æ¡è®°å½•ï¼Œæ¯æ¡è®°å½•åˆ†åˆ«æ˜¯patientidï¼šx,y,width,height,target;x,yè¡¨ç¤ºbboxçš„å·¦ä¸Šè§’çš„åæ ‡ï¼Œwidthæ˜¯å®½åº¦ï¼Œå³xçš„èŒƒå›´ï¼Œheightæ˜¯é«˜åº¦ï¼Œå³yçš„èŒƒå›´ï¼Œtarget=0è¡¨ç¤ºæ²¡æœ‰è‚ºç‚ï¼Œ=1è¡¨ç¤ºæœ‰è‚ºç‚ï¼Œå¯¹åº”class_infoï¼Œnormalåˆ™bboxä¿¡æ¯emptyï¼Œtarget=0ï¼›No Lung Opacity / Not Normalåˆ™bboxä¿¡æ¯emptyï¼Œtarget=0ï¼›å› æ­¤lung opacities in data is associated with pneumoniaï¼›lung opacities åˆ™åŒ…å«äº†bboxçš„ä¿¡æ¯åŒæ—¶target=1æ³¨æ„æ•°æ®é›†å­˜åœ¨ä¸€åç—…äººå¯¹åº”å¤šæ¡è®°å½•çš„æƒ…å†µï¼Œå¹¶ä¸æ˜¯æœ‰28989åç—…äººï¼Œç»è¿‡edaå¯çŸ¥å­˜åœ¨25684åç—…äººï¼ˆpatientidï¼‰(training data ä¸­æœ‰25684å¼ dcm)å®é™…ä¸Šå› ä¸ºbboxä¿¡æ¯è¡¨æ˜¯ä¸€æ¡è®°å½•åªèƒ½è¡¨ç¤ºä¸€ä¸ªbboxï¼Œæ•…å­˜åœ¨ä¸€ä¸ªç—…äººçš„cxrä¸­æœ‰å¤šä¸ªbboxï¼Œè€Œclassè¡¨å’Œlabelè¡¨ç›¸ä¸€è‡´ï¼Œæ‰€ä»¥ä¿æŒå¯¹åº”å…³ç³»classå­˜åœ¨é‡å¤æ•°æ®è®­ç»ƒé›†ä¸­dcmæ–‡ä»¶ä¸åªæ˜¯imageï¼Œè¿˜æœ‰meta informationï¼Œå¦‚sexï¼Œageç­‰ï¼Œæ˜¯å¦éœ€è¦æ·»åŠ æ­¤ç±»å±æ€§ï¼Œä»è€Œè€ƒè™‘ç›¸å…³çš„å†…å®¹è¿›è¡Œåˆ¤æ–­ï¼ŸpatientId - A patientId. Each patientId corresponds to a unique image.x_ - the upper-left x coordinate of the bounding box.y_ - the upper-left y coordinate of the bounding box.width_ - the width of the bounding box.height_ - the height of the bounding box.Target_ - the binary Target, indicating whether this sample has evidence of pneumonia.No Lung Opacity / Not Normal and Normal have together the same percent (69.077%) as the percent of missing values for target window in class details information.æ˜¾ç„¶å­˜åœ¨æ­£æ ·æœ¬åå°‘çš„æƒ…å†µï¼Œéœ€è¦åšæ•°æ®å¢å¼ºï¼Œè¿‡é‡‡æ ·ï¼Ÿè§’åº¦åè½¬ï¼Ÿ models ChexNetThe CheXNet algorithm is a 121-layer deep 2D Convolutional Neural Network; a Densenet after Huang &amp; Liu. The Densenetâ€™s multiple residual connections reduce parameters and training time, allowing a deeper, more powerful model. The model accepts a vectorized two-dimensional image of size 224 pixels by 224 pixels.CheXNet is a 121-layer Dense Convolutional Network (DenseNet) (Huang et al., 2016) trained on the ChestX-ray 14 dataset. DenseNets improve flow of information and gradients through the network, making the optimization of very deep networks tractable. We replace the final fully connected layer with one that has a single output, after which we apply a sigmoid nonlinearity. The weights of the network are initialized with weights from a model pretrained on ImageNet (Deng et al., 2009). The network is trained end-to-end using Adam with standard parameters (ÃŸ1 = 0.9 and ÃŸ2 = 0.999) (Kingma &amp; Ba, 2014). We train the model using minibatches of size 16. We use an initial learning rate of 0.001 that is decayed by a factor of 10 each time the validation loss plateaus after an epoch, and pick the model with the lowest validation loss.","categories":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://aier02.com/categories/Kaggle/"}],"tags":[{"name":"segmentation","slug":"segmentation","permalink":"http://aier02.com/tags/segmentation/"},{"name":"chest X-ray","slug":"chest-X-ray","permalink":"http://aier02.com/tags/chest-X-ray/"},{"name":"ChexNet","slug":"ChexNet","permalink":"http://aier02.com/tags/ChexNet/"},{"name":"EDA","slug":"EDA","permalink":"http://aier02.com/tags/EDA/"}]},{"title":"experience from a Kaggler","slug":"kaggle_expericence","date":"2018-09-05T02:19:10.278Z","updated":"2018-10-03T08:08:35.464Z","comments":true,"path":"2018/09/05/kaggle_expericence/","link":"","permalink":"http://aier02.com/2018/09/05/kaggle_expericence/","excerpt":"","text":"kaggleæ¯”èµ›æ­¥éª¤-ç»éªŒ1.ä»”ç»†é˜…è¯»æ¯”èµ›ä»‹ç»å’Œæ•°æ®æè¿°ï¼›2.æŸ¥æ‰¾ç›¸ä¼¼çš„Kaggleæ¯”èµ›ã€‚ä½œä¸ºä¸€ä¸ªæ¥è§¦ä¸ä¹…çš„Kagglerï¼Œæˆ‘å·²ç»å®Œæˆå¯¹æ‰€æœ‰Kaggleæ¯”èµ›åŸºæœ¬åˆ†æçš„æ”¶é›†ï¼›3.ç ”ç©¶ç›¸ä¼¼æ¯”èµ›çš„è§£å†³æ–¹æ¡ˆï¼›4.é˜…è¯»æœ‰å…³è®ºæ–‡ï¼Œä»¥ç¡®ä¿ä¸é”™è¿‡è¯¥é¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼›5.åˆ†ææ•°æ®ï¼Œå¹¶æ„å»ºå¯é çš„äº¤å‰éªŒè¯ç»“æœï¼›6.æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹è®­ç»ƒã€‚7.ç»“æœåˆ†æï¼ŒåŒ…æ‹¬å¦‚é¢„æµ‹åˆ†å¸ƒã€é”™è¯¯åˆ†æå’Œå›°éš¾æ ·æœ¬ç­‰ï¼›8.æ ¹æ®åˆ†ææ¥æ”¹è¿›æ¨¡å‹æˆ–è®¾è®¡æ–°æ¨¡å‹ï¼›9.åŸºäºæ•°æ®åˆ†æå’Œç»“æœåˆ†ææ¥è®¾è®¡æ¨¡å‹ä»¥å¢åŠ å¤šæ ·æ€§æˆ–è§£å†³å›°éš¾æ ·æœ¬ï¼›10.æ¨¡å‹é›†æˆï¼›11.å¿…è¦æ—¶è¿”å›åˆ°å‰é¢çš„æŸä¸ªæ­¥éª¤ã€‚Qï¼šä½ è§‰å¾—ï¼Œèµ¢å¾—æ¯”èµ›çš„å…³é”®æ˜¯ä»€ä¹ˆï¼Ÿå¯é çš„éªŒè¯æ–¹å¼ï¼Œå€Ÿé‰´å…¶ä»–æ¯”èµ›å¹¶é˜…è¯»ç›¸å…³è®ºæ–‡ï¼Œä»¥åŠè‰¯å¥½çš„è‡ªåˆ¶åŠ›å’Œå¿ƒç†ç´ è´¨ã€‚Qï¼šä½ è®¤ä¸ºä½ æœ€å…·ç«äº‰åŠ›çš„æ¯”èµ›æŠ€å·§æˆ–æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿæˆ‘è®¤ä¸ºåº”è¯¥æ˜¯åœ¨æ¯”èµ›å¼€å§‹æ—¶å‡†å¤‡è§£å†³æ–¹æ¡ˆçš„æ–‡æ¡£ã€‚æˆ‘ä¼šå¼ºè¿«è‡ªå·±å†™å‡ºä¸€ä»½æ¸…å•ï¼ŒåŒ…æ‹¬é¢ä¸´çš„æŒ‘æˆ˜ã€åº”è¯¥é˜…è¯»çš„è§£å†³æ–¹æ¡ˆå’Œè®ºæ–‡ã€å¯èƒ½çš„é£é™©ã€å¯ç”¨çš„éªŒè¯æ–¹å¼ã€å¯èƒ½çš„æ•°æ®å¢å¼ºæ–¹æ³•ä»¥åŠå¢åŠ æ¨¡å‹å¤šæ ·æ€§çš„æ–¹å¼ã€‚è€Œä¸”ï¼Œæˆ‘ä¸æ–­æ›´æ–°è¿™ä¸ªæ–‡æ¡£ã€‚å¹¸è¿åœ°ï¼Œè¿™äº›æ–‡æ¡£ä¸ºæˆ‘åé¢åœ¨å¾ˆå¤šæ¯”èµ›ä¸­å–å¾—ä¸é”™æˆç»©æä¾›äº†æ”¯æŒ","categories":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://aier02.com/categories/Kaggle/"}],"tags":[{"name":"experience","slug":"experience","permalink":"http://aier02.com/tags/experience/"}]},{"title":"a failed experience in Kaggle","slug":"airbus_ship_detection","date":"2018-09-05T02:18:30.433Z","updated":"2018-10-03T08:08:00.121Z","comments":true,"path":"2018/09/05/airbus_ship_detection/","link":"","permalink":"http://aier02.com/2018/09/05/airbus_ship_detection/","excerpt":"","text":"airbus-ship-detection challenge é—®é¢˜æè¿°ï¼šä»å«æ˜Ÿå›¾åƒä¸­æ‰¾åˆ°shipsï¼Œå¹¶ç”¨bboxåˆ†å‰²å‡ºæ¥ éš¾ç‚¹æ‰€åœ¨å¹¶ä¸æ˜¯æ‰€æœ‰å›¾ç‰‡éƒ½æœ‰shipsåœ¨å›¾ç‰‡ä¸­shipsçš„å¤§å°ä¸ä¸€è‡´å›¾åƒåˆ†å‰²ä¸å…è®¸å­˜åœ¨éƒ¨åˆ†é‡å ï¼Œä½†æ•°æ®é›†ä¸­çš„shipså½“ä¸¤è€…ç›´æ¥ç›¸é‚»æ—¶ï¼Œå­˜åœ¨è½»å¾®çš„overlapï¼Œé‡å éƒ¨åˆ†å°†ä¼šè¢«ä½œä¸ºèƒŒæ™¯è€Œç§»é™¤éƒ¨åˆ†å¸¦æœ‰äººå·¥æ ‡æ³¨çš„å›¾ç‰‡ä¸­çš„bboxå¯èƒ½å­˜åœ¨è¾¹ç•Œåƒç´ çš„ä¸¢å¤±train-ship-segmentations.csvæä¾›äº†äººå·¥æ ‡æ³¨çš„bboxä½œä¸ºè®­ç»ƒçš„æ•°æ®å›¾ç‰‡ï¼Œå…¶ä¸­bboxä»¥run-length encoding è¡¨ç¤ºã€‚æ•°æ®é›†å¾ˆå¤§ï¼Œéœ€è¦ç”¨åˆ°gpuï¼Œè®­ç»ƒæ¨¡å‹å¯èƒ½è¦å‡ å¤©æ—¶é—´run-length-encodingè¯„ä»·æ–¹å¼ä¸ºIoU,å³äººå·¥æ ‡æ³¨çš„bboxå’Œé¢„æµ‹çš„bboxçš„ç›¸äº¤éƒ¨åˆ†ä¸ä¸¤è€…åˆå¹¶çš„éƒ¨åˆ†çš„å æ¯”ã€‚ EDA(exploratory data analysis)The images (at least many of them) are slices of the same image, not separate frames taken at different times.data leak:The images in test are just shifted versions of images in train. The problem also happens in train, it looks like airbus had bigger images and then the 768x768 are random crops of the bigger images; but it looks they didnâ€™t check whether there were any overlaps.How to find:Run nearest neighbors on all imagesFor each image take N closest neighbors and find where it overlapsCutting all the images 256x256 they can be found easier because they match almost exactly (except for compression artifacts). up to nowdue to data leak,large data set,few computation source and time limit,I decided to pause the competition.","categories":[{"name":"Kaggle","slug":"Kaggle","permalink":"http://aier02.com/categories/Kaggle/"}],"tags":[{"name":"segmentation","slug":"segmentation","permalink":"http://aier02.com/tags/segmentation/"}]},{"title":"Introduction to statistical learning method U1","slug":"statistic_1","date":"2018-08-27T16:58:59.803Z","updated":"2018-10-03T08:31:04.196Z","comments":true,"path":"2018/08/28/statistic_1/","link":"","permalink":"http://aier02.com/2018/08/28/statistic_1/","excerpt":"","text":"æèˆªè€å¸ˆçš„ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹åŸºæœ¬å·²ç»è¿‡ä¸€éäº†ï¼Œæ„Ÿè§‰åªæ˜¯ç•¥æ‡‚çš®æ¯›ï¼Œç°ä¸ºäº†åŠ å¼ºçŸ¥è¯†ç‚¹çš„è®¤è¯†å’Œéƒ¨åˆ†è¯¾åé¢˜çš„å®ç°ï¼Œæœ‰å¿…è¦è¿›è¡Œä¸ªäººæ€»ç»“ã€‚ ç»Ÿè®¡å­¦ä¹  å®šä¹‰statistical learning æ˜¯å…³äºè®¡ç®—æœºåŸºäºæ•°æ®æ„å»ºæ„å»ºç»Ÿè®¡æ¨¡å‹å¹¶è¿ç”¨æ¨¡å‹å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹å’Œåˆ†æçš„ä¸€é—¨å­¦ç§‘ï¼Œæ˜¯æ¦‚ç‡è®ºã€ç»Ÿè®¡å­¦ã€ä¿¡æ¯è®ºã€è®¡ç®—ç†è®ºã€æœ€ä¼˜åŒ–ç†è®ºå’Œè®¡ç®—æœºç§‘å­¦ç­‰å­¦ç§‘çš„äº¤å‰å­¦ç§‘ã€‚ å­¦ä¹ å¯¹è±¡å’Œç›®çš„ä»æ•°æ®å‡ºå‘ï¼Œæå–æ•°æ®çš„ç‰¹å¾ï¼ŒæŠ½è±¡å‡ºæ•°æ®çš„æ¨¡å‹ï¼ˆæ¦‚ç‡ç»Ÿè®¡æ¨¡å‹ï¼‰ï¼Œå¹¶å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹å’Œåˆ†æ å®ç°ç»Ÿè®¡å­¦ä¹ æ–¹æ³•çš„ä¸€èˆ¬æ­¥éª¤å¾—åˆ°ä¸€ä¸ªæœ‰é™çš„è®­ç»ƒæ•°æ®é›†åˆç¡®å®šåŒ…å«æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹çš„å‡è®¾ç©ºé—´ï¼Œå³å­¦ä¹ çš„æ¨¡å‹çš„é›†åˆç¡®å®šæ¨¡å‹é€‰æ‹©çš„å‡†åˆ™ï¼Œå³å­¦ä¹ çš„ç­–ç•¥å®ç°æ±‚è§£æœ€ä¼˜æ¨¡å‹çš„ç®—æ³•ï¼Œå³å­¦ä¹ çš„ç®—æ³•é€šè¿‡å­¦ä¹ æ–¹æ³•é€‰æ‹©æœ€ä¼˜çš„æ¨¡å‹åˆ©ç”¨æœ€ä¼˜æ¨¡å‹å¯¹æ–°çš„æ•°æ®è¿›è¡Œé¢„æµ‹æˆ–åˆ™åˆ†æç®€è€Œè¨€ä¹‹ï¼šæ•°æ®-&gt;å‡è®¾ç©ºé—´-&gt;ç­–ç•¥-&gt;ç®—æ³•-&gt;æœ€ä¼˜æ¨¡å‹-&gt;é¢„æµ‹åˆ†æ ç›‘ç£å­¦ä¹ ç»Ÿè®¡å­¦ä¹ åŒ…æ‹¬äº†ç›‘ç£å­¦ä¹ ï¼Œéç›‘ç£å­¦ä¹ ï¼ŒåŠç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚ åŸºæœ¬æ¦‚å¿µè¾“å…¥æ‰€æœ‰å¯èƒ½å–å€¼çš„é›†åˆç§°ä¸ºè¾“å…¥ç©ºé—´ï¼ŒåŒç†è¾“å‡ºæ‰€æœ‰å¯èƒ½çš„å–å€¼é›†åˆç§°ä¸ºè¾“å‡ºç©ºé—´ï¼Œé€šå¸¸output space è¿œå°äº input spaceï¼›è€Œç‰¹å¾ç©ºé—´åˆ™æ˜¯æ‰€æœ‰ç‰¹å¾å‘é‡æ‰€åœ¨çš„ç©ºé—´ï¼Œç‰¹å¾å‘é‡ç”¨äºè¡¨ç¤ºä¸€ä¸ªè¾“å…¥å®ä¾‹ï¼Œç‰¹å¾ç©ºé—´çš„æ¯ä¸ªç»´åº¦å¯¹åº”ä¸€ä¸ªç‰¹å¾ï¼Œè¾“å…¥ç©ºé—´å¯ä»¥å’Œç‰¹å¾ç©ºé—´ç›¸åŒï¼Œä¹Ÿå¯æŠŠè¾“å…¥ç©ºé—´æ˜ å°„åˆ°ç©ºé—´ï¼Œæ¨¡å‹å®è´¨æ˜¯å®šä¹‰åœ¨ç‰¹å¾ç©ºé—´ä¸Šçš„ï¼ˆå¯¹ç‰¹å¾å‘é‡è¿›è¡Œå­¦ä¹ ï¼‰ã€‚Xè¡¨ç¤ºè¾“å…¥å˜é‡ï¼ŒYè¡¨ç¤ºè¾“å‡ºå˜é‡ï¼Œè¾“å…¥å˜é‡Xä¸­çš„ç¬¬iä¸ªè¡¨ç¤ºä¸ºxi=(xi(1),xi(2),...,xi(n))Tx_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^Txâ€‹iâ€‹â€‹=(xâ€‹iâ€‹(1)â€‹â€‹,xâ€‹iâ€‹(2)â€‹â€‹,...,xâ€‹iâ€‹(n)â€‹â€‹)â€‹Tâ€‹â€‹Nä¸ªæ•°æ®çš„è®­ç»ƒæ•°æ®é›†(labled)è¡¨ç¤ºä¸ºT={(x1,y1),(x2,y2),...,(xN,yN)}T=\\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\\}T={(xâ€‹1â€‹â€‹,yâ€‹1â€‹â€‹),(xâ€‹2â€‹â€‹,yâ€‹2â€‹â€‹),...,(xâ€‹Nâ€‹â€‹,yâ€‹Nâ€‹â€‹)}è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„å…³ç³»ç”±æ¨¡å‹è¿›è¡Œè¡¨ç¤ºï¼Œæ‰€æœ‰å¯èƒ½çš„ç”±è¾“å…¥ç©ºé—´ï¼ˆç‰¹å¾ç©ºé—´ï¼‰åˆ°è¾“å‡ºç©ºé—´çš„æ˜ å°„çš„é›†åˆç»„æˆäº†å‡è®¾ç©ºé—´ï¼Œå­¦ä¹ çš„èŒƒå›´å±€é™åœ¨å‡è®¾ç©ºé—´ä¸­ã€‚ ç»Ÿè®¡å­¦ä¹ çš„ä¸‰è¦ç´  æ¨¡å‹ç»Ÿè®¡å­¦ä¹ é¦–å…ˆè€ƒè™‘æ˜¯å­¦ä¹ ä»€ä¹ˆæ ·çš„æ¨¡å‹ï¼Œåœ¨ç›‘ç£å­¦ä¹ ä¸­å°±æ˜¯è¦å­¦ä¹ çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒæˆ–è€…å†³ç­–å‡½æ•°ï¼Œç”±æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹æ„æˆçš„é›†åˆç»„æˆäº†å‡è®¾ç©ºé—´F,é€šå¸¸æ˜¯ä¸€ä¸ªç”±å‚æ•°å†³å®šçš„å‡½æ•°æ—.å†³ç­–å‡½æ•°æ¨¡å‹çš„é›†åˆè¡¨ç¤ºä¸º$$F={f|Y=f_\\theta(X),\\theta\\in R^n}$$æ¡ä»¶æ¦‚ç‡æ¨¡å‹çš„é›†åˆè¡¨ç¤ºä¸º$$F={f|P=P_\\theta(Y|X),\\theta\\in R^n}$$å…¶ä¸­Î¸\\thetaÎ¸å–å€¼äºnç»´çš„æ¬§æ°ç©ºé—´ï¼Œç§°ä¸ºå‚æ•°ç©ºé—´ ç­–ç•¥æœ‰äº†å‡è®¾ç©ºé—´ï¼Œæ¥ç€è€ƒè™‘æŒ‰ç…§ä»€ä¹ˆæ ·çš„å‡†åˆ™å­¦ä¹ æœ€ä¼˜çš„æ¨¡å‹ï¼Œç§°ä¸ºç­–ç•¥ã€‚ æŸå¤±å‡½æ•°å’Œé£é™©å‡½æ•°æŸå¤±å‡½æ•°åº¦é‡æ¨¡å‹é¢„æµ‹çš„å¥½åï¼ˆé¢„æµ‹çš„ç»“æœå’Œæ ‡è®°çš„å·®è·ï¼‰ï¼Œé£é™©å‡½æ•°åº¦é‡å¹³å‡æ„ä¹‰ä¸‹çš„æ¨¡å‹é¢„æµ‹çš„å¥½åï¼ˆåœ¨å…·ä½“æŸæ¬¡é¢„æµ‹å¯èƒ½å‡ºé”™çš„æœŸæœ›ï¼‰ æŸå¤±å‡½æ•°è®°ä½œL(f(X),Y)&gt;=0,å¸¸è§ç±»å‹:0-1æŸå¤±å‡½æ•°ï¼ˆ0-1 loss fuction ï¼‰ï¼šL(f(X),Y)={1,Yâ‰ f(X)0,Y=f(X)L(f(X),Y) = \\begin{cases} 1, &amp; Y \\neq f(X)\\\\ 0, &amp; Y = f(X) \\end{cases}L(f(X),Y)={â€‹1,â€‹0,â€‹â€‹â€‹Yâ‰ f(X)â€‹Y=f(X)â€‹â€‹å¹³æ–¹æŸå¤±å‡½æ•°(quadratic loss function):L(f(X),Y)=(Yâˆ’f(X))2L(f(X),Y) =(Y-f(X))^2L(f(X),Y)=(Yâˆ’f(X))â€‹2â€‹â€‹ç»å¯¹æŸå¤±å‡½æ•°(absolute loss fuction):L(f(X),Y)=âˆ£(Yâˆ’f(X))âˆ£L(f(X),Y) =|(Y-f(X))|L(f(X),Y)=âˆ£(Yâˆ’f(X))âˆ£ ç»éªŒé£é™©å’Œç»“æ„é£é™©æ¨¡å‹f(x)å…³äºè®­ç»ƒæ•°æ®é›†çš„å¹³å‡æŸå¤±ç§°ä¸ºç»éªŒé£é™©ï¼šRemp(f)=1Nâˆ‘i=1NL(yi,f(xi))R_{emp}(f) =\\frac 1N\\sum_{i=1}^NL(y_i,f(x_i))Râ€‹empâ€‹â€‹(f)=â€‹Nâ€‹â€‹1â€‹â€‹â€‹i=1â€‹âˆ‘â€‹Nâ€‹â€‹L(yâ€‹iâ€‹â€‹,f(xâ€‹iâ€‹â€‹))ç»“æ„é£é™©ç®€å•è€Œè¨€å°±æ˜¯ç»éªŒé£é™©åŠ å…¥äº†æ­£åˆ™åŒ–é¡¹ï¼Œæ­£åˆ™åŒ–é¡¹ç”¨äºè¡¨ç¤ºæ¨¡å‹å¤æ‚åº¦ï¼Œå› æ­¤ç»“æ„é£é™©æœ€å°åŒ–æ˜¯ä¸ºäº†é˜²æ­¢è¿‡æ‹Ÿåˆè€Œæå‡ºçš„ç­–ç•¥ã€‚Rsrm(f)=1Nâˆ‘i=1NL(yi,f(xi))+Î»J(x)R_{srm}(f) =\\frac 1N\\sum_{i=1}^NL(y_i,f(x_i))+\\lambda J(x)Râ€‹srmâ€‹â€‹(f)=â€‹Nâ€‹â€‹1â€‹â€‹â€‹i=1â€‹âˆ‘â€‹Nâ€‹â€‹L(yâ€‹iâ€‹â€‹,f(xâ€‹iâ€‹â€‹))+Î»J(x)å…¶ä¸­$\\lambda $&gt;=0,ç”¨ä»¥æƒè¡¡ç»éªŒé£é™©å’Œæ¨¡å‹å¤æ‚åº¦ ç®—æ³•ç®—æ³•æ˜¯æŒ‡å­¦ä¹ æ¨¡å‹çš„å…·ä½“æ–¹æ³•ï¼Œä¸€èˆ¬è€Œè¨€å°±æ˜¯æ±‚è§£æ¨¡å‹fä¸­çš„å‚æ•°ä»¥è¾¾åˆ°æ¨¡å‹çš„æœ€ä¼˜åŒ–ï¼Œæ•…å¾ˆå¤šæ—¶å€™ç»Ÿè®¡å­¦ä¹ çš„ç®—æ³•åˆ°æœ€ååŸºæœ¬éƒ½æ˜¯æœ€ä¼˜åŒ–é—®é¢˜ã€‚ æ¨¡å‹è¯„ä¼°å’Œæ¨¡å‹é€‰æ‹© è®­ç»ƒè¯¯å·®è®­ç»ƒè¯¯å·®æ˜¯æ¨¡å‹å…³äºè®­ç»ƒæ•°æ®é›†çš„å¹³å‡æŸå¤±ï¼Œå³å‰é¢æ‰€è¯´çš„ç»éªŒé£é™©ï¼›Rexp(f)=1Nâˆ‘i=1NL(yi,f(xi))R_{exp}(f) =\\frac 1N\\sum_{i=1}^NL(y_i,f(x_i))Râ€‹expâ€‹â€‹(f)=â€‹Nâ€‹â€‹1â€‹â€‹â€‹i=1â€‹âˆ‘â€‹Nâ€‹â€‹L(yâ€‹iâ€‹â€‹,f(xâ€‹iâ€‹â€‹)) æµ‹è¯•è¯¯å·®æµ‹è¯•è¯¯å·®è·Ÿè®­ç»ƒè¯¯å·®ç›¸ä¼¼ï¼Œåªæ˜¯å‰è€…æ˜¯åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šçš„å¹³å‡æŸå¤± è¿‡æ‹Ÿåˆå’Œæ¨¡å‹é€‰æ‹©ä¸€å‘³æœ€æ±‚æ¨¡å‹å¯¹äºè®­ç»ƒæ•°æ®çš„é¢„æµ‹èƒ½åŠ›ï¼Œæ‰€æ±‚å¾—çš„æ¨¡å‹å¾€å¾€ä¼šæ¯”çœŸå®æ¨¡å‹æ›´åŠ å¤æ‚ï¼Œè¿™ç§ç°è±¡å«åšè¿‡æ‹Ÿåˆï¼ˆæ¨¡å‹å‚æ•°è¿‡å¤šï¼Œè®­ç»ƒè¯¯å·®å¾ˆå°ï¼Œä½†æµ‹è¯•è¯¯å·®è¾ƒå¤§æˆ–è€…æ³›åŒ–èƒ½åŠ›å¾ˆå·®ï¼‰ï¼Œè€Œæ¨¡å‹çš„é€‰æ‹©å°±æ˜¯ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆå¹¶æé«˜æ¨¡å‹å¯¹äºæœªçŸ¥æ•°æ®çš„é¢„æµ‹èƒ½åŠ›ã€‚ æ­£åˆ™åŒ–æ¨¡å‹é€‰æ‹©çš„å…¸å‹æ–¹æ³•æ˜¯æ­£åˆ™åŒ–ï¼Œé€šè¿‡ç»“æ„é£é™©æœ€å°åŒ–å®ç°ï¼Œå³åœ¨ç»éªŒé£é™©ååŠ ä¸Šä¸€ä¸ªæ­£åˆ™åŒ–é¡¹ï¼Œä¸€èˆ¬æ˜¯æ¨¡å‹å¤æ‚åº¦çš„å•è°ƒé€’å¢å‡½æ•°ï¼ˆæ¯”å¦‚L_2èŒƒæ•°ï¼‰æ­£åˆ™åŒ–çš„ä½œç”¨æ˜¯é€‰æ‹©ç»éªŒé£é™©å’Œæ¨¡å‹å¤æ‚åº¦åŒæ—¶è¾ƒå°çš„æ¨¡å‹ï¼›é—®é¢˜æ˜¯ä¸ºä»€ä¹ˆéœ€è¦ç®€å•çš„æ¨¡å‹ï¼Ÿä¸€æ–¹é¢æ˜¯è¿‡æ‹Ÿåˆçš„å­˜åœ¨ä½¿å¾—æ¨¡å‹æœ‰å¯èƒ½è¿‡åº¦å…³æ³¨è®­ç»ƒæ•°æ®é›†çš„â€œä¸ªæ€§â€ï¼Œå¯¼è‡´æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä¸‹é™ï¼›å¦ä¸€ä¸ªè§£é‡Šæ˜¯æ ¹æ®å¥¥å¡å§†å‰ƒåˆ€åŸç†ï¼Œâ€œå¦‚æ— å¿…è¦ï¼Œå‹¿å¢å®ä½“â€ï¼Œå³åœ¨æ‰€æœ‰å¯èƒ½é€‰æ‹©çš„æ¨¡å‹ä¸­ï¼Œèƒ½å¤Ÿå¾ˆå¥½åœ°è§£é‡Šæ•°æ®å¹¶ä¸”ååˆ†ç®€å•çš„æ¨¡å‹æ‰æ˜¯æœ€ä¼˜çš„ï¼ˆå°½ç®¡è¿™ä¸ªè¦æ±‚æœ‰ç‚¹è‡ªç›¸çŸ›ç›¾ï¼Œæ­¤æ—¶Î»\\lambdaÎ»å¾€å¾€èµ·åˆ°æŠ˜ä¸­çš„ä½œç”¨ï¼‰ï¼›ä»è´å¶æ–¯ä¼°è®¡çš„è§’åº¦è€Œè¨€ï¼Œæ­£åˆ™åŒ–é¡¹å¯¹åº”äºæ¨¡å‹çš„å…ˆéªŒæ¦‚ç‡ï¼Œå¤æ‚çš„æ¨¡å‹å…ˆéªŒæ¦‚ç‡å°ï¼Œç®€å•çš„æ¨¡å‹å…ˆéªŒæ¦‚ç‡å¤§ã€‚ äº¤å‰éªŒè¯åŸºæœ¬æ€æƒ³æ˜¯é‡å¤åœ°ä½¿ç”¨æ•°æ®ï¼Œå¯¹ç»™å®šçš„æ•°æ®è¿›è¡Œåˆ’åˆ†ï¼Œç„¶åç»„åˆæˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œåå¤è¿›è¡Œè®­ç»ƒã€æµ‹è¯•å’Œæ¨¡å‹çš„é€‰æ‹©ã€‚ ç®€å•äº¤å‰éªŒè¯éšæœºæŠŠæ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸¤éƒ¨åˆ†ï¼ˆè®­ç»ƒé›†æ›´å¤§ï¼‰ï¼Œæ”¹å˜ä¸åŒçš„æ¡ä»¶ä½¿å¾—åœ¨ç›¸åŒçš„è®­ç»ƒé›†ä¸Šä¹Ÿèƒ½å¾—åˆ°ä¸åŒçš„æ¨¡å‹ï¼Œç„¶ååœ¨æµ‹è¯•é›†è¿›è¡Œæ¨¡å‹çš„æµ‹è¯•å’Œé€‰æ‹©ã€‚ SæŠ˜äº¤å‰éªŒè¯éšæœºåœ°æŠŠæ•°æ®é›†åˆ’åˆ†ä¸ºSä¸ªäº’ä¸ç›¸äº¤çš„å­æ•°æ®é›†ï¼Œåˆ©ç”¨S-1ä¸ªå­æ•°æ®é›†ä½œä¸ºè®­ç»ƒæ•°æ®é›†ï¼Œå‰©ä¸‹çš„ä¸€ä¸ªåšä¸ºæµ‹è¯•é›†ï¼›å°†è¿™ä¸€è¿‡ç¨‹å¯¹å¯èƒ½çš„Sä¸­é€‰æ‹©é‡å¤è¿›è¡Œï¼Œä»å¾—åˆ°çš„Sä¸ªæ¨¡å‹ä¸­é€‰æ‹©æµ‹è¯•è¯¯å·®æœ€å°çš„æ¨¡å‹ã€‚ ç•™ä¸€äº¤å‰éªŒè¯S=Næ—¶ï¼Œç”¨äºæ•°æ®ç¼ºä¹çš„æƒ…å†µä¸‹ï¼Œå³æ¯æ¬¡åªæ‹¿ä¸€ä¸ªæ•°æ®æ ·æœ¬ä½œä¸ºæµ‹è¯•é›†ï¼Œè¿›è¡ŒNæ¬¡ç›¸åŒæ“ä½œï¼Œä»Nä¸ªæ¨¡å‹ä¸­é€‰æ‹©æœ€ä¼˜è€…ï¼ˆå¹³å‡æµ‹è¯•è¯¯å·®æœ€å°ï¼‰ã€‚ æ³›åŒ–èƒ½åŠ›generalization alibityæŒ‡ç”±å­¦ä¹ æ–¹æ³•å¾—åˆ°çš„æ¨¡å‹å¯¹äºæœªçŸ¥æ•°æ®çš„é¢„æµ‹èƒ½åŠ›ï¼Œå¸¸ç”¨æµ‹è¯•è¯¯å·®è¯„ä»·å­¦ä¹ æ–¹æ³•çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»…è®°æµ‹è¯•æ•°æ®é›†æ˜¯ååˆ†å®è´µçš„ï¼Œåœ¨å­¦ä¹ æ¨¡å‹çš„è¿‡ç¨‹ä¸­ä¸èƒ½ä½¿ç”¨ï¼Œè¦åœ¨è®­ç»ƒå®Œæˆåæ‰èƒ½ç”¨äºæµ‹è¯•ï¼Œä¿è¯å¯¹äºå­¦å¾—çš„æ¨¡å‹è€Œè¨€ï¼Œæµ‹è¯•æ•°æ®æ˜¯â€œæœªçŸ¥çš„â€ã€‚å¯¹äºæ³›åŒ–èƒ½åŠ›çš„åˆ†æå¾€å¾€é€šè¿‡ç ”ç©¶æ³›åŒ–è¯¯å·®çš„æ¦‚ç‡ä¸Šç•Œè¿›è¡Œï¼Œå¸¸å…·æœ‰ä»¥ä¸‹æ€§è´¨ï¼šæ ·æœ¬å®¹é‡å¢åŠ ï¼Œæ³›åŒ–ä¸Šç•Œè¶‹äº0ï¼›å‡è®¾ç©ºé—´å®¹é‡è¶Šå¤§ï¼Œæ¨¡å‹å°±è¶Šéš¾å­¦ï¼Œæ³›åŒ–è¯¯å·®ä¸Šç•Œå°±è¶Šå¤§ã€‚å­˜åœ¨å®šç†ï¼Œå¯¹äºäºŒç±»åˆ†ç±»é—®é¢˜ï¼Œå½“å‡è®¾ç©ºé—´æ˜¯æœ‰é™ä¸ªå‡½æ•°çš„é›†åˆæ—¶F={f1,f2,f3â€¦,fd},å¯¹ä»»æ„ä¸€ä¸ªå‡½æ•°fâˆˆ\\inâˆˆF,è‡³å°‘ä»¥æ¦‚ç‡1-Î´\\deltaÎ´,ä»¥ä¸‹ä¸ç­‰å¼æˆç«‹ï¼šR(f)&lt;=R^(f)+Ïµ(d,N,Î´)R(f)&lt;=\\hat R(f)+\\epsilon (d,N,\\delta)R(f)&lt;=â€‹Râ€‹^â€‹â€‹(f)+Ïµ(d,N,Î´)å³æ³›åŒ–è¯¯å·®&lt;=è®­ç»ƒè¯¯å·®+Nçš„å•è°ƒé€’å‡å‡½æ•°(å³ç«¯å³ä¸ºæ³›åŒ–è¯¯å·®çš„ä¸Šç•Œ)Ïµ(d,N,Î´)=12N(logd+log1Î´)\\epsilon (d,N,\\delta)=\\sqrt {\\frac 1{2N}(logd+log\\frac 1\\delta)}Ïµ(d,N,Î´)=âˆšâ€‹â€‹2Nâ€‹â€‹1â€‹â€‹(logd+logâ€‹Î´â€‹â€‹1â€‹â€‹)â€‹â€‹â€‹å¯è§æ³›åŒ–è¯¯å·®ä¸Šç•Œå’Œè®­ç»ƒè¯¯å·®ã€æ ·æœ¬å®¹é‡å‡æ­£ç›¸å…³ï¼Œå’Œæ¨¡å‹å¤æ‚åº¦dè´Ÿç›¸å…³ã€‚ ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹ ç”Ÿæˆæ¨¡å‹ç”±ç”Ÿæˆæ–¹æ³•å­¦åˆ°çš„æ¨¡å‹ç§°ä¸ºç”Ÿæˆæ¨¡å‹ï¼Œç”Ÿæˆæ–¹æ³•ç”±æ•°æ®å­¦ä¹ è”åˆæ¦‚ç‡åˆ†å¸ƒP(X,Y),ç„¶åæ±‚å‡ºæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒP(Y|X)ä½œä¸ºé¢„æµ‹çš„æ¨¡å‹ï¼Œå³ï¼šP(Yâˆ£X)=P(X,Y)P(X)P(Y|X)=\\frac {P(X,Y)}{P(X)}P(Yâˆ£X)=â€‹P(X)â€‹â€‹P(X,Y)â€‹â€‹ç”Ÿæˆæ¨¡å‹è¡¨ç¤ºäº†ç»™å®šè¾“å…¥Xäº§ç”Ÿè¾“å‡ºYçš„ç”Ÿæˆå…³ç³»ï¼Œå…¸å‹çš„ç”Ÿæˆæ¨¡å‹ç”±æœ´ç´ è´å¶æ–¯æ³•å’Œéšé©¬å°”å¯å¤«æ¨¡å‹ã€‚ åˆ¤åˆ«æ¨¡å‹ç”±åˆ¤åˆ«æ–¹æ³•å­¦åˆ°çš„æ¨¡å‹ç§°ä¸ºåˆ¤åˆ«æ¨¡å‹ï¼Œåˆ¤åˆ«æ–¹æ³•ç”±æ•°æ®ç›´æ¥å­¦ä¹ å†³ç­–å‡½æ•°f(X)æˆ–è€…æ¡ä»¶æ¦‚ç‡P(Y|X)ä½œä¸ºé¢„æµ‹çš„æ¨¡å‹ã€‚åˆ¤åˆ«æ¨¡å‹è¡¨ç¤ºç»™å®šè¾“å…¥Xåº”è¯¥é¢„æµ‹ä»€ä¹ˆæ ·çš„è¾“å‡ºYã€‚å…¸å‹çš„åˆ¤åˆ«æ¨¡å‹æœ‰kç´§é‚»æ³•ï¼Œæ„ŸçŸ¥æœºï¼Œå†³ç­–æ ‘ï¼Œé€»è¾‘æ–¯è°›å›å½’æ¨¡å‹ï¼Œæœ€å¤§ç†µæ¨¡å‹ï¼Œæ”¯æŒå‘é‡æœºï¼Œæå‡æ–¹æ³•ï¼Œæ¡ä»¶éšæœºåœºç­‰ã€‚å­˜åœ¨éšå˜é‡æ—¶ä¸èƒ½ä½¿ç”¨åˆ¤åˆ«æ–¹æ³•ï¼Œä½†å¯ä»¥ç”¨ç”Ÿæˆæ–¹æ³•ï¼›åˆ¤åˆ«æ–¹æ³•ç›´æ¥é¢å¯¹é¢„æµ‹ï¼Œå‡†ç¡®ç‡æ›´é«˜ã€‚","categories":[{"name":"statistical learning method","slug":"statistical-learning-method","permalink":"http://aier02.com/categories/statistical-learning-method/"}],"tags":[{"name":"basic knowledge","slug":"basic-knowledge","permalink":"http://aier02.com/tags/basic-knowledge/"},{"name":"introduction","slug":"introduction","permalink":"http://aier02.com/tags/introduction/"}]},{"title":"Build personal blog","slug":"blog_tutorial","date":"2018-08-07T11:24:35.583Z","updated":"2018-10-03T08:30:19.069Z","comments":true,"path":"2018/08/07/blog_tutorial/","link":"","permalink":"http://aier02.com/2018/08/07/blog_tutorial/","excerpt":"","text":"æœ€è¿‘åœ¨åŠ å¼ºmlå’ŒcvåŸºç¡€çŸ¥è¯†çš„å­¦ä¹ ï¼Œä¸ºäº†åŠ æ·±ç†è§£ï¼ŒåŒæ—¶è®°å½•è‡ªå·±çš„å­¦ä¹ è¿‡ç¨‹ï¼Œå°è¯•ç€å†™blogï¼›å‘å¥½å‹è¯·æ•™åå¾—çŸ¥å»ºç«‹ä¸ªäººblogçš„æ–¹å¼ï¼Œæˆ‘çš„é€‰æ‹©æ˜¯Hexo(ä¸€ç§é™æ€åšå®¢ç½‘é¡µæ¡†æ¶)+Github pageï¼ˆå…è´¹æ‰˜ç®¡åšå®¢é¡¹ç›®ä»£ç ï¼‰ï¼Œå½“ç„¶è¿˜æœ‰ç§Ÿäº‘æœåŠ¡å™¨å’Œè‡ªå·±å†™åå°å’Œå‰ç«¯çš„æ–¹å¼ï¼Œå‰è€…æ›´åŠ æ–¹ä¾¿å’Œæ˜“äºä¸Šæ‰‹ã€‚ å‰æœŸå‡†å¤‡ å®‰è£…Gitæ¯•ç«Ÿæ‰€æœ‰é¡¹ç›®ä»£ç éƒ½æ˜¯æ‰˜ç®¡åˆ°Githubä¸Šé¢ï¼Œå¿…é¡»ä¿è¯ç³»ç»Ÿä¸­å·²ç»å®‰è£…äº†gitï¼Œterminalä¸­è¾“å…¥gitä¸å‡ºç°commant not foundå³å¯ å®‰è£…Node.jså› ä¸ºHexoçš„ä½¿ç”¨åŸºäºNode.jsï¼Œæ‰€ä»¥è¦å…ˆå®‰è£…Node.jså’Œä»–çš„åŒ…å®‰è£…å™¨npm,å»ºè®®åˆ°å®˜ç½‘ä¸‹è½½pkgå¹¶é€‰æ‹©ç¨³å®šç‰ˆæœ¬(LTS) Githubåˆ›å»ºé¡¹ç›®åœ¨Githubä¸Šé¢åˆ›å»ºä¸€ä¸ªåä¸ºï¼šyourname.github.io çš„ç©ºé¡¹ç›®ï¼Œyournameæ˜¯æŒ‡ä½ åˆ›å»ºçš„githubè´¦æˆ·åï¼ˆgithubä¸Šé¢æ¯ä¸ªç”¨æˆ·çš„ç”¨æˆ·åæ˜¯å”¯ä¸€çš„æ ‡è¯†ï¼‰ï¼Œè¯¥Repositoryå°±æ˜¯ä¹‹åä½ çš„blogæ‰€æœ‰ä»£ç å’Œæ–‡ä»¶å­˜æ”¾çš„åœ°æ–¹ï¼Œblogçš„åœ°å€åœ¨ä¸æ·»åŠ è®¾ç½®çš„æƒ…å†µä¸‹ï¼šhttps://yourname.github.io å®‰è£…Hexoåœ¨ç»ˆç«¯è¾“å…¥npmå‘½ä»¤ä¸‹è½½é™æ€ç½‘é¡µç”Ÿæˆå™¨Hexo1npm install -g hexoè‹¥å‡ºç°permiss deniedç­‰æƒ…å†µï¼Œåˆ™åŠ å…¥sudo1sudo npm install -g hexo åˆ›å»ºHexoæ–‡ä»¶å¤¹åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­åˆ›å»ºHexoæ–‡ä»¶å¤¹ï¼Œcdè¿›å…¥è¯¥æ–‡ä»¶å¤¹ï¼Œç„¶åè¿›è¡Œåˆå§‹åŒ–1Hexo initæ²¡æœ‰é”™è¯¯ï¼Œåˆ™ä»¥åæ‰€æœ‰æ“ä½œéƒ½å¿…é¡»åœ¨è¯¥æ–‡ä»¶ç›®å½•ä¸‹è¿›è¡Œï¼Œç‰¹åˆ«æ³¨æ„æ–‡ä»¶å¤¹Hexo/node_models,æ‰€æœ‰é€šè¿‡npm install XXX --saveæ“ä½œä¸‹è½½çš„ä¾èµ–éƒ½ä¼šå­˜æ”¾åœ¨è¯¥æ–‡ä»¶å¤¹ä¸­ï¼Œæ˜“äºåŒ…çš„ç®¡ç†(â€“saveå‚æ•°çš„æ„ä¹‰å°±æ˜¯å­˜æ”¾åœ¨node_models)é¡ºåˆ©å®Œæˆä¸Šè¿°æ­¥éª¤åå¯ä»¥ç™»é™†https://yourname.github.io æŸ¥çœ‹æ•ˆæœ ç»‘å®šä¸ªäººåŸŸåPs:ä¸æƒ³èŠ±é’±çš„åŒå­¦å¯ä»¥è·³è¿‡ è´­ä¹°å›½å¤–ä¸ªäººåŸŸåå¯¹äºæ²¡æœ‰ä¸ªäººåŸŸåçš„åŒå­¦æ¥è¯´ï¼Œå›½å†…å¤‡æ¡ˆå®åœ¨æ˜¯å¤ªæ¼«é•¿äº†(æˆ‘å‰æ®µæ—¶é—´å°±åœ¨è…¾è®¯äº‘å¤‡æ¡ˆè¿‡ä¸€æ¬¡ï¼Œå‘¨æœŸå¤ªé•¿==)ï¼Œç›´æ¥è´­ä¹°å›½å¤–çš„åŸŸåæ–¹ä¾¿è€Œä¸”é¢„ç®—åŸºæœ¬ä¸€æ ·ï¼Œæ¨ènamesiloï¼Œ60ä¸€å¹´ï¼Œåç¼€å¤šé€‰æ‹©è€Œä¸”æœ‰å…è´¹çš„private protectionï¼›æ³¨å†Œçš„æ—¶å€™æˆ‘å†™çš„å‡çš„ä¿¡æ¯(é™¤äº†é‚®ç®±ï¼‰ï¼›ä¸€å¼€å§‹å…ˆæ£€ç´¢ä½ æƒ³æ³¨å†Œçš„åŸŸåï¼Œç„¶ååœ¨æ ¹æ®è‹±æ–‡æŒ‡å¯¼å–è´­ä¹°å°±okäº†(é»˜è®¤è®¾ç½®),ç½‘ç«™æ”¯æŒalipayï¼Œéå¸¸æ–¹ä¾¿äº†ã€‚ æ›´æ”¹DNSç»Ÿä¸€ä½¿ç”¨ä¸€ä¸ªdnsçš„è¯å¯¹äºä»¥åå¤šåŸŸåç®¡ä¼šæ›´åŠ æ–¹ä¾¿ï¼Œæ¨èä½¿ç”¨å›½å†…çš„å‚å•†dnspodï¼Œç™»é™†åæ·»åŠ ä½ æ›´æ³¨å†Œçš„åŸŸåè¿›è¡Œç®¡ç†ï¼Œä¸»è¦æ·»åŠ è¿ä¸ªä¸»æœºè®°å½•ï¼Œåˆ†åˆ«æ˜¯@å’Œwww,è­¬å¦‚æˆ‘çš„è®¾ç½®ä¸ºå›¾ä¸­Aè®°å½•çš„è®°å½•å€¼ä¸ºgithub pageä¸­é¡¹ç›®çš„ipï¼Œå¯ä»¥é€šè¿‡åœ¨ç»ˆç«¯ä¸­è¾“å…¥ping https://yourname.github.io æ‰¾åˆ°ï¼Œä¹Ÿå¯åœ¨Github helpä¸­æ‰¾åˆ°ã€‚ç™»é™†namesiloï¼Œåœ¨å³ä¾§æ‰¾åˆ°domain manager,è¿›å…¥åé€‰æ‹©æ›´æ”¹nameserver,å¤„ç†è¿‡ç¨‹è¿˜æ˜¯æŒºé•¿çš„ï¼Œæ‰€ä»¥ä¸ç”¨æ€¥ç€ç™»é™†ä½ çš„åŸŸå æ›´æ”¹blogä¸»é¢˜åœ¨ Hexo ä¸­æœ‰ä¸¤ä»½ä¸»è¦çš„é…ç½®æ–‡ä»¶ï¼Œå…¶åç§°éƒ½æ˜¯ _config.ymlã€‚ å…¶ä¸­ï¼Œä¸€ä»½ä½äºç«™ç‚¹æ ¹ç›®å½•ä¸‹ï¼Œä¸»è¦åŒ…å« Hexo æœ¬èº«çš„é…ç½®ï¼›å¦ä¸€ä»½ä½äºä¸»é¢˜ç›®å½•ä¸‹ï¼Œè¿™ä»½é…ç½®ç”±ä¸»é¢˜ä½œè€…æä¾›ï¼Œä¸»è¦ç”¨äºé…ç½®ä¸»é¢˜ç›¸å…³çš„é€‰é¡¹ã€‚ä¸ºäº†æè¿°æ–¹ä¾¿ï¼Œåœ¨ä»¥ä¸‹è¯´æ˜ä¸­ï¼Œå°†å‰è€…ç§°ä¸ºç«™ç‚¹é…ç½®æ–‡ä»¶ï¼Œ åè€…ç§°ä¸ºä¸»é¢˜é…ç½®æ–‡ä»¶ é€‰æ‹©ä¸»é¢˜è¿›å…¥å®˜ç½‘é€‰æ‹©é€‚åˆçš„ä¸»é¢˜[hexo](https://hexo.io/themes/)ï¼Œè¿›å…¥ç›¸åº”çš„Githubåœ°å€ï¼Œgit cloneå‘½ä»¤æŠŠæ•´ä¸ªæ–‡ä»¶å¤¹éƒ½ä¸‹è½½ä¸‹æ¥ï¼Œå­˜æ”¾åœ¨Hexo/themesæ–‡ä»¶å¤¹ä¸­ã€‚ æ›´æ”¹é…ç½®æ–‡ä»¶ä¸€èˆ¬è€Œè¨€ï¼Œä¸‹è½½ä¸‹æ¥çš„ä¸»é¢˜æ–‡ä»¶ä¸­éƒ½æœ‰æ–°æ‰‹å¼•å¯¼ï¼Œæ³¨æ„readmeå°±okäº†ã€‚ éƒ¨ç½²é¡¹ç›®ä»£ç åœ¨Hexo/_config.ymlå’Œthemesé‡Œé¢çš„configæ–‡ä»¶ä¸­æ·»åŠ ä»¥ä¸‹è¯´æ˜1234deploy: type: git repo: https://github.com/aier02/aier02.github.io.git branch: masteræ³¨æ„ä¿®æ”¹repoçš„åœ°å€ä¸ºä½ çš„githubä¸Šé¢çš„åœ°å€(åªè¦ä¿®æ”¹ä½ çš„ç”¨æˆ·å)åœ¨Hexoç›®å½•ä¸‹è¾“å…¥1hexo clean &amp;&amp; hexo g &amp;&amp; hexo då³å¯å°†æœ¬åœ°ä¸‹çš„Hexo/publicæ–‡ä»¶å¤¹æ‰€æœ‰å†…å®¹ä¸Šä¼ åˆ°githubé¡¹ç›®ä¸­(å¯èƒ½éœ€è¦ç”Ÿæˆå…¬ç§é’¥ï¼Œè¯·è‡ªè¡Œç™¾åº¦ï¼‰hexo cleanå®è´¨æ˜¯åˆ é™¤Hexo/publicï¼Œè€Œhexo gåˆ™æ˜¯æ ¹æ®é…ç½®å’ŒHexo/sourceç”ŸæˆHexo/public,æ–°å†™çš„blogå­˜æ”¾åœ¨Hexo/source/_postså³å¯ï¼Œhexo då‘½ä»¤åˆ™æ˜¯å°†Hexo/publicä¸Šä¼ åˆ°gitæœåŠ¡å™¨ä¸­,ä½¿ç”¨ä¸ªäººåŸŸåçš„åŒå­¦è¿˜å¾—åœ¨Githubä¸Šé¢çš„yourname.github.ioä»“åº“æ ¹ç›®å½•ä¸‹æ–°å»ºä¸€ä¸ªCNAMEæ–‡ä»¶ï¼Œå†™å…¥ä½ çš„ä¸ªäººåŸŸå,ä¸éœ€è¦httpå’Œwww,æ¯”å¦‚æˆ‘çš„å°±æ˜¯åœ¨ç¬¬ä¸€è¡Œä¸ºï¼šaier02.com ä¸ªäººå¯„è¯­blogå…³é”®åœ¨äºå†…å®¹ï¼Œæ‰€ä»¥æˆ‘åŸºæœ¬å¾ˆå¤šæ“ä½œéƒ½æ˜¯ä¸ºäº†æ–¹ä¾¿ï¼Œå€˜è‹¥é‡åˆ°äº†ä»€ä¹ˆé—®é¢˜å¯åœ¨ä¸‹æ–¹è¯„è®ºæå‡º(ä¸»è¦æ˜¯çœ‹æˆ‘æ˜¯å¦ä¹Ÿé‡åˆ°äº†å¯ä»¥æä¾›è§£å†³æ–¹æ¡ˆï¼Œæ²¡æœ‰è¸©è¿‡çš„å‘æˆ‘ä¹Ÿä¸ä¼š=ã€‚=ï¼‰,è¿˜æœ‰å¯ä»¥é€‰æ‹©çš„themeå¾ˆå¤šï¼Œå¥‰åŠå„ä½ä»¥å®ç”¨ä¸ºä¸»ï¼Œå†å¥½çœ‹çš„blogæ²¡æœ‰å†…å®¹ä¹Ÿæ˜¯æ²¡ä»·å€¼çš„ã€‚","categories":[{"name":"blog","slug":"blog","permalink":"http://aier02.com/categories/blog/"}],"tags":[{"name":"Hexo","slug":"Hexo","permalink":"http://aier02.com/tags/Hexo/"},{"name":"Github page","slug":"Github-page","permalink":"http://aier02.com/tags/Github-page/"},{"name":"namesilo","slug":"namesilo","permalink":"http://aier02.com/tags/namesilo/"}]}]}